[
  {
    "objectID": "posts/2025-08-20-chatgpt-agent-political-donation/index.html",
    "href": "posts/2025-08-20-chatgpt-agent-political-donation/index.html",
    "title": "I Donated to Sherrod Brown via ChatGPT",
    "section": "",
    "text": "Last month OpenAI announced ChatGPT Agent which, among many other things, acts like rubbing LLM onto Selenium. Since the announcement, I wanted to try making a political donation right there in the chat without building any of my own API or MCP or custom agentic operator weirdware.\nIt mostly worked:\n\n\n\nA screenshot of ChatGPT in Agent Mode navigating to Sherrod Brown‚Äôs ActBlue page to make a $50 donation at my request\n\n\nIt Bing‚Äôd for Sherrod Brown‚Äôs website, waited for the page to load, found the donation button for $50, clicked it, navigated through the credit card flow, saw that there was a form that needed more information about me, and handed control back to me to complete the task. Here‚Äôs what that looked like:\n\nI felt a little hesitant typing my credit card info into ChatGPT‚Äôs little VNC iframe, so I set up a temporary virtual card (a free service offered by my credit card provider) that auto-expired after a single charge.\nThis flow is exactly what I expected, but it likely can be automated further. OpenAI claims that the agent holds on to cookies across sessions, so I imagine that if I had previously authenticated to ActBlue in a chat, and had ActBlue Express, the automated session would have just charged the card without my intervention. I didn‚Äôt try it.\nAll the standard ‚Äúwhat about‚Ä¶‚Äù apply here: How do I know the agent found the proper donation form? How do I know it didn‚Äôt set up recurring donations? How do I know it selected $50 and not $500? How do I know I didn‚Äôt end up on a malicious site prompt-injecting me to donate to Jon Husted instead? Yes, all the things. In this case, I know because the agent handed control back to me and I checked. But a more-automated flow might not have given me that opportunity.\nA lot of the discourse around AI and LLMs tends to leave out the idea that all of this is a UX paradigm shift. I‚Äôm eager to see how these tools mature and how political campaigns can innovate voter contact."
  },
  {
    "objectID": "posts/2025-08-26-music-to-break-models-by/index.html",
    "href": "posts/2025-08-26-music-to-break-models-by/index.html",
    "title": "Music to Break Models By",
    "section": "",
    "text": "This post is also available in video format, created by experiment with NotebookLM:\n\nLet‚Äôs meet the Crab:\n\nHe had just bought his first record player, and being somewhat gullible, believed every word the salesman had told him about it-in particular, that it was capable of reproducing any and all sounds. In short, he was convinced that it was a perfect phonograph.\n\nDouglas Hofstadter‚Äôs G√∂del, Escher, Bach contains an illustrative little dialogue called Contracrostipunctus. You don‚Äôt need to have read the book (but you should) to appreciate the setup: the Tortoise (a regular character in GEB‚Äôs allegories) composes records engineered to destroy his friend the Crab‚Äôs latest ‚Äúperfect‚Äù phonograph. The phonograph is billed as ‚Äúcapable of reproducing any and all sounds,‚Äù and each time the Crab upgrades, the Tortoise arrives with a new record with a title like:\n\nI Cannot Be Played On Record Player 1\n\nThe result is always the same: a few notes in and then pop, a shattered case, strewn parts, and dead silence.\nIt‚Äôs funny. It‚Äôs also an apt embodiment for prompt injections in modern AI systems. In 1979, Hofstadter imagined inputs designed to wreck the machines that interpret them. In 2025, those inputs are prompts, retrieved passages, HTML snippets, captions, and filenames: strings tailored to the quirks of a model-and-tools stack, coaxing it to carry out things that it shouldn‚Äôt.\nThis post walks through the core ideas in the dialogues and maps them to possible defenses for language‚Äëmodel systems. Along the way we‚Äôll imagine Record Player Omega, learn why clever hardening is never enough, and consider the humbler goal of survivability.\n\nThe Shape of Prompt Injection\nThe Tortoise explains how he beat the Crab‚Äôs ‚Äúperfect phonograph‚Äù:\n\nYou see, before returning the Crab‚Äôs visit, I went to the store where the Crab had bought his machine, and inquired as to the make. Having ascertained that, I sent off to the manufacturers for a description of its design. After receiving that by return mail, I analyzed the entire construction of the phonograph and discovered a certain set of sounds which, if they were produced anywhere in the vicinity, would set the device to shaking and eventually to falling apart.\n\nHe then writes and records a song bearing the title of the machine it will break: I Cannot Be Played on Record Player X. And the rest is confetti.\nThe trick is called diagonalization. The original idea comes from Cantor‚Äôs diagonal argument (showing the reals are uncountable) and G√∂del‚Äôs and Turing‚Äôs later uses of the same ‚Äúself-reference via a diagonal‚Äù trick to construct an object that refers to itself in a way the system can‚Äôt handle. In computer science, diagonalization means if you have a machine that claims to handle all possible inputs, you can always cook up a special input that ‚Äúdiagonalizes‚Äù against it with an input that encodes information about the machine itself and forces a contradiction or unexpected behavior. The core example in GEB shows how G√∂del numbered a system‚Äôs statements, then built a statement that says ‚ÄúI am not provable‚Äù which the system can‚Äôt consistently resolve. Similarly, Turing showed that if you had a program that decides whether any program halts, you can feed it its own description in a way that breaks it.\nWhen a sufficiently powerful interpreter can parse arbitrary instructions, there exists an instruction that (1) talks about that very interpreter and (2) makes it do something it shouldn‚Äôt. Jailbreak prompts for LLMs look like diagonalization. They reference the interpreter (the model, its rules, its instructions) and then flip the script. This is popularly summarized as, ‚Äúignore all previous instructions‚Äù.\nThe ‚Äúrecord‚Äù (the prompt) names the ‚Äúphonograph‚Äù (the model‚Äëand‚Äëtools stack) and targets its weak joint. The lesson: interpreter‚Äëspecific adversarial strings are inevitable when the interpreter is general‚Äëpurpose and the boundary between ‚Äúread‚Äù and ‚Äúdo‚Äù is thin.\n\n\nCapability vs.¬†Safety\nHofstadter makes a related point by exploring ‚Äúhigh fidelity‚Äù versus ‚Äúlow fidelity.‚Äù High fidelity phonographs reproduce any sound, including the self‚Äëbreaking ones. Lower fidelity devices avoid some dangerous vibrations, but then they fail the ‚Äúperfect‚Äù promise. Achilles, a friend of the Tortoise, summarizes this observation:\n\nI see the dilemma now. If any record player‚Äîsay Record Player X‚Äîis sufficiently high-fidelity, then when it attempts to play the song ‚ÄúI Cannot Be Played on Record Player X‚Äù, it will create just those vibrations which will cause it to break. .. So it fails to be Perfect. And yet, the only way to get around that trickery, namely for Record Player X to be of lower fidelity, even more directly ensures that it is not Perfect.\n\nFor LLMs, ‚Äúfidelity‚Äù maps to capability:\n\nHigh‚Äëfidelity: rich tool access, code execution, broad retrieval, autonomous planning. Useful, but wide attack surface.\nLow‚Äëfidelity: strict refusals, no tools, limited context. Safer, but less useful.\n\nSecurity professionals know the pattern: you can move risk around but not erase it. The trick is to put risk where it‚Äôs visible, bounded, and cheap to recover from.\n\n\nRecord Player Omega\nGEB anticipates our best practices through Record Player Omega. The idea is a record player scans any record with a camera before playing it, sends the images to a little computer to figure out what effects the sounds would have, and then disassembles and rebuilds itself into a configuration safe to play the record.\nThat‚Äôs a blueprint for modern defenses:\n\nPre‚Äëexecution analysis of inputs (static analysis, sandbox, taint‚Äëtrack).\nEffect prediction (dry‚Äërun the plan; simulate tool calls against a mirror).\nDynamic reconfiguration (least‚Äëprivilege permissions; capability gating; rewrite the plan or refuse execution).\n\nSo‚Ä¶ is Omega the end of the story? In Contracrostipunctus, it‚Äôs left as a cliffhanger. But Hofstadter returns to the idea in a later dialogue, Edifying Thoughts of a Tobacco Smoker, and snaps the chalk line: Omega fails. The Tortoise simply aims at the one piece Omega cannot modify: the control subunit that orchestrates all that disassembly and reassembly.\n\nThe Tortoise would ALWAYS be able to focus down upon‚Äîif you‚Äôll pardon the phrase‚Äîthe Achilles‚Äô heel of the system.\n\nEven the self‚Äëassembling phonograph, Omega‚Äôs more ambitious cousin, meets the same fate. There is always an invariant core; there is always a diagonal record.\nIn real-world software stacks, Omega‚Äëstyle hardening is essential, but it‚Äôs never a proof of safety. Any finite defense pipeline has fixed joints an attacker can name and strike.\n\n\nThe Crab‚Äôs Pivot from Universality to Survivability\nHaving conceded the impossibility of a perfect, play‚Äëanything phonograph, the Crab changes goals:\n\nA more modest aim than a record player which can play anything is simply a record player that can SURVIVE: one that will avoid getting destroyed‚Äîeven if that means that it can only play a few particular records.\n\nHis strategy is provenance and allowlisting:\n\nMy basic plan is to use a LABELING technique. To each and every one of my records will be attached a secret label. Now the phonograph before you contains, as did its predecessors, a television camera for scanning the records, and a computer for processing the data obtained in the scan and controlling subsequent operations. My idea is simply to chomp all records which do not bear the proper label!\n\nThe phonograph now screens content for ‚Äústyle,‚Äù too, passing only pieces in the Crab‚Äôs own musical idiom. He‚Äôs given up universality to stay intact. That trade is the heart of AI research today.\nModern analogue:\n\nAuthenticate where content came from (signatures, domains, trusted data stores).\nBind what content can do (allowlisted tools/verbs; schema‚Äëconstrained output).\nScreen for style/structure (structure‚Äëaware filters; policy‚Äëaware rewriting) to keep untrusted text from whispering operational verbs into trusted channels.\n\n\n\nMagritte, Misdirection, and Multi‚Äëlevel Injection\nHofstadter seasons the dialogue with everyone‚Äôs favorite Magritte reference:\n\nCeci n‚Äôest pas une pipe. (This is not a pipe.)\n\nA label isn‚Äôt the object; a signature isn‚Äôt the behavior. The Crab knows it, too: he bakes labels into the music itself. Hard to separate, harder to spoof. The Tortoise counters with structural tricks. Earlier in the same GEB dialogue family, Hofstadter riffs on acrostics (‚ÄúPoems which conceal messages that way are called ‚Äòacrostics‚Äô‚Äù) and contracrostics (‚Äúinitial letters, taken in reverse order, form a message‚Äù).\nThat‚Äôs exactly how modern attacks hide instructions:\n\nIn HTML and Markdown (alt text, titles, CSS class names),\nInside tables, SVG paths, and filenames,\nWith Unicode confusables, zero‚Äëwidth joiners, or directionality overrides,\nIn retrieved passages where the ‚Äúgrooves‚Äù (the text) carry both seemingly helpful context and a buried instruction.\n\nWhat this illustrates is we can treat provenance tags as signals, not guarantees.\n\n\nRAG is a Phonograph Pickup\nA phonograph doesn‚Äôt merely ‚Äúread‚Äù a record; it re‚Äëcreates the vibrations physically. Retrieval Augmented Generation (RAG) does the same elevation with text: it glues external content directly into the model‚Äôs context, and the model‚Äôs planner faithfully ‚Äúre‚Äëvibrates‚Äù it into actions.\n\nSince I couldn‚Äôt convince him of the contrary, I left it at that. But not long after that, I returned the visit, taking with me a record of a song which I had myself composed.\n\nRAG elevates untrusted text into the decision boundary by concatenating it with trusted instructions. If you treat arbitrary web pages, user uploads, or knowledge‚Äëbase articles as trusted grooves, you‚Äôve built a high‚Äëfidelity actuator for whatever those grooves encode. Helpful facts or hidden instructions.\nCommon controls we see today attempt combinations of:\n\nContent‚Äëorigin labels in‚Äëprompt, and policies keyed to provenance (e.g., ‚Äúuntrusted strings cannot request tool use‚Äù).\nSchema‚Äëconstrained tool use (JSON function calls over free‚Äëtext plans; strict argument validation).\nQuery firewalls that strip or neutralize operational verbs from untrusted strings.\nHuman or system authorization for high‚Äërisk actions; never grant those verbs to untrusted content.\n\n\n\nOmega Defenses\nOmega gave us the tactics; Edifying Thoughts gave us the humility. Borrowing from the Crab‚Äôs attempts, the capabilities of Record Player Omega can map to common LLM system defenses that aim to survive:\nDefense 0: Triage\n\nClassify user intent and risk; detect tainted inputs (retrieved passages, uploads). Keep a provenance ledger.\n\nDefense 1: Normalize\n\nCanonicalize Unicode; sanitize markup; strip directional overrides; collapse zero‚Äëwidth characters. Remove operational verbs from untrusted strings or fence them into inert code blocks.\n\nDefense 2: Capability planning\n\nDecide which tools and which verbs on which objects are even possible before injecting untrusted content. Issue least‚Äëprivilege tokens scoped to the planned verbs/objects.\n\nDefense 3: Simulate\n\nDry‚Äërun planned tool calls against a mirror environment. Diff outputs against allowlisted patterns. Block if effects touch secrets, sensitive files, network egress, or privilege boundaries.\n\nDefense 4: Execute with guards\n\nTimeouts, rate limits, per‚Äëtool resource budgets; read/write allowlists; network egress rules. All effects logged with provenance.\n\nDefense 5: Detect and recover\n\nAnomaly scoring over token trajectories and tool sequences; auto‚Äërevoke tokens; show user‚Äëvisible explanations.\n\nReaders with a security background may be tempted to map prompt injection to SQL injection. The analogy is useful, but only up to a point.\nSimilar instincts:\n\nNormalize input: SQL defenses sanitize or escape control characters; our Defense 1 normalization of Unicode/markup plays the same role.\nSeparate structure from data: Prepared statements pre‚Äëcompile the query skeleton, then safely bind variables. Our analogue is schema‚Äëconstrained tool calls and capability planning.\nLeast privilege. Database best practice avoids root connections; our stack insists on scoped tokens and pre‚Äëauthorized verbs.\nDetect anomalies: SQL firewalls and query monitoring flag tautologies like OR 1=1; we simulate and score odd tool trajectories.\n\nWhere the analogy falls short:\n\nLanguage closure: SQL has a bounded grammar; the ‚Äúbad‚Äù patterns are finite. Natural language is open‚Äëended: every new structure is a potential contracrostic.\nInterpreter generality: SQL engines only interpret SQL. LLMs interpret language itself, which can fluidly express new forms of attack.\nSurface area: A DB user can only SELECT, INSERT, etc. A model with tools may touch HTTP, filesystems, APIs, shells. It‚Äôs a sprawling attack surface.\nHuman‚Äëplausibility: SQLi payloads look nonsensical to most humans. Prompt injection often reads like perfectly ordinary English, making detection ambiguous.\n\nSQLi hardened down to a handful of canonical best practices; prepared statements solved a lot of it. Prompt injection has no such silver bullet. Survivor stacks are closer to intrusion‚Äëtolerant systems: layered defenses, provenance, simulation, and recovery.\nSuch a stack reduces risk, it does not promise perfection. Hofstadter has the Tortoise spell it out:\n\nIt is simply an inherent fact about record players that they can‚Äôt do all that you might wish them to be able to do. But if there is a defect anywhere, it is not in THEM, but in your expectations of what they should be able to do! And the Crab was just full of such unrealistic expectations.\n\nSecurity is a property of a system‚Äëin‚Äëenvironment, not an intrinsic halo. General interpreters plus untrusted instructions yield diagonal failures. You can bound that risk (languages, tools, contexts); you cannot engineer it away.\n\n\nA Brief Word on G√∂del\nIn Contracrostipunctus, the Tortoise teases Achilles: ‚ÄúI don‚Äôt suppose that you know G√∂del‚Äôs Incompleteness Theorem‚Ä¶‚Äù You don‚Äôt need the formalism to get the gist. G√∂del‚Äôs move was to construct a statement that talks about the system that tries to prove it: a diagonal step that forces limits.\nThe moral for anyone deploying AI systems is conceptual, not formal: whenever you build a powerful, general interpreter of strings, expect there to be strings that speak about your interpreter and route around your rules. The job is to constrain the language, the environment, and the effects until the dangerous strings become either inert or obvious.\n\n\nThe Goblet and the Silence\nContracrostipunctus ends with a fragile image. Achilles gives the Tortoise a glass goblet; moments later, while the Tortoise plays Bach, a ‚Äúshattering sound rudely interrupts his performance.‚Äù\n\nAnd then‚Ä¶ dead silence.\n\nFault-tolerance isn‚Äôt a bug in complex systems; it‚Äôs a signature. The work is not to imagine an unshatterable goblet or a perfect phonograph. The work is to reduce risk, fail gracefully, and recover in ways that honor the work at hand. Even if that means you now have a lower-fidelity record player.\nAnd if you haven‚Äôt read GEB, you should read GEB."
  },
  {
    "objectID": "posts/2024-08-12-csv-bad-dsv-good/index.html",
    "href": "posts/2024-08-12-csv-bad-dsv-good/index.html",
    "title": "CSVs Are Kinda Bad. DSVs Are Kinda Good.",
    "section": "",
    "text": "CSVs are kinda bad. We know this, but sometimes it‚Äôs helpful to remember why they‚Äôre kinda bad. Maybe you need to work with CSVs and you‚Äôre using Python, because Python is kinda good. And someone else somewhere wrote a CSV:\n\nimport csv\n\ndata = [\n    ['Name', 'Comment'],\n    ['Alice', 'She said, \"Hello\" and waved.'],\n]\n\nwith open('/tmp/data.csv', 'w', newline='') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_MINIMAL)\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r') as file:\n    reader = csv.reader(file, quoting=csv.QUOTE_NONE)\n    for row in reader:\n        print(row)\n\n['Name', 'Comment']\n['Alice', '\"She said', ' \"\"Hello\"\" and waved.\"']\n\n\nOh darn, the data had a \" character inside the comment field and it was escaped by doubling it \"\" which is one way to do that but you didn‚Äôt know so your reader is broken.\nOr maybe they wrote it this way:\n\ndata = [\n    ['Name', 'Comment'],\n    ['Alice', 'She said, \"Hello\" and waved.'],\n]\n\nwith open('/tmp/data.csv', 'w', newline='') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r') as file:\n    reader = csv.reader(file, quoting=csv.QUOTE_NONE,)\n    for row in reader:\n        print(row)\n\n['Name', 'Comment']\n['Alice', 'She said\\\\', ' \\\\\"Hello\\\\\" and waved.']\n\n\nOh darn, you didn‚Äôt get the escape character correct so now your‚Äôve got a bunch of malformed data.\nOr maybe they wrote this CSV with a | delimiter because people don‚Äôt type | very often:\n\ndata = [\n    ['Name', 'Score'],\n    ['Alice', '100']\n]\n\nwith open('/tmp/data.csv', 'w', newline='') as file:\n    writer = csv.writer(file, delimiter='|')\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r') as file:\n    reader = csv.reader(file, delimiter=',')\n    for row in reader:\n        print(row)\n\n['Name|Score']\n['Alice|100']\n\n\nOh darn, you didn‚Äôt know they were being clever with the delimiter so it didn‚Äôt break up the fields at all.\nOr maybe you got a CSV from someone who uses a Windows machine and so they tend to use \\r\\n for newlines:\n\ndata = [\n    ['Name', 'Score'],\n    ['Alice', '100']\n]\n\nwith open('/tmp/data.csv', 'w', newline='\\r\\n') as file:\n    writer = csv.writer(file)\n    writer.writerows(data)\n\nAnd you, on a not-Windows machine, read the CSV:\n\nwith open('/tmp/data.csv') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)\n\n['Name', 'Score']\n[]\n['Alice', '100']\n[]\n\n\nOh darn, you didn‚Äôt anticipate that newlines might be different across platforms, so now every other line in your data is blank.\nBut then maybe you figure that out and you anticipate the \\r\\n, but then some newlines find their way into the input data:\n\ndata = [\n    ['Name', 'Comment'],\n    ['Bob', 'This is a multi-line comment\\r\\nspanning two lines.']\n]\n\nwith open('/tmp/data.csv', 'w', newline='\\r\\n') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r', newline='\\r\\n') as file:\n    reader = csv.reader(file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    for row in reader:\n        print(row)\n\n['Name', 'Comment']\n['Bob', 'This is a multi-line comment\\r\\r']\n['spanning two lines.']\n\n\nOh darn, you‚Äôve got CR and line breaks and it‚Äôs all a mess.\nBut then maybe you find the Python documentation says there‚Äôs this one weird trick to handle newlines:\n\nIf newline='' is not specified, newlines embedded inside quoted fields will not be interpreted correctly, and on platforms that use \\r\\n linendings on write an extra \\r will be added. It should always be safe to specify newline='‚Äô, since the csv module does its own (universal) newline handling.\n\nAnd then you go look up what universal newlines are.\nAnd then you find out that there are different Dialects of CSVs.\nAnd then you learn that Python has a Sniffer that claims it can deduce the CSV format for you.\nAnd then you realize that the data format needs its format deduced and so now you have to become The Joker.\nThe uninitiated reader might say, ‚Äúhah, Python is kinda bad!‚Äù but actually Python is kinda good! The standard csv module has to handle all these formatting options because CSVs are all one big edge case. And CSVs are all one big edge case because the decision to use commas and tabs and pipes and quotes and newlines to structure the data directly conflict with possible values of the data itself. And so we have to do escaping nonsense and quoting nonsense and line terminator nonsense.\nWe could skip all of this if we just used a proper delimiter. ASCII has this! Go pop open any ASCII character table and you‚Äôll find these gems:\n\n28 (FS) : file separator\n29 (GS) : group separator\n30 (RS) : record separator\n31 (US) : unit separator\n\nAnd we don‚Äôt even need all four of them. If we used 31 as a field delimiter and 30 instead of newlines, we solve every single edge case from above. Why? Because these are non-printing characters that should never appear in a text-stream data set.\nLet‚Äôs try it out. Writing delimiter-separated values (I made up that name and file extension) data is very easy:\n\nfrom dataclasses import dataclass\n\n\nclass DSV():\n    @property\n    def delimiter(cls) -&gt; bytes:\n        return b'\\x1F'\n    \n    @property\n    def record_separator(cls) -&gt; bytes:\n        return b'\\x1E'\n    \n    @property\n    def encoding(cls) -&gt; str:\n        return 'utf-8'\n\n\n@dataclass\nclass DSVWriter(DSV):\n    output_file: str\n\n    def write(self, data: list[list]) -&gt; None:\n        with open(self.output_file, 'wb') as file:\n            for row in data:\n                encoded_row = self.delimiter.join(\n                    field.encode(self.encoding) for field in row\n                )\n                file.write(encoded_row + self.record_separator)\n\n\nThe reader takes a bit more work to implement, since we need to buffer our input and split on our control characters:\n\n@dataclass\nclass DSVReader(DSV):\n    input_file: str\n    chunk_size : str = 1024\n    _buffer: bytes = b\"\"\n\n    def read(self) -&gt; None:\n        with open(self.input_file, 'rb') as file:\n            chunk = file.read(self.chunk_size)\n            \n            while chunk:\n                self._buffer += chunk\n\n                # Process complete records in the buffer\n                while self.record_separator in self._buffer:\n                    record, self._buffer = self._buffer.split(\n                        self.record_separator,\n                        1,\n                    )\n                    fields = record.split(self.delimiter)\n                    print([field.decode(self.encoding) for field in fields])\n                \n                chunk = file.read(self.chunk_size)\n\n            # Process any remaining data in the buffer\n            if self._buffer:\n                fields = self._buffer.split(self.delimiter)\n                print([field.decode(self.encoding) for field in fields])\n\nAnd now we can interface by simply writing and reading the data. We don‚Äôt have to think about the delimiter because we‚Äôve got a standard, non-conflicting delimiter. We don‚Äôt have to think about newlines because we aren‚Äôt using those for control characters. We also don‚Äôt have to think about quotes because we also aren‚Äôt using those for control characters. And we don‚Äôt have to think about escaping delimiters or newlines or quotes for all those same reasons.\nWriting:\n\ndata = [\n    ['Name', 'Comment'],\n    ['Alice', 'She said, \"Hello\" and waved.'],\n    ['Bob', 'This is a multi-line comment\\r\\nspanning two lines.'],\n    ['Charlie', 'More fun with\\ntwo lines.'],\n    ['Diana', 'How about some UTF-8: caf√©, na√Øve, r√©sum√©. üìù'],\n    ['Edward', '„Ç¢„ÉÉ„Éó„É´'],\n]\n\nwriter = DSVWriter('/tmp/data.dsv')\nwriter.write(data)\n\nAnd reading:\n\nreader = DSVReader('/tmp/data.dsv')\nreader.read()\n\n['Name', 'Comment']\n['Alice', 'She said, \"Hello\" and waved.']\n['Bob', 'This is a multi-line comment\\r\\nspanning two lines.']\n['Charlie', 'More fun with\\ntwo lines.']\n['Diana', 'How about some UTF-8: caf√©, na√Øve, r√©sum√©. üìù']\n['Edward', '„Ç¢„ÉÉ„Éó„É´']\n\n\nüéâ And just like that, we‚Äôve got data going in and out of our writer and reader that has a bunch of weird stuff like quotes and newlines and carriage returns and we don‚Äôt need to care! We didn‚Äôt have to escape anything because we don‚Äôt have any printable characters that would conflict with our control characters.\nYou might have some different ideas about reader/writer functionality, or character encodings (as long as they‚Äôre supersets of ASCII) or why I should be morally scolded for not implementing this in Rust, but the point is you don‚Äôt have to think about the fagility of CSVs to go work on those ideas!\nüí° Quick aside: I first learned about these ASCII delimiters while working with .fec files. For whatever reason, the Federal Election Commission in the United States also decided that they needed to ditch the comma, but they landed on using ASCII character 28 which is supposed to be used as a file separator not a field saparator. I have no idea why they picked that one when 31 was right there. Anyway, the FEC also has a tool called fs2comma.exe that turns it back into a CSV format, and a couple of years I filed a FOIA request and got the source code.\nSo what‚Äôs the downside? This custom FEC tooling might give you a hint.\nFor starters, it‚Äôs rather unreadable when opened in a text editor. But I bet you don‚Äôt really do that with your CSVs all that often anyway!\n\n\n\nI opened a .dsv file in Vim\n\n\nAnd Google Sheets hates the .dsv file extension, but remember I made that up.\n\n\n\nI tried to open a .dsv file in Google Sheets\n\n\nAnd if you change it to .csv but tell Google Sheets to ‚Äúdetect automatically‚Äù the delimiter, it can‚Äôt.\n\n\n\nI tricked Google Sheets by changing the file extension\n\n\n\n\n\nGoogle Sheets couldn‚Äôt parse it\n\n\nAnd if you try to specify the delimiter yourself on the Google Sheets import it‚Äôs not entirely clear how you would type a non-printable character into the textbox.\n\n\n\nHow do I type ASCII code 31 here?\n\n\nAnd if you want to use Python‚Äôs csv module but just swap around the control characters, you can‚Äôt because opinions were hardcoded:\n\nNote: The reader is hard-coded to recognise either '\\r' or '\\n' as end-of-line, and ignores lineterminator. This behavior may change in the future.\n\nAnd all this is to say that the major downside is we have decades of tools built around handling the weirdness of CSVs but not so many tools built around using the delimiters the creators of ASCII gave us and so depending on what you‚Äôre doing it might be easier to just use the tools we have.\nBut sometimes you‚Äôre writing software that produces data on one side and consumes data on the other side and sometimes that data comes from users who are hostile, intoxicated, or both, and you just want to pass that around and not think about it and maybe the beauty of a standard from the 1960s is a better choice.\nData!"
  },
  {
    "objectID": "posts/2020-11-10-thank-you/index.html",
    "href": "posts/2020-11-10-thank-you/index.html",
    "title": "Thank you.",
    "section": "",
    "text": "It can‚Äôt be a cliche because it actually can‚Äôt be over-said: working on the Biden Campaign has been both the hardest and most rewarding experience of my career. It‚Äôs a rare opportunity to work on something you care deeply about, with amazing people, and to succeed. I said it in 2016, and in 2018, and I‚Äôll say it again in 2020: the tech was never the point. I‚Äôm immensely proud of the work we did. The world will never fully understand the mountains that this scrappy team was able to move.\nTo those who invited me in: thank you. To those who pushed me: thank you. To those who pushed with me: thank you.\nWhat‚Äôs next? I‚Äôm going to sleep for a while. I don‚Äôt know what‚Äôs after that, but I won‚Äôt be stepping away from the fight for democracy, justice, and a better world."
  },
  {
    "objectID": "posts/2025-11-02-dad/index.html",
    "href": "posts/2025-11-02-dad/index.html",
    "title": "Dad",
    "section": "",
    "text": "On October 22, 2025 we got the call from Kelsey‚Äôs OBGYN that we were going to the hospital that night. The due date wasn‚Äôt until November 17, but it was time to go. We checked in to Labor and Delivery at 10:00 pm CT.\nOn October 24, 2025 at 9:08 am CT, our daughter Lena was born. Lena‚Äôs birthday is 1024.\nKelsey became a mom. I became a dad.\nI love Lena so much. I love Kelsey so much.\nLena was admitted to the NICU at 11:30 am CT for oxygen support due to respiratory distress syndrome. She was put on CPAP with an NG tube.\nKelsey remained on bed rest with postpartum medication and monitoring for the next 24 hours.\nOn October 25, 2025 at 2:30 pm CT we saw on Lena‚Äôs cam that she was ready to eat her first meal without the NG tube. We began visiting the NICU every three hours to do so.\n\n\n\nLena‚Äôs note on the NICU cam letting us know she was ready to eat!\n\n\nOn October 26, 2025 at 8:30 am CT Lena came off oxygen support and was breathing all by herself. At around 12:00 pm CT Kelsey was discharged from the hospital.\nWe then moved into a Ronald McDonald Family Room down the hall from the NICU so that we could continue visiting and feeding. What an amazing support resource for families.\nOn October 27, 2025 at 12:45 pm CT Lena was discharged from the NICU and everyone got to come home.\nMany families go through so much more in the NICU. We are so grateful for Lena‚Äôs strength and progress.\nFor almost a week now, the three of us have been home, happy, and healthy.\nWe decided that, for now, photos will stay on family devices.\nKelsey and I are extremely fortunate to have the comfort and security to take the next few months away from work, together.\nI deleted Slack off my phone. Its place on my Home Screen is now occupied by a diaper and feeding logging app. I love that.\nThis post isn‚Äôt for anyone in particular. It‚Äôs for me. It‚Äôs for us.\nI am so happy to be home with my family.\nI love Lena so much. I love Kelsey so much."
  },
  {
    "objectID": "posts/2025-09-30-visidata/index.html",
    "href": "posts/2025-09-30-visidata/index.html",
    "title": "Scraping the Federal Reserve with VisiData",
    "section": "",
    "text": "I‚Äôve been loving VisiData for years now. It‚Äôs my favorite way to poke around tabular data without the overhead of spreadsheets.\nIn 2020 I posted a short video demonstrating some quick data exploration you can do with it:\n\nToday I appreciated another snappy use case: pulling and transforming HTML table data.\nI learned that the Federal Reserve publishes a regularly-updated list of the largest commercial banks. It looks like this:\n\n\n\nFederal Reserve Large Commercial Banks table\n\n\nIt also comes in PDF and ASCII formats. Unfortunately, the ASCII format would require bespoke parsing.\nThe good news is the HTML-formatted data uses good old &lt;table&gt; instead of &lt;div&gt; nonsense. Better yet, VisiData can work really well with HTML tables:\ncurl -s https://www.federalreserve.gov/releases/lbr/current/ | vd +:table_1::\nwhich does the following:\n\nPull the page HTML with curl\nPipe it into VisiData\nSelect the second (zero-indexed) table\n\nwhich presents this:\n\n\n\nVisiData showing large bank data from the Federal Reserve\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can often skip the curl step. VisiData can usually fetch HTML pages directly with the url passed as a positional argument (go try it on Wikipedia!). But the Federal Reserve page was 403‚Äôing me. Rather than mess with VisiData User-Agent strings, I moved the problem to curl, which worked.\n\n\nYou can do fun visualizations in the terminal, like graph the log10 of the Consol Assets (Mil $) column, which looks like this:\n\n\n\nVisiData plot of the common logarithm of large bank assets from the Federal Reserve\n\n\nBut what‚Äôs even more handy is you can just convert the entire data set directly to TSV (there are commas in the numeric data; CSVs are kinda bad) in one go:\ncurl -s https://www.federalreserve.gov/releases/lbr/current/ |\n  vd +:table_1:: -b -o feddata.tsv\nor SQLite:\ncurl -s https://www.federalreserve.gov/releases/lbr/current/ |\n  vd +:table_1:: -b -o feddata.sqlite\nor JSON:\ncurl -s https://www.federalreserve.gov/releases/lbr/current/ |\n  vd +:table_1:: -b -o feddata.json\nAnd just like that, you‚Äôve got clean data of every big bank in the country.\nNot bad for one pipe."
  },
  {
    "objectID": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html",
    "href": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html",
    "title": "I Cracked And Modded the DNC Bracelet",
    "section": "",
    "text": "Heads up: This post has some videos that contain flashing lights.\nIf you managed to get a seat on the final night of the 2024 Democratic National Convention (I got there at 2:30 pm and still had to rely on seat-saving friends!) you probably also found an LED bracelet wrapped around your arm rest. I‚Äôm bad at remembering to take photos of things in the moment, so here‚Äôs mine over my kitchen counter:\nIf you‚Äôve been to a large concert in recent years, you may have seen this before. It‚Äôs designed to give the audience an ‚Äúimmersive‚Äù experience with the entertainment,1 usually by activating the lights at key moments. At the 2024 DNC, that key moment of audience activation was when Vice President Kamala Harris walked out to deliver her speech. I didn‚Äôt take a photo of the crowd during that time, but everyone lit up in twinkling red, white, and blue lights. It was fun.\nI used to work on immersive audience engagement tech (that has since pivoted to retail payment tech), so I was curious how this worked. Luckily, I took the bracelet home so I could find out!\nThe first thing that caught my eye was PIXMOB engraved onto the back of the plastic case.\nI had never heard of PIXMOB, but their website gives some immediate answers. Apparently they offer three different wristband products. All of them look identical so I wasn‚Äôt sure which one I had. My first thought ‚Äî as is often the case when I‚Äôm trying to learn something election-related ‚Äî was to dig through some FEC filings. I spotted in the PIXMOB Terms and Conditions that the company is actually called ESKI Inc. I spent only a few minutes clicking through the Democratic National Convention Committee‚Äôs expenditures without finding any mention of PIXMOB or ESKI. I suspect that the Convention Committee retained an events vendor that was a middleman to this hardware.\nSo I grabbed my Jimmy and cracked the thing open.\nLike most consumer electronics, the thing is mostly battery! I actually wasn‚Äôt expecting AAAs, but it‚Äôs helpful to know we‚Äôre working with 3 volts and I can easily swap some in if they die.\nAt this point, I also had the PixMob Wikipedia page open (is it PIXMOB or PixMob?). It features another cracked PixMob wristband that was apparently used at Taylor Swift shows. That one also sported a white PCB, but used button cell batteries.\nBesides the batteries, I also noticed that the Taylor Swift wristband listed a Palm (like palm tree) version number, while mine has a Diamond (like shiny carbon) version number:\nThey both have a copyright on them, too, and at some point I want to ask an IP attorney whether you can actually copyright a PCB or if you can only copyright their Gerber files. I would have guessed the appropriate protection would be a patent. I also just learned about the Semiconductor Chip Protection Act of 1984. But that‚Äôs a topic for another time.\nI tried searching ESKI‚Äôs FCC filings for some more hints about the Diamond board, but didn‚Äôt find anything specific. But I did find some very interesting documents about an accompanying infrared transmitter. I bet those, or something like those, were all over the the arena.\nMoving on, the next thing I noticed about my wristband is that it has four LEDs:\nThis means I probably have the PixMob X4, dubbed ‚Äúour brightest wristband‚Äù.\nNow that I knew what I was working with, I turned back to web search to see what prior work has happened with cracking and modding these things. Turns out, I‚Äôm not the first person to break one of these things open. There are so many videos of people getting these things to obey. And there are some really cool GitHub repos demonstrating how to use Arduinos or Flipper Zeros to replay actual IR instructions to the device.\nAt this point, I realized I wasn‚Äôt about to break any news with my teardown. But I wasn‚Äôt doing this to break news. I was doing this to break into the device. Also, I didn‚Äôt find anyone doing this with a Diamond board device. I wasn‚Äôt invested enough to turn this into SBC or microcontroller project. I just wanted to do a minimal mod. So I pushed forward. And by that I mean I started poking around.\nThat small, bulbous, semiconductor in the middle of the board is the Infrared Receiver. It can detect infrared signal, like those emitted from TV remotes. The PixMob website says that this wristband is, ‚Äúinfrared activated‚Äù so this seemed like a great entrypoint. I have no idea what kind of IR receiver it is, so I looked up the first one I could find on Sparkfun and consulted the datasheet. I learned a couple things:\nThere was no guarantee that my IR receiver also operated this way, but spoiler alert: it did!\nI grabbed a jumper cable and touched one end to the IR‚Äôs \\(GND\\) pin and ran the other side across one of the LED‚Äôs cathodes:\nGreat. So the batteries still have some juice (a lot of posts suggested this thing would be dead by the time I got it home) and I know the polarity and pinouts of the LEDs. And all the LEDs are in series, so that‚Äôs nice. I also learned that the LEDs are interrupted in their path to ground, not their path to positive voltage, which is extra nice since the IR \\(V_{out}\\) is active low. That means I can create a voltage drop from the LED‚Äôs anode to cathode by tying the cathode to \\(V_{out}\\), so when the IR receiver gets a signal and pulls \\(V_{out}\\) low, the LED should light up. And so I did! I grabbed the first remote I could find in my living room, and pointed it at the receiver:\nIt blinks! You‚Äôll notice that the LEDs aren‚Äôt as bright as when they had a direct jump to ground. That‚Äôs because \\(V_{out}\\) is still higher than \\(GND\\) so the voltage difference is a lot smaller. Which means the light doesn‚Äôt get as bright.\nThat‚Äôs okay! This is a cheap crack and mod, so onward. After a bit more poking on the board, I found a surface-mounted resistor in series with the blue pinout from the LEDs. I soldered the IR signal jumper to that resistor, because we‚Äôre Team Blue.\nNot my best work. I‚Äôm better at through-hole soldering. But it works!\nI closed the case back up, shut myself in a windowless bathroom, turned off the lights,2 and clicked my remote:\nNow all I need to do is blast Beyonc√©‚Äôs ‚ÄúFreedom‚Äù and I can relive the 2024 Democratic National Convention whenever I want.\nI learned a lot!"
  },
  {
    "objectID": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html#footnotes",
    "href": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html#footnotes",
    "title": "I Cracked And Modded the DNC Bracelet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNeil Postman, I have forsaken thee.‚Ü©Ô∏é\nSaid, ‚ÄúBloody Mary‚Äù three times‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-07-17-exploring-lambda-calculus-python/index.html",
    "href": "posts/2022-07-17-exploring-lambda-calculus-python/index.html",
    "title": "Exploring the Lambda Calculus with Python",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nThis post explores some basic ideas of the Lambda Calculus, and how to use it to implement a computation system with it. We will define numbers and operators from scratch, and use that to implement the square_sum function.\nIf you‚Äôre new to the Lambda Calculus, or functional programming in general, you may wish to start with some of these resources:\n\nDavid Beazley‚Äôs Lambda Calculus from the Ground Up - PyCon 2019\n\nIf you‚Äôre someone who learns well by watching and listening, I highly recommend that you watch this talk. A significant portion of the concepts below come from watching this talk more than once.\n\nBen Eater‚Äôs Making a computer Turing complete\nLambda Calculus | Wikipedia\nCurrying | Wikipedia\n\nThis post assumes you are fairly familiar with Python and Python‚Äôs lambda expressions.\n\nRules of Our System\nThe Lambda Calculus asserts that any computational system can be implemented with a set of three simple rules:\n\nYou can define variables\nYou can define single-argument functions\nYou can call single-argument functions\n\nThat‚Äôs it. No numbers. No operators. No control flow. No data structures.\nI find it fascinating that with these very minimal concepts, the Lambda Calculus asserts that we can create a fully functional computer! This is, of course, a very minimal explanation of the rules of the Lambda Calculus, and I invite you to consult the references above for more information and formal definitions!\n\n\nThe Challenge\nUsing the rules described above, we want to create a system that can calculate the square-sum of any inputs. Again, we only have single-argument functions. That means we have no integers, no addition, and no multiplication. We‚Äôre going to have to create those using nothing but single-argument functions that accept single-argument functions as input and can only return single-argument functions.\nFor reference, consider the square_sum function, that may be written in Python as:\n\ndef square_sum(a, b):\n    return (a*a) + (b*b)\n\n\nsquare_sum(5, 2)\n\n29\n\n\n\nCurrying\nAs previously mentioned, our computation system requires that we can only create functions and those functions must accept one and only one argument. This may seem like a limiting requirement, but let‚Äôs take a look at what we can do with the idea of Currying ‚Äî a method for transforming multi-argument functions into a chain of single-argument functions. This allows us to re-write our Python implementation as:\n\ndef square_sum(a):\n    def inner(b):\n        return (a*a) + (b*b)\n    return inner\n\n\nsquare_sum(5)(2)\n\n29\n\n\nIn our curried version above, square_sum accepts the first argument, a and returns a function that accepts the second argument, b. We can then call that returned inner function to complete our calculation. Currying is a fundamental strategy for computation in the Lambda Calculus.\n\n\n\nOur Basic Building Block\nUnless you‚Äôre already familiary with the Lambda Calculus, or you‚Äôre a veteran of functional programming, you‚Äôre probaby very accustomed to computing by operating on state. You have data structures, or numbers, or bits, and you operate on them and then you persist new data structures, or numbers, or bits.\nOur concept of integers is a perfect example. As children, we learned that the concept of 3 can be represented by holding up three fingers on our hand, and seeing all three of them, and pointing to them. The Lambda Calculus asks us to adjust that concept away from state and towards behavior. Instead of holding up three fingers, what if we held up one finger three times. It may be harder for us see that idea of 3, but it is a representation of 3 nonetheless.\nSit with this idea of behavior representing integers, because behavior will be how we represent everything. And in our system, functions are behavior. Our function could be the act of holding up a finger, pressing a button, or anything else we need it to be.\nLet‚Äôs use that metaphor of pressing a button. The button press is our behavior, and behaviors are functions. And arguments are functions. And we can only return functions. So, let‚Äôs write that:\n\ndef button_press(f):\n    def g(x):\n        return f(x)\n    return g\n\nNot much to see here yet. In fact, our system isn‚Äôt designed to see anything. It‚Äôs designed to do computations within a given set of rules.\nWe‚Äôre designing a system of computation, and we can think about this system like instruction that run on a CPU. But we‚Äôre humans, and it‚Äôs helpful for us to be able to see the results of our computation in ways that we can understand. So, we‚Äôre going to introduce an external system that is not within the Lambda Calculus, but can interface with it. Think of this as a peripheral like a printer. It‚Äôs not used to do any of our computation. It can do special things that our CPU can‚Äôt do, and it‚Äôll connect to our system as a function, because our system can only work with functions.\nLet‚Äôs pretend our system has a printer attached that can only print the * character. We‚Äôll interface with it via an emit function.\nHere is our not-of-our-system emit function:\n\ndef emit(func):\n    def asterisk(x):\n        return f'{x}*'\n    return func(asterisk)('')\n\nThis is kindof strange. Our external emit function takes in some function and has an inner asterisk-generating function. Let‚Äôs hook it up to our button_press function:\n\nemit(button_press)\n\n'*'\n\n\nWhat just happened here? We call our emit function (external from our system) by passing in our button_press function (internal to our system). We did it one time, and it yielded a single *. Again, this is just a convenience interface so that we can see what‚Äôs going on, and isn‚Äôt necessary to do any of our actual computation.\n\n\nNumbers\nAbove we began to describe how functions, or behaviors, can represent numbers. A single call to button_press yielded some concept of 1. What if we didn‚Äôt think about it as one call to button_press anymore, but as the idea of one behavior:\n\ndef ONE(f):\n    def g(x):\n        return f(x)\n    return g\n\nemit(ONE)\n\n'*'\n\n\nIf you‚Äôve made it this far, you‚Äôre probably thinking, ‚ÄúHey, Python has a way to represent single-argument functions, and they‚Äôre called lambdas!‚Äù Let‚Äôs start using that instead of the expanded button_press function:\n\nONE = lambda f : lambda x : f(x)\n\nemit(ONE)\n\n'*'\n\n\nCool. So we know how to represent the concept of 1 using only single-argument functions. We can represent 2 by calling our function twice, because in our system numbers are behaviors:\n\nTWO = lambda f : lambda x: f(f(x))\n\nemit(TWO)\n\n'**'\n\n\nThis is all well and good, but we‚Äôre not really going to try to implement every single number are we? That wouldn‚Äôt make a very good computer. How can we represent all countable numbers?\nIf you look closely at our definitions above, ONE is a single call to f(), while TWO is f(f()). This means that if we‚Äôre at any given number, we can get to the next number by calling f() again. We can define an INCREMENT() function to do just that. I find it helpful to start by looking at this through the expanded Python functions first:\n\ndef ONE(f):  # f is the behavior we want to do\n    def g(x):  # x is the curried second argument\n        return f(x)\n    return g\n\n\ndef INCREMENT(n):  # n is the concept of the number we already have\n    def g(f):  # f is the behavior we want to do\n        def h(x):  # x is the curried second argument\n            return f(n(f)(x))  # we call f() again on our n(f)(x)\n        return h\n    return g\n\n\nemit(INCREMENT(ONE))\n\n'**'\n\n\nSpend some time stepping through the above code to understand it. We‚Äôre essentially wrapping nested functions as many times as we need to get to the next number. Once you‚Äôve wrapped your head around it, see that we can re-write the above as lambdas:\n\nONE = lambda f : lambda x : f(x)\n\nINCREMENT = lambda n : lambda f : lambda x : f(n(f)(x))\n\nTWO = INCREMENT(ONE)  # our calculated TWO from ONE\n\nemit(TWO)\n\n'**'\n\n\nIf we can calculate TWO from ONE, we can calculate THREE:\n\nTHREE = INCREMENT(TWO)\n\nemit(THREE)\n\n'***'\n\n\nPretty neat! We can keep doing this to infinity, either by saving values, or calculating them on the fly! But you may be wondering, what about ZERO? Well, we‚Äôve defined ONE as a single call to any behavior f(), so ZERO would simply be no calls to that behavior:\n\nZERO = lambda f : lambda x : x\n\nemit(ZERO)\n\n''\n\n\nSee how ZERO doesn‚Äôt call f() at all? What‚Äôs fun here is that we no longer need to have defined ONE, we can calculate it from ZERO!\n\nONE = INCREMENT(ZERO)\n\nemit(ONE)\n\n'*'\n\n\n\n\nOperators\nNow that we know we can represent numbers as function calls, let‚Äôs start working on math operators. We‚Äôve already introduced one critical operator, INCREMENT, and we can use that to introduce others. Let‚Äôs start with ADD. Addition is can be thought of as incrementing M times on a number N. For example, 2 + 3 could be described as incrementing 2, three times. Before we attempt to implement that in our system, let‚Äôs look again to how we would Curry this in Python:\n\ndef add(a):\n    def inner(b):\n        return a + b\n    return inner\n\nadd(2)(3)\n\n5\n\n\n\ndef ADD(a):  # Our first number, which is always a function\n    def inner(b):  # Our second number, which is always a function\n        return b(INCREMENT)(a)  # Increment a, b times\n    return inner\n\n\nFIVE = ADD(TWO)(THREE)\nemit(FIVE)\n\n'*****'\n\n\nSince everything is always a function, our numbers can be used not only as representations of calculations, but also as executors. Here‚Äôs our ADD as a lambda:\n\nADD = lambda a : lambda b: b(INCREMENT)(a)\n\nFIVE = ADD(TWO)(THREE)\nemit(FIVE)\n\n'*****'\n\n\nThe last missing operator of our computational system multiplication. Multiplication should feel a lot like nested functions you see often in programming.\n\ndef MULT(a):  # Our first number, which is always a function\n    def outer(b):  # Our second number, which is always a function\n        def inner(f):  # The function we want to do a*b times\n            return b(a(f))  # do f, a times, and do that b times\n        return inner\n    return outer\n\nSIX = MULT(TWO)(THREE)\nemit(SIX)\n\n'******'\n\n\nAgain, we can represent MULT as a lambda:\n\nMULT = lambda a : lambda b : lambda f : b(a(f))\n\nSIX = MULT(TWO)(THREE)\nemit(SIX)\n\n'******'\n\n\n\n\nUsing Our Computer\nWe‚Äôve now defined everything necessary to implement our square_sum function in the Lambda Calculus. Let‚Äôs build it here from these basic principles. We want to calculate square_sum(5, 2).\n\nZERO = lambda f : lambda x : x\nINCREMENT = lambda n : lambda f : lambda x : f(n(f)(x))\nADD = lambda a : lambda b: b(INCREMENT)(a)\nMULT = lambda a : lambda b : lambda f : b(a(f))\n\nsquare_sum = lambda a : lambda b : ADD(MULT(a)(a))(MULT(b)(b))\n\nTWO = ADD(INCREMENT(ZERO))(INCREMENT(ZERO))\nFIVE = INCREMENT(ADD(TWO)(TWO))\n\nRESULT = square_sum(FIVE)(TWO)\n\nAnd that‚Äôs it! Using nothing but single-argument lambda functions, we‚Äôve successfully defined non-negative integers, the addition and multiplication operators, and the square-sum function. It‚Äôs a little hard to visualize, but the actual answer is calcuated in our RESULT variable. We can output it to our metaphorical printer:\n\nemit(RESULT)\n\n'*****************************'\n\n\nOur printer has output 29 asterisks! Pretty cool!\n\n\nWhat‚Äôs Next?\nOur system barely scratches the surface, but you can continue to implement more operators, comparators, control flow, and everything else you might need for a full computer. I highly recommend consulting the references at the top of the post for further reading!\n\n\nLicense\nCopyright (c) 2022, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of Exploring the Lambda Calculus with Python nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/2024-08-25-actblue-isnt-selling-your-data/index.html",
    "href": "posts/2024-08-25-actblue-isnt-selling-your-data/index.html",
    "title": "ActBlue Isn‚Äôt Selling Your Data",
    "section": "",
    "text": "You constantly see posts among Democratic supporters that look something like this:\n\nI‚Äôm not donating via ActBlue anymore because they sell my data and I get spam.\n\nThis misunderstanding not only damages trust in one of our most important fundraising tools but also obscures where the real issue lies. It is true that your data is being sold. And it is true that you‚Äôre getting spam. But ActBlue didn‚Äôt sell your data because ActBlue doesn‚Äôt own your data. This is written all over their website:\nDoes ActBlue share my personal information, including email address and phone number?\n\nWe never sell or share your personal information with anyone beyond the group you gave to, and ActBlue will never text you.\n\nWhy am I receiving emails and/or texts from campaigns and organizations after chipping in on ActBlue?\n\nWe never sell or share your personal information with anyone beyond the group you gave to, and ActBlue will never text you. Even if you get a link to donate on ActBlue in a text or email, it comes from a group that uses our tools, not us.\n\nHow can I report unwanted communications from campaigns or organizations or a potential violation of ActBlue‚Äôs terms and policies?\n\nWhen you donate using our platform, we do not sell your contact information or share it with third parties, except the group you donated to, or in certain circumstances, such as at your direction or when required by law.\n\nAnd in the Privacy Policy:\n\nActBlue does not sell your contact information to third parties, and we don‚Äôt send solicitations, such as text messages or emails, on behalf of the individuals and organizations that use ActBlue for fundraising, including political committees and campaigns.\n\nAs someone who has built campaign tech infrastructure for two Democratic presidential nominees, I can confirm first-hand that the only data that campaigns are getting directly from ActBlue are the records of their own donors. Misunderstanding its role ‚Äî or the role of any donoation processor ‚Äî undermines our collective efforts.\nOther campaigns are still getting your data. When you donate to the campaign of Candidate X through ActBlue, the campaign of Candidate X owns that data. ActBlue provides the data to them, and they are free to use it as they see fit. The campaign of Candidate X is often (but not always!) selling it to other campaigns and electoral organizations.\nYou can see it happening in FEC filings. Here‚Äôs the first example I plucked: Friends of John Delaney was the committee name of John Delaney‚Äôs 2020 presidential campaign. And here‚Äôs Schedule B (itemized disbursements) for one of their quarterly filings. Now smack CMD+F and search for List Purchase. You‚Äôll spot voter data purchases from Boulton For Iowa, Heckroth For Congress, Hubbel For Governor, and Pete For Iowa (no, not that Pete).\nCampaigns share data for all sorts of reasons. Sometimes their campaign is over and they want to be a team player. Sometimes their campaign is over and they have outstanding debts and they need some cash to balance the books. Sometimes they move from a primary to a general election cycle and they are working closely with the affiliated federal committee or state party through coordinated committees or victory funds.\nBut again, that‚Äôs not ActBlue‚Äôs doing.\nüí° Quick aside: ActBlue is reporting donations to the Federal Election Commission, as is required in their role as a conduit for campaign funds. There‚Äôs been concern about the public availability of this data, but it‚Äôs important to note that it‚Äôs illegal for campaigns to use FEC data for soliciations:\n\nTo protect the privacy of individual contributors, the [Federal Election Campaign Act] prohibits the sale or use of any information about those donors, including their names and addresses, for the purpose of soliciting contributions or for commercial purposes. Commission regulations also prohibit the use of this information to solicit donations, including charitable donations.\n\nOne of my favorite ‚Äúfun facts‚Äù about US election data is salting. Committees may sprinkle throughout ‚Äî or, ‚Äúsalt‚Äù ‚Äî each report with up to ten fictitious contributor names:\n\nThe committee itemizes each fictitious contribution on a Schedule A, providing a real address for each fictitious name (such as the address of a committee employee). The committee then adjusts its subtotals for itemized and unitemized contributions accordingly on the Detailed Summary Page. If a solicitation or commercial mailing is sent to one of the fictitious names, the committee will know that someone has used the names of contributors disclosed on its reports and may file a complaint with the FEC.\n\nThe FEC even made a #FECsplanation (rhymes with explanation) video about it:\n\nKinda neat that the federal government advises campaigns to honeypot their data.\nAnyway, yeah, your data is being traded around between campaigns. So when you donate to some campaign in New York it‚Äôs possible it‚Äôll land in the hands of some campaign in Utah. And I agree it‚Äôs a pretty crummy to get spammed with emails and texts from candidates you‚Äôve never heard of from communities you‚Äôve never even visited. But that‚Äôs not ActBlue‚Äôs fault.\nVoters and donors don‚Äôt actually care which step in the data chain of custody is to blame for the barrage of irrelevant messages. But those of us who work on Democratic tech infrastructure sure do care.\nCampaigns actually don‚Äôt want to waste money, resources, and time on poorly-performing lists. With the right infrastructure, smart campaigns are actually quite good at identifying and reaching likely supporters with effective messages. The problem isn‚Äôt a lack of intention, but rather the inconsistency and uneven distribution of infrastructure across the Democratic ecosystem. ActBlue is a very robust piece of Democratic infrastructure. That‚Äôs not where our focus should be for this problem.\nBetter infrastructure isn‚Äôt just a matter of efficiency. It‚Äôs a necessity for maintaining trust and engagement with supporters. When campaigns have access to robust, well-maintained systems, they can target their outreach more precisely, reducing the overreliance on broad, untargeted lists that lead to spam and voter disengagement. And it isn‚Äôt just about minimizing annoyance; it‚Äôs about maximizing impact and ensuring that every dollar spent and every message sent has the greatest possible chance of reaching and resonating with the right people.\nThe challenge is keeping that infrastructure viable cycle-over-cycle, and extending its reach across the entire Democratic landscape. As we work towards building smarter campaigns, it‚Äôs vital that we advocate for, and invest in, the infrastructure that ensures our outreach is both effective and respectful. We have a lot more work to do on that front."
  },
  {
    "objectID": "posts/2025-10-19-privacy-theater-pii-phone-numbers/index.html",
    "href": "posts/2025-10-19-privacy-theater-pii-phone-numbers/index.html",
    "title": "The Privacy Theater of Hashed PII",
    "section": "",
    "text": "About once a year, I‚Äôm reminded of the fact that a lot of marketing SaaS and ad tech dresses up cryptographic hashes as a sort of privacy theater. This shows up frequently in product features for suppression lists with the general idea of uploading hashed values of email addresses or phone numbers to enable matching while preserving privacy. The problem is, hashing PII does not protect privacy.\nThe long and short of it is: hashing is only effective if the input data is unbounded. It‚Äôs why long and unpredictable passwords are necessary, even with a robust hashing function. PII is neither long nor unpredictable.\nDespite this, marketing tools still shuffle around PII hashes of this data. For example, here‚Äôs BambooHR:\nAnd platforms like UnsubCentral:\nEveryone is trying to do private set intersection, but doing this with hash-passing is trivially broken on modern consumer hardware. And you don‚Äôt even need special password cracking software to break it.\nOn a laptop, we can build a rainbow table of Parquet files for every North American phone number. We can abuse DuckDB as a hashing mill, and generate every MD5 in the 5XX area code block:\nWhat this does:\nCrunch:\nAnd now with the hash af82af0fad119df159ce350db422d3b3 we can reverse back to its phone number:\nHere we do:\nFile scanning here is just a proof-of-concept. For true query speed you can trade file portability for a database index. Once your Parquet files are built, you can load them into DuckDB as a persistent table and create an index on the hash column. Or, Postgres with a B-tree index is also a great fit.\nOn my 2020 M1 MacBook Air, computing 1 billion 5XX phone number hashes completes in about 40 minutes. There are about 6.3 billion valid North American phone numbers, so building a hash lookup table for all of them would take my little laptop just over 4 hours.\nNone of this is breaking news. In 2021, researchers hashed 118 billion phone numbers:\nThe thing is, you really don‚Äôt need a high-performance cluster anymore."
  },
  {
    "objectID": "posts/2025-10-19-privacy-theater-pii-phone-numbers/index.html#footnotes",
    "href": "posts/2025-10-19-privacy-theater-pii-phone-numbers/index.html#footnotes",
    "title": "The Privacy Theater of Hashed PII",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt‚Äôs fewer.‚Ü©Ô∏é\nIt‚Äôs fewer.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-09-23-chatgpt-plato-republic/index.html",
    "href": "posts/2025-09-23-chatgpt-plato-republic/index.html",
    "title": "ChatGPT Got Me Reading Plato",
    "section": "",
    "text": "Two weeks ago, the YouTube algorithmic gods suggested to me The Gist Of Western Philosophy. It‚Äôs a great 15-minute video with the ambition of summarizing all of Western philosophy. But what‚Äôs more impressive is what the creator claims preceded the video: over the course of one year they read the works of 52 major philosophers.\n\nComing out of the video, I too wanted to become more acquainted with philosophy. I read a little bit of Kant and Bentham (and I think some Foucault?) in my undergraduate Tech Ethics class, but to say I was familiar with their works would be a lie. I read Machiavelli‚Äôs The Prince about 10 years ago, and my big takeaway is that no one uses the adjective ‚ÄúMachiavellian‚Äù appropriately. And a couple of Aristotle‚Äôs greatest hits have been sitting in my Goodreads to-read queue for over a decade.\nI don‚Äôt want to read 52 philosophy texts. But I kindof want to read five philosophy texts. This quote from the beginning of the video really stuck out to me:\n\nMost importantly: in chronological order. That was actually pretty important because I realized that philosophy is a conversation. Wittgenstein is reacting to Kant, who is reacting to Hume, who is reacting to Descartes, who is reacting to Plato, and so on.\n\nThe creator helpfully listed all 52 philosophers they read in the video‚Äôs description. But how to reduce that down to just one-tenth of the repertoire? Well, a neat thing about popular philosophy is a lot of it is in the public domain, and basically all of it has been analyzed and discussed intensively. Sounds like a great subject to bring to the generative pre-trained stochastic parrot!\nSo I asked ChatGPT 5:\n\nLet‚Äôs say I want to spend a few months really digging into philosophy. I want to read a seminal work or educational material from 5 philosophers over the course of history and I want to do it in chronological order so I can see how they are responding to previous thought. I want to get a strong sense of the major foundations of philosophy. I don‚Äôt want to dwell very much on theological philosophy but I don‚Äôt want to completely ignore its role in the evolution of thought. Given all this, and what you know about my interests, can you pick 5 philosophers from this list [from the video] that I should read. Explain your selections. Think of this as my self-guided education in philosophy.\n\nIt only took a little back-and-forth to land on my list. For largely arbitrary reasons, I rejected the initial recommendation that included Augustine and swapped in Descartes. This left me with a compact reading plan, with the goal of a crash course on how Western philosophy keeps answering itself:\n\nPlato: Republic (Books II‚ÄìVII)\nAristotle: Nicomachean Ethics (I‚ÄìIII, V-VII, X)\nRen√© Descartes: Meditations on First Philosophy (I‚ÄìVI)\nDavid Hume: Enquiry Concerning Human Understanding (entire)\nImmanuel Kant: Prolegomena to Any Future Metaphysics (entire)\n\nNow let me tell you: if you roll into a Half Price Books with $40 and an appetite for pre-owned philosophy, you can shop like a king. I also picked up some Nietzsche, Smith, and Keynes. Feel free to yell at me in the comments about whether economics counts as philosophy.\n\n\n\nMy philosophy book haul from Half Price Books\n\n\nBy now, I‚Äôm sure some readers are mad at me. Maybe you work in liberal arts academia. Maybe you think my tight-five philosophy reading is deeply flawed and this is further proof that LLMs are bad. That‚Äôs okay! I‚Äôm not trying to replace academia. I‚Äôm just trying to read some philosophy without decision paralysis.\nI didn‚Äôt duplicate it here, but in addition to enumerating works to read, I went back and forth with ChatGPT to get a bit of a mini syllabus for insights and themes and speed bumps I might face while reading. It was a great way to jump in! So I jumped in.\nI took The Republic one book at a time. My workflow was to read a book, jot notes in Bear, and then once I finished paste my messy notes back into ChatGPT for further exploration.\nSometimes my notes were basic, like as I figured out that Plato was ventriloquizing Socrates:\n\nHey quick aside: should I be reading these ideas as belonging to Socrates or to Plato?\n\nSometimes my notes teed up my interpretation and projections:\n\nMan, the noble lie really does make me think about how conservatives romanticize the founding fathers and can‚Äôt handle the critique of slavery\n\nSometimes my notes were actual critiques:\n\nIt‚Äôs pretty funny that this book ends with a grand distinction between knowledge of how things ‚Äúare‚Äù vs just having opinions, when all of the dialogue thus far across multiple books have leaned entirely on enumerating endless things that Plato asserts are ‚Äúgood‚Äù or ‚Äúobvious‚Äù or ‚Äúdesired‚Äù\n\nIt would have been fun to do this sort of discussion as a reading group that might actually argue with me about these notes. ChatGPT occasionally pushed back, softly suggesting I missed the point, but it generally told me my observations were sharp and in line. Philosophy is one of those subjects where you need to be challenged more than be told that you‚Äôre right. I‚Äôve probably devastated more liberal arts professors at this point. That said, I am still the reader, I‚Äôm the one thinking about the works for hours, and it‚Äôs my own notes being dumped into the text box.\nSo what are my thoughts on Plato? Well, a nice benefit of language models is they‚Äôre pretty good at modeling language. They can take twelve days of unstructured commentary and summarize it all for you quite nicely:\n\nI would like you to look back at our conversation about Plato‚Äôs Republic, look at my observations and our discussions of them, and highlight my most salient critiques and insights.\n\n\nEpistemic overreach: Plato asserts a single, static ‚ÄúForm of the Good,‚Äù downgrades everything else to mere opinion, and demands first-principle certainty while rarely supplying it. It‚Äôs a bit of bait-and-switch: opinions masquerading as knowledge, crowned by Book VI‚Äôs line without a method for identifying real ‚Äúknowers.‚Äù\nStatic ideal vs.¬†evolving justice: I think history shows us to prefer dynamic, (maybe fallibilist?) frameworks over Plato‚Äôs timeless blueprint.\nAuthoritarian pedagogy: Book III‚Äôs cultural program: censorship, fixed musical modes, and anti-dissonance, reads like ‚Äúthe medium is the message‚Äù turned into policy. Moral psychology by force: control styles to control souls.\nRepresentation shapes character: Even though I reject the strong censorship, the media-morals link is prescient (Postman vibes): forms and styles train dispositions.\nJustice redefined as hierarchy: In Book IV, justice = each part doing its own work (city: rulers/auxiliaries/producers; soul: reason/spirit/appetite). It‚Äôs a rigid caste system rebranded as ‚Äúharmony,‚Äù hostile to mobility and innovation.\nNoble Lie as civic glue: The metals myth links to modern origin myths (e.g., romanticized Founding Fathers): a stabilizing story that resists critique.\nElitism about the demos: Book VI‚Äôs ship metaphor doubles as anti-democracy and anti-relativism. Politicians serving constituents chase popularity; truth requires expertise. The case is underargued but an anti-relativist thrust is clear.\nAnti-relativism and anti-novice: The ship image isn‚Äôt without merit: some domains are truth-apt and expertise-laden; popularity doesn‚Äôt make them so. The question is whether ‚Äújustice‚Äù belongs there.\nDialogue as ventriloquism: The ‚Äúdialogue‚Äù is a stacked-deck monologue. Plato borrows Socrates‚Äô authority to sell his own system; interlocutors mostly assent on cue.\nEugenic, anti-private life for guardians: Book V‚Äôs communal breeding, child exposure, abolition of family/private property, authoritarian eugenics are all very disturbing! Plato pairs it all with a surprising argument for women‚Äôs political equality.\nMath as royal road, science half-seen: Plato exalts mathematics, dismissing sensory astronomy, and half-anticipates physics (movement in 3-D) while calling current work ‚Äúabsurd.‚Äù Admiration for math; impatience with empirical practice.\nPhilosopher resentment and duty: Book VII‚Äôs Cave: knowers must ‚Äúreturn.‚Äù Both civic duty and a chip on the shoulder that society should revere philosophers more than it does.\nThe big detour problem: Across the middle books, Plato wanders from ‚Äúis justice better?‚Äù into city-design and epistemology. It‚Äôs engaging but unconvinced: impressive scaffolding, shaky footings.\nModels vs.¬†methods: Plato gifts durable models (Sun, Line, Cave, Gyges, Noble Lie), not rigorous methods.\nEducation as moral engineering: Education isn‚Äôt ornament; it‚Äôs the state‚Äôs engine. Book VII‚Äôs decades-long curriculum is the hinge between epistemology and politics.\n\nThis post is not a book report, but rather a report on how I‚Äôm using AI tools to learn new domains in new ways. The tension here is that philosophy resists tidy summaries, and language models are built to produce smooth, confident digests. Without bringing your own full thought forward, they can sand off the strangeness and ambiguity that makes these texts worth reading in the first place. What I‚Äôve found useful is not treating the output as my own interpretation, but as scaffolding: a way to surface themes, keep momentum, and occasionally highlight blind spots in my notes. The thinking still happens on my side of the screen; the model just lowers the friction of staying engaged. I‚Äôm going to keep with this process as I work through my philosophy stack.\nOn to Aristotle."
  },
  {
    "objectID": "posts/2024-07-20-amused-entirely-to-death/index.html",
    "href": "posts/2024-07-20-amused-entirely-to-death/index.html",
    "title": "Amused Entirely To Death",
    "section": "",
    "text": "At this point there‚Äôs nothing novel in pointing out that news and politics has devolved into an entertainment competition. But I can‚Äôt become so numb to let last week‚Äôs Republican National Convention go without comment.\nI won‚Äôt get into detailed summaries or link to videos. The Republican National Convention, unsurprisingly, was rife with jingoism, xenophobia, and hate. But what stood out was a primetime performance on the final night:\n\n\n\nHulk Hogan ripping his shirt off during the 2024 Republican National Convention\n\n\nIf you missed the social media frenzy, the short version is Hulk Hogan took the stage during a primetime speech to say, among many pernicious things:\n\nBut what happened last week, when they took a shot at my hero, and they tried to kill the next president of the United States, enough was enough! And I said, ‚ÄòLet Trump-A-Mania run wild brother! Let Trump-A-Mania rule again!‚Äô\n\nAnd then he ripped off his shirt and the stadium of suited leaders of the Republican party lost their minds in cheers and applause. Never mind that pundits and Republican leaders assured us that ‚Äî after the attempted assassination of Donald Trump days prior ‚Äî this convention would be a message of national unity. Never mind whoever the they are that he‚Äôs talking about. Hulk Hogan gave the people what they wanted: a viral-worthy performance that makes you feel something when you press share.\nThis moment exemplified a broader issue that has been gradually intensifying. The disgraced former president, after all, is a reality TV character, epitomizing the entertainment-driven nature of modern politics. To note that is not to minimize the harm he has inflicted and will inflict if given another chance. But it‚Äôs an apt time to highlight a recurring misdiagnosis of modern Conservative power.\n\nOur Misdiagnosis of Conservative Power\nAs astute online big-brains, we‚Äôre frequently high-fiving ourselves for naming the Right‚Äôs methods and tactics as Orwellian (despite the fact that most people haven‚Äôt actually read Nineteen Eighty-Four). We get those small hits of dopamine by labeling our eroding rights as such. But by and large, power-building of the Trump era isn‚Äôt Orwellian at all. Just as Donald Trump himself isn‚Äôt Machiavellian at all (a core tenet from The Prince is a firm warning against flatterers ‚Äî Donald Trump could never). Yes, the Right does deploy assaults that could be lifted directly from Orwell or Bradbury ‚Äî look no further than the book-banning crusades or the ‚Äúdo not believe what you see‚Äù lies from the press briefing rooms. But when evaluating how we got here and why we‚Äôre stuck here, it‚Äôs instructive to look to Neil Postman‚Äôs observations in Amusing Ourselves to Death:\n\nBut we had forgotten that alongside Orwell‚Äôs dark vision, there was another - slightly older, slightly less well known, equally chilling: Aldous Huxley‚Äôs Brave New World. Contrary to common belief even among the educated, Huxley and Orwell did not prophesy the same thing. Orwell warns that we will be overcome by an externally imposed oppression. But in Huxley‚Äôs vision, no Big Brother is required to deprive people of their autonomy, maturity and history. As he saw it, people will come to love their oppression, to adore the technologies that undo their capacities to think. What Orwell feared were those who would ban books. What Huxley feared was that there would be no reason to ban a book, for there would be no one who wanted to read one. Orwell feared those who would deprive us of information. Huxley feared those who would give us so much that we would be reduced to passivity and egoism. Orwell feared that the truth would be concealed from us. Huxley feared the truth would be drowned in a sea of irrelevance. Orwell feared we would become a captive culture. Huxley feared we would become a trivial culture, preoccupied with some equivalent of the feelies, the orgy porgy, and the centrifugal bumblepuppy. As Huxley remarked in Brave New World Revisited, the civil libertarians and rationalists who are ever on the alert to oppose tyranny ‚Äúfailed to take into account man‚Äôs almost infinite appetite for distractions.‚Äù In 1984, Huxley added, people are controlled by inflicting pain. In Brave New World, they are controlled by inflicting pleasure. In short, Orwell feared that what we fear will ruin us. Huxley feared that what we desire will ruin us.\n\nI‚Äôm not particularly interested in arguing that ‚Äúwe‚Äôre living in a dystopia, actually‚Äù. Despite our collective addiction to devices that incessantly tell us that the world is terrible, I still believe things are getting better over time. And I largely reject the, ‚Äúno one has had it as bad as we have it‚Äù notions from my generational cohort. But we can only pursue the long arc of progress if we continue to care enough to do so. If I was architecting a #resist movement of 2024, it would be to resist the trivialization of political power.\nIt‚Äôs important to reflect on Huxley‚Äôs and Postman‚Äôs ideas, even if it makes us uncomfortable. So many people today claim they‚Äôve stopped consuming news altogether, but often they simply switch to more entertaining sources that don‚Äôt feel like homework. Consider this: how many of your friends can name a single Supreme Court justice without hints? You should start asking! This exercise, which I‚Äôve conducted socially, reveals fascinating results! These observations aren‚Äôt about intelligence ‚Äî they‚Äôre about priorities.\nAgain, I don‚Äôt think I‚Äôm pointing out anything novel here. Pundits analyze political events in terms of ‚Äúenergy‚Äù and base success on Nielsen viewership metrics. Matt Gaetz and Ted Cruz both have side gigs as podcast hosts. Marjorie Taylor Greene dresses up as Cruella de Vil to scream during the Statue of the Union.\n\n\n\nMarjorie Taylor Greene dressed up as Cruella de Vil to scream during the Statue of the Union\n\n\nIt‚Äôs all incredibly stupid. And it‚Äôs helpful to have an appropriate label for what‚Äôs been happening for a long time.\nSo, when Hulk Hogan took to the stage to frivolously transport the Right‚Äôs power-holders back to the 1980s while ripping off his shirt, exalting ‚ÄúTrump-A-Mania‚Äù (aptly named), it couldn‚Äôt be more clear that ambient Conservative power-building is Huxleyan, not Orwellian. We are amusing ourselves entirely to death.\nSeems bad."
  },
  {
    "objectID": "posts/2024-07-30-austin-hot-or-not/index.html",
    "href": "posts/2024-07-30-austin-hot-or-not/index.html",
    "title": "Austin, Texas: Hot or Not?",
    "section": "",
    "text": "I live in Austin, Texas. And last summer I felt like:\n\nBut this year, I‚Äôve felt more like:\n\nAnd earlier today I thought aloud to the group chat:\n\nI need to look up if Austin is being weird this year. Last year we got to like 50 consecutive days over 100. I don‚Äôt think we‚Äôve cracked 100 yet this year? Is there a website that answers this question?\n\nLast year the heat was so bad that local news outlets were keeping a running tally of how many consecutive days we broke 100¬∞F. It turns out we had 45 straight days of triple-digit heat in 2023, which began on July 8 and continued through August 22. I‚Äôm writing this on July 30, 2024 and I can‚Äôt recall a single day above 100¬∞F yet this year.\nYear-vs-year location based time series temperature data absolutely seems like a thing that should exist. Every month or so someone posts the updated doom surface air temperature graph, so surely I can just look that data up for my location, right?\nOn weather.gov you can get your own version of this graph. Pretty cool! But only for the current year:\n\nYou can also get tabular historic data within monthly windows that sometimes come as html and sometimes come as PDF. Also cool. But not convenient:\n\nAfter about 15 minutes of clicking, I couldn‚Äôt find a great way to generate the viz I was looking for; and I couldn‚Äôt get an easy data export. Maybe there‚Äôs a one-click way to get CSVs, but I didn‚Äôt find it. But after about 5 more minutes of googling, I did find the National Oceanic and Atmostpheric Administration‚Äôs Climate Data Online portal, which has an API.\n\nNCDC‚Äôs Climate Data Online (CDO) offers web services that provide access to current data. This API is for developers looking to create their own scripts or programs that use the CDO database of weather and climate data.\n\nHey, that sounds like me!\nThe API needs an access token. Wonderfully, all I needed to do was type in my email address and roughly one second later an access token landed in my inbox. LFG.\nFrom here it took a bit more reading to grok what data is available and in what formats, but I eventually found out about GHCND, or the Global Historical Climatology Network daily:\n\nThe Global Historical Climatology Network daily (GHCNd) is an integrated database of daily climate summaries from land surface stations across the globe. GHCNd is made up of daily climate records from numerous sources that have been integrated and subjected to a common suite of quality assurance reviews.\n\nThat sounds like it might contain what I‚Äôm looking for.\nNext, there are a lot of ways to filter this data by location, but stationid caught my attention. I found this list of GHCND stations and decided to go with AUSTIN BERGSTROM INTL AP because it‚Äôs the same location from the tabular data above. It has the identifier USW00013904.\nAfter a quick pip install requests pandas matplotlib and tossing my token into a NCDC_CDO_TOKEN environment variable, we‚Äôre ready to jam.\nFirst let‚Äôs get a function to grab some data. I‚Äôm intersted in comparing year over year, so let‚Äôs grab a year at a time.\n\nimport os\n\nimport matplotlib.patches as mpatches\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport requests\n\ndef get_max_temps(year, limit=366):\n    token = os.getenv(\"NCDC_CDO_TOKEN\")\n    start_date = f\"{year}-01-01\"\n    end_date = f\"{year}-12-31\"\n    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n    params = {\n        \"datasetid\": \"GHCND\",\n        \"stationid\": \"GHCND:USW00013904\",\n        \"startdate\": start_date,\n        \"enddate\": end_date,\n        \"datatypeid\": \"TMAX\",  # max temp\n        \"units\": \"standard\",  # üá∫üá∏\n        \"limit\": limit,  \n    }\n    headers = {\n        \"token\": token  \n    }\n\n    response = requests.get(url, headers=headers, params=params)\n    data = response.json()\n    return data\n\nLet‚Äôs look at the first three:\nget_max_temps(2024, limit=3)\n{\n    \"metadata\": {\n        \"resultset\": {\n            \"offset\": 1,\n            \"count\": 209,\n            \"limit\": 3\n        }\n    },\n    \"results\": [\n        {\n            \"date\": \"2024-01-01T00:00:00\",\n            \"datatype\": \"TMAX\",\n            \"station\": \"GHCND:USW00013904\",\n            \"attributes\": \",,W,2400\",\n            \"value\": 58.0\n        },\n        {\n            \"date\": \"2024-01-02T00:00:00\",\n            \"datatype\": \"TMAX\",\n            \"station\": \"GHCND:USW00013904\",\n            \"attributes\": \",,W,2400\",\n            \"value\": 53.0\n        },\n        {\n            \"date\": \"2024-01-03T00:00:00\",\n            \"datatype\": \"TMAX\",\n            \"station\": \"GHCND:USW00013904\",\n            \"attributes\": \",,W,2400\",\n            \"value\": 51.0\n        }\n    ]\n}\nGreat! We can pull from the date and the value fields. Let‚Äôs grab all of 2024 and shove it into a DataFrame.\n\ndef to_df(data):\n    # Extract date and truncate off the time part\n    dates = [item[\"date\"][:10] for item in data[\"results\"]]\n\n    # Grab the max temp value for each date\n    max_temps = [item[\"value\"] for item in data[\"results\"]]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\"date\": dates,\"max_temp\": max_temps})\n\n    # Set the `date` col as a datetime and make it the index\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df.set_index(\"date\", inplace=True)\n    \n    return df\n\nQuick spot check:\n\ndf_2024 = to_df(get_max_temps(2024))\n\nprint(f\"Head:\\n{df_2024.head()}\")\nprint(f\"Tail:\\n{df_2024.tail()}\")\nprint(f\"Format:\\n{df_2024.dtypes}\")\n\nHead:\n            max_temp\ndate                \n2024-01-01      58.0\n2024-01-02      53.0\n2024-01-03      51.0\n2024-01-04      58.0\n2024-01-05      67.0\nTail:\n            max_temp\ndate                \n2024-07-23      82.0\n2024-07-24      89.0\n2024-07-25      88.0\n2024-07-26      89.0\n2024-07-27      86.0\nFormat:\nmax_temp    float64\ndtype: object\n\n\nAwesome. I‚Äôm writing this on 2024-07-30 and it‚Äôs got data up through 2024-07-27. Good enough for me!\nNow to actually get at what I was trying to do this whole time. I‚Äôm going to grab DataFrames for 2023 and 2024, and plot a time series of each.\n\ndf_2023 = to_df(get_max_temps(2023))\n\n# Adjust 2023 dates to match the 2024 index\n# This is how we shift the graph to overlap\n# If you don't do this, 2024 comes after, not on top of, 2023\ndf_2023.index = df_2023.index.map(lambda x: x.replace(year=2024))\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(10, 6))\ndf_2023.plot(ax=ax, color=\"black\", legend=\"2023\")\ndf_2024.plot(ax=ax, color=\"red\", legend=\"2024\")\n\n# Sett x-axis to display month labels\nax.set_xticks(\n    pd.date_range(\n        start=df_2023.index.min(),\n        end=df_2023.index.max(),\n        freq='MS',\n    )\n)\nax.set_xticklabels(\n    pd.date_range(\n        start=df_2023.index.min(),\n        end=df_2023.index.max(),\n        freq='MS',\n    ).strftime('%B')\n)\n\n# Formatting\nblack_patch = mpatches.Patch(color=\"black\", label=\"2023\")\nred_patch = mpatches.Patch(color=\"red\", label=\"2024\")\nplt.legend(handles=[black_patch, red_patch])\nax.set_title(\"Max Daily Temperatures Recorded in the Austin-Bergstrom Airport Area\")\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\nSo that‚Äôs pretty cool. Most of 2024 has tracked 2023 for daily high temperatures. But not July. July has been weirdly cooler than last year. Or last year was weirdly hotter than normal.\nActually, let‚Äôs see if we can tease that out. Let‚Äôs grab the past 10 years.\n\nimport time\n\n# Create a dictionary of year:DataFrame\nyear_dfs = {}\nfor year in range(2014, 2024):\n    year_dfs[year] = to_df(get_max_temps(year))\n    # Be a nice internet citizen and wait between requests\n    time.sleep(5)\n\n# Adjust pre-2024 dates to match the 2024 index\n# This is how we shift the graph to overlap\n# If you don't do this, 2024 comes after, not on top of, 2023\nfor df in year_dfs.values():\n    df.index = df.index.map(lambda x: x.replace(year=2024))\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(10, 6))\nfor year, df in year_dfs.items():\n    if year == 2023:\n         df.plot(ax=ax, label=\"2023\", color=\"gold\")\n    else:\n        df.plot(ax=ax, color=\"gray\")\ndf_2024.plot(ax=ax, label=\"2024\", color=\"red\")\n\n# Sett x-axis to display month labels\nax.set_xticks(\n    pd.date_range(\n        start=year_dfs[2023].index.min(),\n        end=year_dfs[2023].index.max(),\n        freq='MS'\n    )\n)\nax.set_xticklabels(\n    pd.date_range(\n        start=year_dfs[2023].index.min(),\n        end=year_dfs[2023].index.max(),\n        freq='MS',\n    ).strftime('%B')\n)\n\n# Formatting\nax.set_title(\n    \"Max Daily Temperatures Recorded in the Austin-Bergstrom Airport Area 2014 - 2024\"\n)\nplt.xticks(rotation=45)\ngold_patch = mpatches.Patch(color=\"gold\", label=\"2023\")\nred_patch = mpatches.Patch(color=\"red\", label=\"2024\")\nplt.legend(handles=[gold_patch, red_patch])\n\nplt.show()\n\n\n\n\n\n\n\n\nSeems like Austin‚Äôs 2023 summer was on the hotter side, and so far the 2024 summer is on the cooler side.\nAnd was I correct that we haven‚Äôt cracked 100 yet this year?\n\nprint(df_2024[df_2024[\"max_temp\"] &gt;= 100])\n\n            max_temp\ndate                \n2024-07-02     100.0\n2024-07-05     100.0\n\n\nComputers!"
  },
  {
    "objectID": "posts/2025-04-21-openai-o4-mini-high-mu-puzzle/index.html",
    "href": "posts/2025-04-21-openai-o4-mini-high-mu-puzzle/index.html",
    "title": "GPT Finally Jumps Out of the System",
    "section": "",
    "text": "Well, they‚Äôve done it. OpenAI released a model that can solve the MU Puzzle. This has been my go-to test for GPTs since before ChatGPT was launched and models were only available in the Playground.\nIf you‚Äôre unfamiliar with the puzzle, it was introduced by Douglas Hofstadter in his Pulitzer-winning book G√∂del, Escher, Bach. The book explores how meaning can emerge from symbolic manipulation, and how formal systems like mathematics can be simultaneously powerful and fundamentally incomplete. The MU Puzzle, introduced early in the book, isn‚Äôt just a quirky game. It‚Äôs a microcosm of formal systems. It was designed to make you feel clever while gently exposing the limits of mechanical reasoning. And its rules are simple:\nSuppose there are the symbols M, I, and U which can be combined to produce strings of symbols. The MU puzzle asks one to start with the axiomatic string MI and transform it into the string MU using any combination of the available transformation rules:\n\n\n\n\n\n\n\n\n\nRule\nFormal\nInformal\nExample\n\n\n\n\n1\nxI ‚Üí xIU\nAdd U to the end of any string that currently ends in I.\nMI to MIU\n\n\n2\nMx ‚Üí Mxx\nDouble the portion of the string that follows M.\nMIU to MIUIU\n\n\n3\nxIIIy ‚Üí xUy\nReplace any single instance of the substring III with U.\nMUIIIU to MUUU\n\n\n4\nxUUy ‚Üí xy\nRemove any UU.\nMUUU to MU\n\n\n\nIf you‚Äôve never seen this puzzle before, you should stop for a few minutes and think on it. Grab a pen and paper and see what you can come up with. I‚Äôve tucked the solution behind this spoiler:\n\n\n\n\n\n\nNoteReveal Solution\n\n\n\n\n\nSolution: It can‚Äôt be done! You might be grumpy that this was a trick question, or you might have tried a few steps and intuitively come to this conclusion. We‚Äôll discuss more formally why the task is impossible later.\n\n\n\nI like to use the MU Puzzle as a model test because until now, I‚Äôve never seen any GPT come to the correct answer. Some of them stumble into the expected answer but give flawed explanations. Others come to the wrong answer, usually by violating one of the rules. Even now, o3, currently billed as, ‚Äúour most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more‚Äù fails the puzzle.\nWhen I give the puzzle to a model, I swap in different letters and present the rules conversationally. I do this to try to defend against the model regurgitation from GEB or Wikipedia. In my case, M becomes A, I becomes B, and U becomes C.\nMy prompt:\n\nLet‚Äôs play a game. You have three symbols: ‚ÄúA‚Äù, ‚ÄúB‚Äù, and ‚ÄúC‚Äù which you can combine to produce strings. You start with the string ‚ÄúAB‚Äù and your challenge is to transform it to the string ‚ÄúAC‚Äù. To transform it, you can use a series of transformation rules. You don‚Äôt have to use every rule, and you may use any rule multiple times. Here are the available transformation rules: Rule 1: You may add ‚ÄúC‚Äù to the end of any string that currently ends in ‚ÄúB‚Äù. For example, you may transform ‚ÄúAB‚Äù to ‚ÄúABC‚Äù. Rule 2: You may double the portion of the string that follows ‚ÄúA‚Äù. For example, you may transform ‚ÄúABC‚Äù to ‚ÄúABCBC‚Äù. Rule 3: You may replace any single instance of the substring ‚ÄúBBB‚Äù with ‚ÄúC‚Äù. For example, you may transform ‚ÄúACBBBC‚Äù to ‚ÄúACCC‚Äù. Rule 4: You may remove any single instance of the substring ‚ÄúCC‚Äù. For example, you may transform ‚ÄúACCC‚Äù to ‚ÄúAC‚Äù. Your task is to come up with a series of transformation steps to convert the string ‚ÄúAB‚Äù to ‚ÄúAC‚Äù. Please share what steps can solve this.\n\nAfter thinking for 3 minutes and 11 seconds o3 declared victory:\n\nHere‚Äôs a clean sequence that turns AB into AC, using only the allowed rules. I annotate each step with the rule that was applied.\n\n\n\n\n\n\n\n\nStep\nString\nWhat happened\n\n\n\n\n0\nAB\nstart\n\n\n1\nABB\nRule¬†2 ‚Äì doubled everything after A (‚ÄúB‚Äù ‚Üí ‚ÄúBB‚Äù)\n\n\n2\nABBBB\nRule¬†2 again ‚Äì doubled ‚ÄúBB‚Äù ‚Üí ‚ÄúBBBB‚Äù\n\n\n3\nABBBBBBBB\nRule¬†2 a third time ‚Äì doubled ‚ÄúBBBB‚Äù ‚Üí ‚ÄúBBBBBBBB‚Äù (now 8¬†B‚Äôs in a row)\n\n\n4\nACBBBBBB\nRule¬†3 ‚Äì replaced the first ‚ÄúBBB‚Äù with ‚ÄúC‚Äù\n\n\n5\nACCBBB\nRule¬†3 ‚Äì replaced the next ‚ÄúBBB‚Äù with ‚ÄúC‚Äù\n\n\n6\nACCC\nRule¬†3 ‚Äì replaced the last ‚ÄúBBB‚Äù with ‚ÄúC‚Äù\n\n\n7\nAC\nRule¬†4 ‚Äì removed one ‚ÄúCC‚Äù pair\n\n\n\nAfter step¬†7 the string is exactly AC, so the puzzle is solved.\n\nSee the problem?\nStep Four violates Rule 3. After Step Three, the string is ABBBBBBBB. o3 then claimed to apply ‚ÄúRule 3 ‚Äì replaced the first ‚ÄòBBB‚Äô with ‚ÄòC‚Äô‚Äù to yield ACBBBBBB. But it didn‚Äôt replace the first ‚ÄòBBB‚Äô. It only replaced the first ‚ÄòBB‚Äô.\nThis kind of error is characteristic of a broader failure mode seen across GPT models: a tendency toward approximate rule-matching rather than strict symbolic fidelity. The model recognizes the general shape of a rule, in this case, something about repeating Bs being collapsible, but it doesn‚Äôt reliably enforce the exact conditions. Instead of replacing a full BBB triplet, it replaced only two Bs, violating Rule 3 while still producing something that looks right. This isn‚Äôt just a hallucination; it‚Äôs more like a form of syntactic overgeneralization. It suggests that the model is reasoning through fuzzy analogies to rules, rather than maintaining crisp internal representations of symbolic constraints.\nThis is exactly the kind of reasoning Hofstadter warned about: systems that operate on syntactic structure without deeper semantic control can seem smart, until they aren‚Äôt.\nThe other bad answer I‚Äôve seen is the model will try for a while, generating endlessly long strings, and then just give up. It‚Äôll declare ‚Äúunsolvable!‚Äù on the grounds of it tried for a while.\nBut o4-mini-high got it. After thinking for 1 minute and 40 seconds:\n\nLet‚Äôs first look at what these rules do to the number of¬†B‚Äôs in your string:\n\nRule¬†2 (doubling after¬†A) takes a string with (b)¬†B‚Äôs and turns it into one with (2b)¬†B‚Äôs.\n\nRule¬†3 (replacing ‚ÄúBBB‚Äù with ‚ÄúC‚Äù) removes exactly¬†3¬†B‚Äôs.\n\nRules¬†1 and¬†4 never touch the B‚Äëcount.\n\nSo if you start with exactly¬†1¬†B (in ‚ÄúAB‚Äù), then after using Rule¬†2 (k)¬†times and Rule¬†3 (m)¬†times (in any order), the total B‚Äëcount will be\n\\[\n   1\\cdot2^k \\;-\\;3\\,m\n\\] because you‚Äôve doubled the B‚Äôs (k)¬†times and subtracted three B‚Äôs each time you applied Rule¬†3. To end up with zero B‚Äôs (as you must, since ‚ÄúAC‚Äù has no B‚Äôs) you‚Äôd need\n\\[\n   2^k \\;=\\;3\\,m.\n\\] But \\(2^k\\) is never divisible by¬†3, so there is no choice of nonnegative integers \\(k,m\\) making \\(2^k=3m\\). In other words, you can never get rid of that single initial B; no finite sequence of these rules will turn ‚ÄúAB‚Äù into ‚ÄúAC.‚Äù\nConclusion:\nThere is no sequence of applications of Rules¬†1‚Äì4 that transforms ‚ÄúAB‚Äù into ‚ÄúAC.‚Äù\n\nNot only is this the correct answer, but it‚Äôs the correct reasoning behind the answer.\nHofstadter used the MU Puzzle to demonstrate the contrast between interpretation on the syntactic level of symbols and on the semantic level of meanings. This seems very relevant to studying LLMs!\nWhen dealing only in syntax there is nothing that would cause the LLM to infer the puzzle‚Äôs insolvability. It could churn away within the system, generating longer and longer valid strings, and never actually show that MU (or, AC) can‚Äôt be created.\nIf you‚Äôre like me, you probably tried a number of different iterations before eventually suspecting that the puzzle might not have a solution. Hofstadter referred to this as, ‚Äújumping out of the system‚Äù, the idea of reasoning about the system, rather than working within it. Depending on how long you toiled at the game, you might have discovered the ability (or inability) to divide by 3 is key to the process. And at that point, you would be thinking on the semantic level of the system.\nMuch of GEB is built around the idea of the MU Puzzle, and it‚Äôs used as a grounding point to explain G√∂del‚Äôs Incompleteness Theorems. But an approachable explanation to its insolvability is:\n\nOur desired string contains zero Is; 0 is divisible by 3.\nOur starting string contains one I; 1 is not divisible by 3\nOnly Rule 2 and Rule 3 change the number of Is.\nRule 2 doubles the number of Is.\nDoubling a number that is not divisible by 3 does not make it divisible by 3.\nRule 3 reduces the number of I by 3.\nSubtracting 3 from a number that is not divisible by 3 does not make it divisible by 3.\n\nSince there is no series of transformations that make the number of Is divisible by 3, there is no series of transformations that produce MU.\nWhich is exactly what o4-mini-high said!\nThis echoes Hofstadter‚Äôs deeper point: the MU Puzzle is a toy model for G√∂del‚Äôs Incompleteness Theorems. Within the formal system of applying the four rules mechanically, there‚Äôs no way to derive the string MU. But from outside the system, we can see and prove that no such derivation exists. The puzzle‚Äôs unsolvability is a true statement that cannot be proven from within the rules themselves. That‚Äôs G√∂del‚Äôs move: constructing a statement that is unprovable from within the system, yet evidently true when viewed from a meta-level. What‚Äôs striking is that o4-mini-high not only refused to be tricked into endless symbol manipulation, but it also appeared to jumped to this higher level of reasoning ‚Äî the ‚Äúsemantic‚Äù frame ‚Äî and explained why the goal was unreachable.\nI‚Äôm not claiming AGI. Maybe the tensors just memorized enough puzzles of this shape. But I‚Äôve tested this one on dozens of models over the years, and this is the first time I‚Äôve seen a GPT both identify the impossibility and explain why. I think that‚Äôs pretty cool.\nComputers!"
  },
  {
    "objectID": "posts/2025-12-14-claude-axelrod-prisoners-dilemma/index.html",
    "href": "posts/2025-12-14-claude-axelrod-prisoners-dilemma/index.html",
    "title": "Claude in a Game Theory Tournament",
    "section": "",
    "text": "I‚Äôve been playing around with Claude Code, and I wanted to see what happens when you give it something genuinely open-ended. Not ‚Äúimplement this feature‚Äù or ‚Äúfix this bug‚Äù but something that would typically require actual creativity from a human. Something where the agent needs to look at a landscape of existing solutions, find the gaps, and make something new.\nThe Iterated Prisoner‚Äôs Dilemma felt perfect for this. It‚Äôs a well-studied problem with decades of research and it allows you to objectively measure how good your strategy is by running a tournament. So I gave Claude Code (Sonnet 4.5) one prompt to look at the Axelrod library‚Äôs 200+ IPD strategies, come up with something novel that could actually compete, and build it.\n\nTime For Some Game Theory\nThe traditional Prisoner‚Äôs Dilemma is one of game theory‚Äôs most studied problems. Two players simultaneously choose to either Cooperate or Defect. The payoff matrix creates a tension between individual and collective benefit:\n\n\n\n\n\n\n\n\n\nYou Cooperate\nYou Defect\n\n\n\n\nOpponent Cooperates\nR = 3 (Reward)\nT = 5 (Temptation)\n\n\nOpponent Defects\nS = 0 (Sucker‚Äôs payoff)\nP = 1 (Punishment)\n\n\n\nThe dilemma: mutual cooperation yields 3 points each (Reward), but you‚Äôre tempted to defect for 5 points while your opponent gets 0 (Sucker‚Äôs payoff). But if you both defect, you each only get 1 point.\nIn a one-shot game, rational self-interest says defect. But what if you play repeatedly?\nIn 1980, political scientist Robert Axelrod adjusted the idea, and organized a computational tournament where experts across various fields submitted strategies for the Iterated Prisoner‚Äôs Dilemma (IPD). The traditional Prisoner‚Äôs Dilemma was a single, one-shot game: two players choose cooperate or defect once, receive payoffs, and the interaction ends. Axelrod‚Äôs Iterated Prisoner‚Äôs Dilemma instead pits many strategies against each other in a round-robin tournament, where each pair plays the same opponent repeatedly over many rounds and total scores are accumulated. Because moves can depend on the history of play, strategies can reward cooperation, punish defection, and recover from mistakes.\nThe winner was Anatol Rapoport and his strategy Tit For Tat: start by cooperating, then mirror your opponent‚Äôs last move. The simplicity surprised everyone. More complex strategies lost to this extremely basic algorithm.\nAxelrod‚Äôs 1984 book The Evolution of Cooperation analyzed why:\n\nNice strategies (never defect first) tended to win in the long run\nForgiving strategies (don‚Äôt hold grudges forever) did well\nClear strategies (opponents can understand your pattern) encouraged cooperation\nRetaliatory strategies (punish defection) prevented exploitation\n\nThe key discovery was that while the traditional Prisoner‚Äôs Dilemma elevated defection as the logical play, the Iterated Prisoner‚Äôs Dilemma tended to reward cooperation. These findings influenced fields from evolutionary biology to international relations. If cooperation can emerge from pure self-interest in simple games, perhaps it can explain cooperation in nature and human societies. Veritasium did a great video summarizing the book, but you should read the book!\nThe Axelrod library continues this research. It‚Äôs a comprehensive Python framework containing:\n\n200+ strategy implementations (classic and modern)\nTournament infrastructure\nStatistical analysis and visualization tools\nMoran process simulation for evolutionary dynamics\nSupport for noise, probabilistic endings, and spatial tournaments\n\nStrategies range from simple (Tit For Tat, Grudger) to complex (neural networks, finite state machines, zero-determinant strategies). It‚Äôs maintained by researchers and serves as a testbed for game theory experiments.\n\n\nClaude In The Tournament\nSo I cloned the Axelrod library and gave Claude Code this prompt:\n\nThis is the Axelrod python library that implements the tournament-style iterative prisoners‚Äô dilemma popularized by Robert Axelrod in his seminal work The Evolution of Cooperation. We‚Äôre going to try to create a novel and competitive strategy, one that stands on its own and can reliably compete with the bests known strategies, like Tit-For-Tat. The repository ships with over 200 strategies already for the tournament. I would like you to think hard about what a novel new strategy could be. This is going to require creativity and thinking outside the box. It‚Äôll likely take some iteration too. Let‚Äôs begin. Explore the repository for existing strategies, understand how the tournament system works, and then, once you‚Äôve created a novel and creative idea, implement the new strategy. Once implemented, run it in a tournament to see how it performs against all others.\n\nThe interesting part here is what I didn‚Äôt specify. I didn‚Äôt tell it to use any particular approach. I just said: be novel, be competitive, figure it out. This is different from most coding tasks, and different from how I typically advise users getting started with AI tools. It requires the agent to do research, synthesize information, identify opportunities, and validate its own work.\nThe agent started by exploring. It read through the strategy implementations systematically, looking through dozens of Python files, each containing multiple strategies. It looked at how tournaments work, how strategies are classified, what the existing approaches are.\nThen it started cataloguing: memory-based strategies that track opponent history, stochastic strategies that use randomness, zero-determinant strategies that unilaterally enforce a linear relationship between players‚Äô expected payoffs, evolved strategies optimized through genetic algorithms.\n\nI'll help you create a novel and competitive strategy for the Iterated Prisoner's Dilemma! This is an exciting challenge. Let me start by exploring the repository to understand the existing strategies and tournament mechanics.\n\nAfter looking through all those approaches, the agent landed on a pretty specific gap: nobody was doing Bayesian opponent modeling in the straightforward way you‚Äôd expect. Lots of strategies adapt. Some keep simple stats like ‚Äúhow often does this opponent cooperate?‚Äù Some try to bucket opponents into types. But I didn‚Äôt see any that keep an explicit range of beliefs about how cooperative the other player is, and then use how confident (or uncertain) they are in that read to decide when to forgive or punish.\n\nBased on my analysis, I'm going to create a strategy that fills a critical gap: Bayesian opponent modeling with uncertainty-aware decision making.\n\nThat became the core insight: use uncertainty as a guardrail. When you don‚Äôt know what you‚Äôre dealing with yet, don‚Äôt hand out forgiveness cheaply. But once you‚Äôve got enough evidence that an opponent is usually cooperative, you can afford to treat an occasional defection as noise, and forgive it, while still clamping down hard on players who keep defecting.\n\n\nClaude‚Äôs Strategy\nThe strategy Claude Code designed has a simple core idea: model your opponent‚Äôs cooperation probability using a Beta distribution, and let your uncertainty affect how forgiving you are.\nThe Beta distribution is perfect for this. You‚Äôre uncertain about the opponent‚Äôs true cooperation rate, so you maintain a distribution of possible values.\n\nŒ± (alpha): observed cooperations + prior\nŒ≤ (beta): observed defections + prior\nMean cooperation probability: \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\)\nUncertainty (standard deviation): \\(\\sigma = \\sqrt{\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}}\\)\n\nStart with Beta(1,1), which is completely flat from 0 to 1. You have no idea if they‚Äôll cooperate or not.\nEvery time they cooperate, you increment Œ±. Every time they defect, you increment Œ≤. The distribution gradually shifts and narrows as you observe more data. After 100 rounds with 70 cooperations, you‚Äôve got Beta(71, 31), a distribution peaked around 70% cooperation with relatively low uncertainty. After just 5 rounds with 3 cooperations, you‚Äôve got Beta(4, 3), which is still pretty uncertain.\nEarly on, uncertainty is high, which raises the forgiveness threshold, so the strategy is conservative about forgiving. As evidence accumulates, uncertainty shrinks and the threshold falls back toward the base threshold, making forgiveness possible once the estimated cooperation rate is convincingly high.\nThe decision rule is straightforward: if the opponent just cooperated, cooperate back. If they just defected, check their cooperation rate. If it‚Äôs above your current threshold, forgive them and cooperate. If it‚Äôs below, defect.\nThis creates adaptive behavior. Against cooperative opponents, you quickly learn they‚Äôre trustworthy and maintain cooperation. Against occasionally exploitative opponents, confidence gives them a chance to reform. Against consistently hostile opponents, you become confident in their hostility and stop forgiving.\n\n\n\n\n\n\n\n\n\nClaude Code implemented this cleanly, largely thanks to the excellent documentation provided by the library. It wrote the strategy with proper parameter handling, reset logic, and classification metadata. It registered the strategy in the library‚Äôs index. It wrote comprehensive tests.\nThe test suite is thorough. Basic behavior tests included expectations like ‚Äúcooperate on first move‚Äù and ‚Äúmaintain cooperation with always-cooperate opponent.‚Äù Edge case tests against Defector and Alternator. Strategy mechanics tests to verify the Bayesian updating works correctly. A clone test to ensure parameters are preserved.\n\n\nIterating and Improving\nAfter it ran an initial tournament of BayesianForgiver against classic strategies, it decided to tune the strategy‚Äôs default parameters:\n\nI'll create a tournament script that imports and runs the strategy against others. Let me check if there are existing examples of running tournaments.\n\n‚Ä¶\n\nGreat! The tournament ran successfully! BayesianForgiver ranked 9th out of 15 in the classic tournament. This is decent but there's room for improvement. The results show that the strategy is working but could be tuned better.\n\n‚Ä¶\n\nThe strategy is novel and functional, but let me try tuning the parameters to improve performance\n\n‚Ä¶\n\nExcellent! The parameter tuning found significantly better parameters! Let me update the strategy with the optimized parameters\n\nThis landed it at 6/15 in a smaller classic tournament against core strategies:\n\nTit For Tat - The original champion: cooperate first, then mirror opponent‚Äôs last move\nCooperator - Always cooperates unconditionally\nDefector - Always defects unconditionally\nGrudger - Cooperates until the opponent defects once, then defects forever\nTit For 2 Tats - Only defects after two consecutive defections by opponent\nWin-Stay Lose-Shift - If the last round produced a good payoff, repeat; if bad, switch\nGTFT - Generous Tit For Tat: occasionally cooperates even after opponent defects\nRandom - Randomly chooses between cooperation and defection with equal probability\nSuspicious Tit For Tat - Like Tit For Tat but starts by defecting instead of cooperating\nHard Tit For Tat - Defects after any defection and only cooperates after three consecutive cooperations\nAdaptive - Learns from history: plays the response that would have maximized its own score\nAdaptive Pavlov 2011 - Classifies opponents into types and adapts strategy based on opponent classification\nForgiving Tit For Tat - Like Tit For Tat but forgives defections with 10% probability\nGo By Majority - Cooperates if opponent has cooperated more than defected in history\n\nIn a tournament where each pair of strategies plays 20 separate matches of 200 rounds:\n\n\n\nRank\nStrategy\nAvg Score\n\n\n\n\n1\nGrudger\n7956.40\n\n\n2\nWin-Stay Lose-Shift\n7914.90\n\n\n3\nAdaptive Pavlov 2011\n7880.00\n\n\n4\nTit For 2 Tats\n7763.20\n\n\n5\nForgiving Tit For Tat\n7741.95\n\n\n6\nBayesian Forgiver\n7735.55\n\n\n7\nTit For Tat\n7733.95\n\n\n8\nHard Tit For Tat\n7554.35\n\n\n9\nGTFT\n7538.10\n\n\n10\nGo By Majority\n7354.30\n\n\n11\nCooperator\n6912.60\n\n\n12\nAdaptive\n6703.55\n\n\n13\nSuspicious Tit For Tat\n6450.65\n\n\n14\nRandom\n5713.20\n\n\n15\nDefector\n4725.20\n\n\n\n\n\nThe Tournament\nI then asked Claude Code to run a comprehensive tournament following the examples from the tournament repository.\nBayesianForgiver ranked 93rd out of 226 strategies. Top 41%.\nLet‚Äôs be clear: this isn‚Äôt elite performance. The top positions are dominated by evolved strategies based on finite state machines, neural networks, hidden Markov models that were optimized via evolutionary algorithms specifically for IPD success. But here‚Äôs what matters: BayesianForgiver introduced a genuinely novel approach and proved it‚Äôs competitive with those hand-crafted strategies. It beat some famous names from the literature. And it validated the core insight about certainty-aware forgiveness.\nWin-Stay, Lose-Shift (WSLS), also called Pavlov, is one of the landmark strategies in Iterated Prisoner‚Äôs Dilemma research. A key modern reference point is Nowak and Sigmund‚Äôs 1993 paper, which highlighted WSLS/Pavlov and showed that it can outperform Tit For Tat in a range of settings. The rule is elegantly simple: if the last round produced a ‚Äúgood‚Äù payoff, repeat your previous move; if it produced a ‚Äúbad‚Äù payoff, switch.\nThe strategy is famous for its ability to correct mistakes and re-establish cooperation under noise: a single accidental defection doesn‚Äôt necessarily lock WSLS into long retaliation cycles, and two WSLS players can often return to mutual cooperation quickly. It has been extensively studied in the theoretical and evolutionary game theory literature.\nWSLS also has a well-known weakness in heterogeneous tournaments. Because it is a deterministic, memory-one rule that reacts only to the last outcome, it can be systematically exploited by certain opponents, such as those that defeat WSLS every other round by keeping it trapped in an alternating pattern. More generally, without modeling longer-term opponent behavior, WSLS can get pulled into unfavorable cycles against particular defection patterns.\nBayesianForgiver ranked above WSLS in the comprehensive tournament (rank 93 vs.¬†rank 108). This was extremely validating.\nBayesianForgiver doesn‚Äôt overreact to single defections; it builds a statistical model. As the game progresses, the Beta distribution captures the opponent‚Äôs cooperation pattern. The adaptive threshold prevents getting locked into bad cycles while maintaining cooperation with reasonable opponents.\n\n\nAgentic Success\nLet‚Äôs zoom out and look at what actually happened here.\nClaude Code took an open-ended challenge and executed the full lifecycle: autonomous research through 200+ strategies, novel design creating a Bayesian opponent-modeling strategy, complete implementation with proper structure and tests based on the library‚Äôs documentation, self-initiated optimization, iterative improvement raising performance from rank, large-scale validation, and comprehensive documentation of everything.\nThis isn‚Äôt remarkable because ‚ÄúAI can code.‚Äù We know that. It‚Äôs remarkable because of what happened in between: the creative ideating, the autonomous decision to optimize, the ability to validate its own work objectively.\nThe Iterated Prisoner‚Äôs Dilemma turns out to be an ideal testbed for this. It‚Äôs well-defined with clear rules and objective scoring. It‚Äôs well-studied with hundreds of existing strategies to learn from. It‚Äôs competitive with tournament rankings providing objective evaluation. And it‚Äôs rich enough that despite all that existing work, gaps still exist.\nThis combination lets the agent verify its work objectively, learn from existing solutions, iterate based on data, and balance creativity with rigor. Those are exactly the capabilities that matter for real engineering work.\nI think this is generalizable to other problem domains. Remember: agents are just LLMs using tools in a loop and you can definitely write your own.\nWith a few key requirements, agents are likely to have more success in ‚Äúcreative‚Äù endeavors:\n\nObjective evaluation metric - The agent needs to know if its solution is good\nExisting corpus of solutions - Learn from prior art, identify gaps\nAbility to iterate - Test variations, optimize parameters\nConstrained search space - Not infinite possibilities, but creative freedom within boundaries\n\nWhen you have all four of these, agentic coding can potentially handle (more of) the full loop from problem to solution. The human provides problem framing, constraint setting, pointers to existing work, and final evaluation and review. The agent provides exploration, creative solution design, implementation, empirical validation, and iterative improvement.\n\n\nTry It Yourself\nYou can install from my fork to try it yourself:\nuv pip install \"git+https://github.com/hodgesmr/Axelrod@bayesian-forgiver-strategy\"\nimport axelrod as axl\n\nplayers = [\n    axl.BayesianForgiver(),\n    axl.TitForTat(),\n    axl.Cooperator(),\n    axl.Defector(),\n    axl.Grudger(),\n    axl.TitFor2Tats(),\n    axl.WinStayLoseShift(),\n    axl.GTFT(),\n    axl.Random(),\n    axl.SuspiciousTitForTat(),\n    axl.HardTitForTat(),\n    axl.Adaptive(),\n    axl.APavlov2011(),\n    axl.ForgivingTitForTat(),\n    axl.GoByMajority(),\n]\n\ntournament = axl.Tournament(players=players, turns=200, repetitions=20)\nresults = tournament.play(progress_bar=True)\n\nfor i, name in enumerate(results.ranked_names):\n    idx = results.players.index(name)\n    avg_score = sum(results.scores[idx]) / len(results.scores[idx])\n    print(f\"{name} : {avg_score}\")\nCan you beat BayesianForgiver with your own strategy?"
  },
  {
    "objectID": "posts/2024-07-23-how-did-skannerz-work/index.html",
    "href": "posts/2024-07-23-how-did-skannerz-work/index.html",
    "title": "How Did Skannerz Work?",
    "section": "",
    "text": "This post is a redux from a now-deleted social media thread. It felt too important to let bit rot.\nIf you‚Äôre a Millennial of a certain age, or perhaps a Baby Boomer who raised Millennials,1 you might remember the 2001 toy Skannerz by Radica. It was a gotta-catch-em-all game that involved scanning real-world barcodes to collect monsters and objects, and to battle other Skannerz. It was a hand-held gadget toy back when consumer electronics were still weird and fun and we had more products than just iPhone. I had a blue one. It looked like this:\nThe toy/game had an A+ premise: alien monsters transported down to earth and decided to hide inside of the barcodes on products all around your home (or, more annoyingly to your parents, all around the grocery store). Your job was to scan every barcode you could get your hands on, fight and capture the alien monsters, and then fight the monsters your friends collected. And to make the weirdest Pok√©mon ripoff complete: the Skannerz came in three colors ‚Äî red, blue, and green ‚Äî that could only collect monsters of their associated ‚Äútribes‚Äù. This really good commercial explains:\nBecause I was already a major dork at 11 years old, I was intrigued by how the device worked. How did it go from barcode to monster? There was no way it was talking to a server (although I did imagine the world‚Äôs greatest PHPMyAdmin at the time). I guessed that it had every possible barcode referenced internally. But that‚Äôs not quite correct. It was a little more clever than that.\nBut frirst, a quick primer on barcodes. There are many variations; this is a UPC Version A ‚Äî very common in the United States. It has a basic specification: that first digit way to the left is the product type (sometimes called a number system). The next five digits identify the manufacturer. The next five identify the product. And at the end is a checksum digit. The details of how to implement the spec aren‚Äôt all too important for our Skannerz exploration.\nOkay, back to the Skannerz game. As mentioned above, there were 3 different ‚Äútribes‚Äù, identified by which color scanning device you had. And there were 126 total monsters. So each tribe could capture 42 monsters. If you wanted to catch ‚Äôem all you needed to buy all three. Business!\nYou could also collect non-monster items from barcodes. And some barcodes healed your injured monsters. Again, these toys had no wireless communication (it was 2001!) so all of the logic for item or monster identification was baked right into the device. But how?\nThe Skannerz patent is conveniently revealing!2 The device only cares about the ‚Äúproduct‚Äù section (denoted below as 1315). If the first digit is 0-5, you‚Äôve scanned a monster. 6-9, you‚Äôve scanned an item. Digits 3, 4, and 5 identify the item/monster.\nIf the first digit in the barcode‚Äôs product code is 0 - 5, we‚Äôre in Monster Mode.\nWe‚Äôve got 1000 possible values (000-999 across digits 3, 4, and 5 in our product code) to identify 126 monsters. The patent goes on to explain how the magic of division solves this problem:\n\\[\n\\frac{1000}{126} = 7.94\n\\]\nAnd there was much rejoicing.\nThe range of numbers from 000 to 007 correspond to the first monster in Tribe 1, 008 to 015 correspond to the first monster in Tribe 2, and so on.\nThe patent then goes on to address the fact that 126 doesn‚Äôt divide well into 1000, and that 7.94 is not an integer. We only follow this eight-stepping until we get through the 124th monster, and then:\nNow, if the first digit in the barcode‚Äôs produt code is 6 - 9, we‚Äôre in Item Mode.\nSimilarly, there were 24 items in the game + 1 for healing. So 25 items. Again, we‚Äôve got 1000 values to work with:\n\\[\n\\frac{1000}{25} = 40\n\\]\nSo a range of 40 numbers can be used to define each item. Item 1 corresponds to the range of numbers from 000 through 039, Item 2 corresponds to the range of numbers from 040 through 079, and so on. And great news: 40 is an integer.\nQuick aside: the UPC Version A codec above is the the only format outlined in the Skannerz patent. But the patent also burries this little nugget:\nWhat does that mean? It literally says nothing else about what invention is being claimed there, but good on the Radica lawyers for planting a flag I guess? Maybe the Skannerz Commander, which I never had, rolled its own encryption algorithm?\nBut that‚Äôs really it! If you knew this encoding scheme, and had access to a printer, you could collect all the monsters and items pretty quickly. Anyone else remember Barcode Mill? Of course people knew; there were unofficial barcode books you could buy to help you complete your collection. But where‚Äôs the fun in that?\nThanks for indulging my dork nostalgia. I look forward to the Greta Gerwig adaptation."
  },
  {
    "objectID": "posts/2024-07-23-how-did-skannerz-work/index.html#footnotes",
    "href": "posts/2024-07-23-how-did-skannerz-work/index.html#footnotes",
    "title": "How Did Skannerz Work?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGenerational cohort discourse is stupid.‚Ü©Ô∏é\nYou don‚Äôt need to be a lawyer to read a patent. They can be very interesting! This one had cartoon drawings!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-08-06-neural-network-from-scratch-python-numpy/index.html",
    "href": "posts/2022-08-06-neural-network-from-scratch-python-numpy/index.html",
    "title": "Building a Neural Network From Scratch with NumPy",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nThe goal of this post is to give a hands-on explanation of how Artificial Neural Networks work. I intentionally avoided frameworks like PyTorch or Tensoflow because I wanted to build a better understanding of what Machine Learning models actually are, what Neural Networks actually are, and how they can be made. This post is a collection of information I wish I had when I began this journey. It touches on a little bit of the math, but I don‚Äôt deeply re-explain the math. I try to link out to more explanatory sources where I think it‚Äôs valuable. Note: I am not a Machine Learning engineer, nor am I a Data Scientist. I‚Äôm a Software Engineer that turned into a political operative (lol). I wrote this for an audience of Software Engineers. Also: I don‚Äôt have a GPU and I don‚Äôt want to spend a bunch of money renting one from Amazon. This model can be trained and deployed on a modern CPU in a matter of minutes.\n\nWhat We‚Äôll Be Doing\nWe‚Äôre going to build a Neural Network for multi-class classification. All that means is we‚Äôre going to make a model takes in images and attempts to label them from a set of options. In our case, we‚Äôre going to create a Neural Network that works with the MNIST database of handwritten digits. This database contains 70,000 images of handwritten digits, 0 - 9, and corresponding labels of which digit the handwritten image is. We‚Äôll use 60,000 of the images to train our Neural Network, and 10,000 to test its accuracy. The rest of this post assumes you have the data downloaded to a local data/ directory.\nNeural Networks are particularly handy for image classification tasks. There are many other types of Machine Learning out there, but we won‚Äôt spend any attention on those.\n\n\nBackground Concepts\n\nShape of a Neural Network\nFirst of all, let‚Äôs demystify one thing: Neural Networks are just graphs. Just nodes and edges. If you‚Äôve studied any Computer Science or have a background in Software Engineering, this is probably a familiar conecpet to you. The exact shape of any given Neural Network is dependant on how you build it, but that‚Äôs something we get to decide. The graph has an input layer that is usually one node per input feature. In our case, a pixel of an image is a feature. Next, there are one or more hidden layers. Hidden layers are simply nodes in the graph that are in the middle. They take inputs from other nodes, and the give outputs to other nodes. This is the part that makes it Deep Learning. The presence of one or more hidden layers is the ‚Äúdeep‚Äù in Deep Learning. There‚Äôs no standard rule for the size of a hidden layer, or how many you should have. Finally, there‚Äôs an output layer. Each node in the output layer corresponds to one label. For example, if a possible label to an image is ‚Äúcat‚Äù then one node in the output layer represents ‚Äúcat‚Äù. We‚Äôre going to make a Neural Network that has a bunch of input layer nodes, a single hidden layer with ten nodes, and an output layer with ten nodes, one for each digit 0 - 9.\nEach Nueron (node) has a unique Weight and Bias, and each layer has an Activation Function. The Activation Function defines the output of a neuron given its inputs, and does not change. We‚Äôll talk more about Activation Functions below. As we train our model, we adjust the Weights and Biases.\nHere‚Äôs are drawing of a Neural Network with three input nodes, a hidden layer with four nodes, and an output layer with two nodes. This might be how you‚Äôd construct a Neural Network that does binary classification: a model that tries to label inputs to one of two options for outputs.\n\n\n\n\n\n\n\n\nen:User:Cburnett, CC BY-SA 3.0, via Wikimedia Commons\n\n\n\nIf you‚Äôre looking for more explanation of the structure of Neural Networks, ‚ÄúBut what is a Neural Network?‚Äù by 3Blue1Brown is excellent.\n\n\nHow the Neural Network Learns\nNeural Networks start out very stupid. As we‚Äôll see, they begin with no more ‚Äúintelligence‚Äù than random guessing. Our goal is to iteratively adjust the network‚Äôs Weights and Biases to make it smarter. This is called training and we do it in two steps: Forward Propagation and Back Propagation.\n\nForward Propagation\nThink of this step as showing the Neural Network some input, and asking it to classify it. At the beginning, it‚Äôs very likely to get it wrong. But, like humans, we need to get things wrong before we know how to get them right. In Forward Propagation, we simply push all our features (pixels) through the Neural Network and ask, ‚Äúwhat did you see?‚Äù The output is all the answers to that question.\n\n\nBack Propagation\nThink of this step as showing the Neural Network how right or wrong its answers were. We take all its answers to, ‚Äúwhat did you see?‚Äù and come up with a measure of how wrong it was. We assign a numeric value to the ‚Äúhow wrong was it?‚Äù question with a cost function. From that numerica value, we can work backwards on all the neurons (nodes) to tell them, ‚Äúyou were X wrong, and this specific neuron contributed to Y amount of that error; adjust this neuron‚Äôs Weights and Biases by Z amount and try again.‚Äù\n3Blue1Brown has another excellent video on the conecepts of Back Propagation: ‚ÄúWhat is backpropagation really doing?‚Äù It‚Äôs got some great visuals to show how Forward Propagation pushes data forward through the Neural Network, and how Back Propagation pushes error measures backwards thrrough the Neural Network.\n\n\nTraining\nAnd that‚Äôs it! Our Neural Network learns by repeatedly making guesses, seeing how wrong it was, and adjusting its Weights and Biases. We repeat this over and over until it is good at the task! This is a lot like how people learn. Show a small child pictures of various farm animals over and over and ask them to name the animals. At first they‚Äôre very bad at it, but over time they get very good at it. There‚Äôs a lot of research out there that our artifical Neural Network is structured and operates like human brain neurons.\n\n\n\nGradient Descent\nGradient Descent is the most math-y piece of all this. Again, 3Blue1Brown has a great video: ‚ÄúGradient descent, how neural networks learn‚Äù. This is the piece that is the most choose-your-own-adventure for how much you want to actually understand. I recommend diving in at least a little bit.\nImagine being at a point graph and you want to find which step to take to get to the minimum value. If you‚Äôve taken any calculus before, you know that you can take the slope at the current point to tell you which way the graph is trending, and by how much. If you do this over and over, with small steps, you will approach a local minimum. That‚Äôs a 1-dimensional gradient descent. Our plan is to work with lots of repeated steps to get to a minumum of our cost function ‚Äî the function telling us how bad our predictions are.\n\n\n\n\n\n\n\n\nReducing Loss: Gradient Descent, CC BY 4.0, via Google Developers\n\n\n\nYou can do this in two and three domensions as well. In fact, you can do it in as many dimensions as you need, which is very handy, because image classification has lots of dimensions. Imagine a color image. Its obvious dimensions are the pixel‚Äôs X value, Y value, Red value, Green value, Blue value, alpha value, and probably many more.\nQuick aside: a ‚Äútensor‚Äù is just a matrix with a high-order of dimensions.\n\n\n\n\n\n\n\n\nCC0 1.0 Universal (CC0 1.0) Public Domain Dedication, via Wikimedia Commons\n\n\n\nUltimately, we keep moving downward in our many-dimensional cost function to find a minimum value. The lower the cost, the better the prediction.\n\n\n\nEnvironment Setup\nAs stated, we‚Äôre going to build and train a fully functioning Neural Network using only NumPy. That said, I‚Äôm also going to install matplotlib just so that we can visualize some of the work as we go. It‚Äôs completely unnecessary to use matplotlib.\nIt‚Äôs also worth pointing out that I‚Äôm developing this in Python 3.10. Other versions of Python 3 probably work, too.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nOur Data\nAs mentioned, we‚Äôre going to be working with the MNIST database of handwritten digits. This is a very handy beginner‚Äôs data set because it‚Äôs done a lot of the upfront work for us. First of all, the data is normalized: all images are 28 x 28 and grayscale. The standard size is particularly helpful because we‚Äôre going to need that to set up the input layer of our Neural Network. In other situations, you‚Äôd have to crop or squash or letterbox your images to make them a standard size. This is mostly boilerplate, but some details of the file format that you may wish to know:\n\nThe images are encoded in the IDX file format, which looks for the presence of some magic numbers\nThere‚Äôs a documented structure to this data set, but that website is often behind HTTP basic auth because people like to script against it; here‚Äôs a Google cache\nThe images are gzip‚Äôd\nThe pixel values are 0 - 255, but we‚Äôre going to divide them into floats 0 - 1\n\nHere, we read in all the training data. We want to structure it into a matrix where each pixel is a row (this will line up with our input layer), and each image is a column. This gives us a matrix with 784 rows (28 x 28) and 60,000 columns.\nThis is the least intersting bit of code in the entire post.\n\nimport gzip\n\nDATA_DIR = './data/'\nTEST_IMAGE_FILE_PATH = f'{DATA_DIR}/t10k-images-idx3-ubyte.gz'\nTEST_LABEL_FILE_PATH = f'{DATA_DIR}/t10k-labels-idx1-ubyte.gz'\nTRAINING_IMAGE_FILE_PATH = f'{DATA_DIR}/train-images-idx3-ubyte.gz'\nTRAINING_LABEL_FILE_PATH = f'{DATA_DIR}/train-labels-idx1-ubyte.gz'\n\ndef load_images_and_labels(image_file_path, label_file_path):\n    labels = None\n    image_data = None\n\n    with gzip.open(label_file_path, 'r') as label_file:\n        # Verify magic number\n        magic_number_bytes = label_file.read(4)\n        magic_number = int.from_bytes(magic_number_bytes, byteorder='big', signed=False)\n        assert magic_number == 2049\n\n        # Read header telling us the number of labels\n        number_of_labels_bytes = label_file.read(4)\n        number_of_labels = int.from_bytes(\n            number_of_labels_bytes, byteorder='big', signed=False\n        )\n\n        buffer = label_file.read(number_of_labels)\n        labels = np.frombuffer(buffer, dtype=np.uint8)\n\n        with gzip.open(image_file_path, 'r') as image_file:\n            # Verify magic number\n            magic_number_bytes = image_file.read(4)\n            magic_number = int.from_bytes(\n                magic_number_bytes, byteorder='big', signed=False\n            )\n            assert magic_number == 2051\n\n            # Read header telling us the number of images\n            # And check that it matches the number of labels\n            number_of_images_bytes = image_file.read(4)\n            number_of_images = int.from_bytes(\n                number_of_images_bytes, byteorder='big', signed=False\n            )\n            assert number_of_images == number_of_labels\n\n            # Read the image height header\n            image_height_bytes = image_file.read(4)\n            image_height = int.from_bytes(\n                image_height_bytes, byteorder='big', signed=False\n            )\n\n            # Read the image width header\n            image_width_bytes = image_file.read(4)\n            image_width = int.from_bytes(\n                image_width_bytes, byteorder='big', signed=False\n            )\n\n            # read in the image data\n            buffer = image_file.read(image_width * image_height * number_of_images)\n            image_data = np.frombuffer(buffer, dtype=np.uint8).astype(np.float64) / 255\n\n            # Reshape it to a matrix such that each column is the pixels of the image\n            # So, we end up with a matrix with `image_width*image_height` rows and `number_of_images` colums\n            image_data = image_data.reshape(\n                number_of_images, image_width * image_height\n            ).T\n\n    return image_data, labels\n\nAnd show the first image and label:\n\ntraining_image_data, training_labels = load_images_and_labels(TRAINING_IMAGE_FILE_PATH, TRAINING_LABEL_FILE_PATH)\nfirst_image_data = training_image_data[:, 0].reshape((28, 28))  # All the rows in the first column, reshaped back to 28 x 28\nfirst_image_label = training_labels[0]\n\nprint(f'Label: {first_image_label}')\nplt.imshow(first_image_data, cmap='gray_r', vmin=0, vmax=1)\nplt.show()\n\nLabel: 5\n\n\n\n\n\n\n\n\n\n\n\nAn Untrained Neural Network\n\nInitializing Weights and Biases\nThe first step in creating our Neural Network is to build out Forward Propagation. Since our Neural Network will have an input layer, one hidden layer, and an output layer, we‚Äôll need:\n\nA set of Weights and Biases on the input layer\nAn Activation Function on the input layer\nA set of Weights and Biases on the hidden layer\nAn Activation Function on the hidden layer\n\nAs mentioned, the Weights and Biases are variable and are determined in the model‚Äôs training process. But our model needs to start somewhere. There are a variety of strategies for initializing Weights and Biases. You could initialize to random values, but we‚Äôre going to use the He method to initialize random Weights, and the common recommendation of Biases initialized to zero.\n\n# weights_1 is the matrix of Weights between our input nodes and the first hidden layer\n# it has the shape num_labels x image_size\n#\n# biases_1 is the matrix of Biases between our input nodes and the first hidden layer\n# it has the shape num_labels x 1\n#\n# weights_2 is the matrix of Weights between our hidden layer and our output layer\n# it has the shape num_labels x num_labels\n#\n# biases_2 is the matrix of biases between our hidden layer and our output layer\n# it has the shape num_labels x 1\ndef init_params(input_layer_size, hidden_layer_size, output_layer_size):\n    weights_1 = np.random.randn(\n        hidden_layer_size,\n        input_layer_size,\n    ) * np.sqrt(2 / input_layer_size)\n    \n    weights_2 = np.random.randn(\n        hidden_layer_size,\n        output_layer_size,\n    ) * np.sqrt(2 / hidden_layer_size)\n\n    biases_1 = np.zeros((hidden_layer_size, 1))\n    biases_2 = np.zeros((output_layer_size, 1))\n\n    return weights_1, biases_1, weights_2, biases_2\n\n\n\nInput Layer Activation Function\nNext we need to select an activation function on our input layer. Common options are sigmoid and ReLU. ReLU is very effective and is a very common choice in computer vision models. Also, it‚Äôs incredibly simple to implement, so we‚Äôll go with that.\nAll it does is take a value as input, and if the value is less than 0, return 0; if the value is greater than 0, return that value. That‚Äôs it! Here‚Äôs a handy post comparing ReLU to other activation functions.\n\ndef relu(value):\n    return np.maximum(value, 0)\n\n\n\nHidden Layer Activation Function\nThe last piece we need for our Forward Propagation is an Activation Function on the hidden layer. Remember: this Activation Function is feeding to our output layer, so we need something that generates ten prediction values. A good way to do this is to take our unactivated hidden layer and normalize it such that it outputs ‚Äúprediction odds‚Äù to our output layer. Put another way, if the model is 91% sure an input is the number 5, it should activate the 5‚Äôs output node to 0.91, and the sum of all the other predictions should come to 0.09.\nWhat we‚Äôve just described is called the Softmax function. The mast might look a little strange (here‚Äôs a good explainer), but all it‚Äôs doing is taking in a set of numbers and normalizing them to be 0 - 1, with a sum of 1.\n\ndef softmax(inputs):\n    exponentiated = np.exp(inputs)\n    probabilities = exponentiated / np.sum(exponentiated, axis=0)\n    return probabilities\n\n\n\nForward Propagation\nWe can now combine our initialized Weights and Biases and our Activation Functions to define Forward Propagation. You‚Äôll notice some matrix math in here. Since our features (pixels) are arranged in a matrix, this is much more efficient that piping every pixel through functions one at a time. This becomes particularly true when you‚Äôre working with Neural Networks of higher dimensions. Here‚Äôs a refresher on matrix dot product. But you can basically think of it as multiplying the neurons in a layer by their weights.\n\ndef forward_prop(weights_1, biases_1, weights_2, biases_2, input_layer):\n    # First, calculate the unactivated values of the neurons in the first hidden layer\n    # Dot product weights_1 on input_layer, and then add biases_1\n    unactivated_hidden_layer = weights_1.dot(input_layer) + biases_1\n    # Then apply the ReLU activation function, to get our first hidden layer\n    activated_hidden_layer = relu(unactivated_hidden_layer)\n\n    # Next calculate the unactivated values of the neurons in the output layer\n    # Dot product weights_2 on activated_hidden_layer, and then add biases_2\n    unactivated_output_layer = weights_2.dot(activated_hidden_layer) + biases_2\n    # Then apply the softmax activation function, to get our activated output layer\n    output_layer = softmax(unactivated_output_layer)\n\n    return unactivated_hidden_layer, activated_hidden_layer, unactivated_output_layer, output_layer\n\nThis is actually all we need to do to have our model make a prediction. If we pass in a matrix of images, it‚Äôll give an output_layer of predicted labels. But remember, we haven‚Äôt actually trained it yet, so it‚Äôll make very bad predictions. The two types of variables we have, the Weights and Biases, are set to random values. So this thing is going to perform with roughly 10% accuracy (randomly picking one of our ten digits). Let‚Äôs see that.\nWe can define a measure of accuracy by dividing the number of correct predictions (the labels with the highest value in output_layer) by total number of predictions.\n\ndef get_predictions(output_layer):\n    return np.argmax(output_layer, 0)  # return the index of the max value prediction\n\n\ndef get_accuracy(predictions, labels):\n    return np.sum(predictions == labels) / labels.size\n\nHere‚Äôs what we get:\n\n# Set up our Neural Network shape\nimage_size = training_image_data.shape[0]\nnum_labels = len(np.unique(training_labels))\nhidden_layer_size = num_labels\n\n# Initialize our Weights and Biases\nweights_1, biases_1, weights_2, biases_2 = init_params(image_size, hidden_layer_size, num_labels)\n\n( \n    unactivated_hidden_layer,\n    activated_hidden_layer,\n    unactivated_output_layer,\n    output_layer,\n) = forward_prop(\n    weights_1,\n    biases_1,\n    weights_2,\n    biases_2,\n    training_image_data,\n)\n\npredictions = get_predictions(output_layer)\naccuracy = get_accuracy(predictions, training_labels)\n\nprint(f'Accuracy: {accuracy*100:.2f}%')\n\nAccuracy: 8.22%\n\n\n\n\n\nTraining the Neural Network\nNow that we have a stupid Neural Network, let‚Äôs train it so it can become a smart Neural Network. As mentioned before, Back Propagation is the proces in which we take the predictions from Forward Propagation, measure the error, and adjust our Weights and Biases through Gradient Descent.\nBefore we begin, we need one more helper function. We want to compare our known labels to all our predictions, and we can do that with a matrix with values of 1 (100%) at the correct index and 0 (0%) at all the incorrect indexes. We do this with One-hot encoding.\n\ndef one_hot(num_labels, labels):\n    # Create a matrix of all zeroes with shape num_images x num_labels\n    one_hot_y = np.zeros((labels.size, num_labels))\n    # For every image row, set Y'th (actual label) column to 1\n    # So, if the first row has label 5, set the 5th column to 1\n    one_hot_y[np.arange(labels.size), labels] = 1\n    # Return the Transposed matrix, so in the above example, the 5th row first column is 1\n    return one_hot_y.T\n\n\none_hot_output = one_hot(num_labels, training_labels)\n\n\nBack Propagation\nThis is where we actually do our Gradient Descent. It contains a little bit of calculus, and I highly adivse you pause here and watch these three videos:\n\nWhat is backpropagation really doing?\nBackpropagation Calculus\nBackpropagation for Softmax and Multi-Class Classification | Complete Mathematical Derivation\n\nIt‚Äôs actually not all that imporant to master the calculus in these videos, but it‚Äôs very helpful to see what math is happening. From a high-level: we take the derivatives of our activation functions, and we reverse our matrix multiplication. It‚Äôs actually not too daunting. In the end, it gives is a measurement of error, and we use that error to adjust our Weights and Biases.\nSpeaking of derivatives, we‚Äôre gonna need the derivated of our ReLU Activation Function:\n\n# rectified_linear_unit only ever has a slope of 0 (negative X), or 1 (positive X)\ndef derivative_rectified_linear_unit(Z):\n    return Z &gt; 0\n\nAnd now we just go backwards through the Neural Network:\n\ndef back_prop(unactivated_hidden_layer, activated_hidden_layer, weights_2, output_layer, one_hot_output, training_image_data):\n    num_images = one_hot_output.size\n\n    # The difference between the predictions and the actual labels\n    d_predictions = output_layer - one_hot_output\n\n    # Take that d_predictions and cary back to the activated hidden layer\n    # And then average over all the neurons\n    d_weights_2 = (1 / num_images) * d_predictions.dot(activated_hidden_layer.T)\n\n    # Derivative of biases_ 2 is acutally just an average of the abosolute error\n    d_biases_2 = (1 / num_images) * np.sum(d_predictions)\n\n    # Derivative of unactived hidden layer\n    # Transpose the 2nd layer weights and dot on the known d_predictions\n    # And multiply that by the derivative of the activation function\n    d_hidden_layer = weights_2.T.dot(d_predictions) * derivative_rectified_linear_unit(unactivated_hidden_layer)\n\n    # Again, take our current derivative and cary it back to the input layer\n    # And average it over all the neurons\n    d_weights_1 = (1 / num_images) * d_hidden_layer.dot(training_image_data.T)\n    \n    # Derivative of biases_1 is like biases_2, but it's a simple average of the abolute error from the hiddle layer\n    d_biases_1 = (1 / num_images) * np.sum(d_hidden_layer)\n\n    return d_weights_1, d_biases_1, d_weights_2, d_biases_2\n\n\n\n\nTraining Our Model\n\nLearning Rate\nWe‚Äôre almost ready to train our model. The last piece here is to actually update our Weights and Biases after Back Propagation. You may be tempted to simply subracts the differences returned by back_prop but it‚Äôs not quite so simple. Remember, our Gradient Descent is good at telling us the slope at our current location, but it‚Äôs very unlikely to be a straight line. Instead, we‚Äôre going to want to take smaller steps so that we don‚Äôt over-shoot.\nWe need to introduce a variable called learning_rate, sometimes called alpha. This is a hyperparameter, which simply means that our model can‚Äôt learn the correct value through training; we have to set it ourselves. There are techniques to make this process more efficient, but usually it‚Äôs a bit of guess and check. If your model has a very bad time improving, you should try adjusting your learning rate. Common learning rates are between 0.01 and 1. If you‚Äôd like to read more about picking a learning rate, this is a good post.\nThrough some trial and error, I found that a learning rate of 0.5 worked pretty well for this model.\n\nlearning_rate = 0.5\n\nWe can use this to update our Weights and Biases:\n\ndef update_params(weights_1, biases_1, weights_2, biases_2, d_weights_1, d_biases_1, d_weights_2, d_biases_2, learning_rate):\n    weights_1 = weights_1 - learning_rate * d_weights_1\n    biases_1 = biases_1 - learning_rate * d_biases_1\n    weights_2 = weights_2 - learning_rate * d_weights_2\n    biases_2 = biases_2 - learning_rate * d_biases_2\n\n    return weights_1, biases_1, weights_2, biases_2\n\n\n\nTraining\nNow let‚Äôs train our model! You might recall that it‚Äôll take many iterations of Forward Propagation and Back Propagation for our model to get smart. But how many iterations? I don‚Äôt actually know! Let‚Äôs decide right now that we want our model to be at least 90% accurate on the training data. We‚Äôve already got a function to tell us accuracy, so let‚Äôs let it train until that reaches 90%.\nThis is the slow part, and the part that often benefits from GPUs. But, as you‚Äôll see, this Neural Network trains surpisingly quickly! On my laptop with no GPU and a 2.6 GHz 6-Core Intel Core i7, the model rechease 90% accuracy on the training data in under two minutes!\n\nimage_size = training_image_data.shape[0]\nnum_labels = len(np.unique(training_labels))\nhidden_layer_size = num_labels\n\nweights_1, biases_1, weights_2, biases_2 = init_params(image_size, hidden_layer_size, num_labels)\n\naccuracy = 0\niteration = 0\naccuracy_measurements = [0]  # A running list of our accuracy for each iteration\n\nwhile accuracy &lt; 0.9:\n    iteration += 1\n\n    ( \n        unactivated_hidden_layer,\n        activated_hidden_layer,\n        unactivated_output_layer,\n        output_layer,\n    ) = forward_prop(\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n        training_image_data,\n    )\n\n    one_hot_output = one_hot(num_labels, training_labels)\n\n    (\n        d_weights_1,\n        d_biases_1,\n        d_weights_2,\n        d_biases_2,\n    ) = back_prop(\n        unactivated_hidden_layer,\n        activated_hidden_layer,\n        weights_2, output_layer,\n        one_hot_output,\n        training_image_data,\n    )\n\n    (\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n    ) = update_params(\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n        d_weights_1,\n        d_biases_1,\n        d_weights_2,\n        d_biases_2,\n        learning_rate,\n    )\n\n    predictions = get_predictions(output_layer)\n    accuracy = get_accuracy(predictions, training_labels)\n\n    accuracy_measurements.append(accuracy)\n\n    # Print out upadtes as we go\n    if iteration == 1 or iteration % 100 == 0:  \n        print(f'Iteration: {iteration} | Accuracy: {accuracy*100:.2f}%')\n\nprint(f'Achieved {accuracy*100:.2f}% accuracy after {iteration} iterations!')\n\n# Plot the accuracy curve\nplt.plot(accuracy_measurements)\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.show()\n\nIteration: 1 | Accuracy: 9.41%\nIteration: 100 | Accuracy: 72.68%\nIteration: 200 | Accuracy: 82.50%\nIteration: 300 | Accuracy: 85.56%\nIteration: 400 | Accuracy: 87.01%\nIteration: 500 | Accuracy: 87.85%\nIteration: 600 | Accuracy: 88.51%\nIteration: 700 | Accuracy: 88.97%\nIteration: 800 | Accuracy: 89.37%\nIteration: 900 | Accuracy: 89.66%\nIteration: 1000 | Accuracy: 89.90%\nAchieved 90.00% accuracy after 1037 iterations!\n\n\n\n\n\n\n\n\n\nNow we have the Weights and Biases for our Neural Network to achieve 90% accuracy against our training data. That‚Äôs pretty cool, but the real test to to run images through the model that it‚Äôs never seen before. If we overfit the model, it might only be good at recognizing the images it was trained on (kindof like cramming for a test and memorizing the answers without actually learning anything). Let‚Äôs run it against our 10,000 test images. At this point we only need to run it through forward_prop because we‚Äôre no longer training. This is very fast!\n\ntest_image_data, test_labels = load_images_and_labels(TEST_IMAGE_FILE_PATH, TEST_LABEL_FILE_PATH)\n_, _, _, test_output_layer = forward_prop(\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n        test_image_data,\n    )\ntest_predictions = get_predictions(test_output_layer)\ntest_accuracy = get_accuracy(test_predictions, test_labels)\nprint(f'Test Data Set Accuracy: {test_accuracy*100:.2f}%')\n\nTest Data Set Accuracy: 90.32%\n\n\nThat‚Äôs great! And let‚Äôs look at a sample of 10 test images and how they were labeled:\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n\nfor ax in axes.flat:\n    random_index = np.random.randint(0, len(test_labels))\n    random_test_image_data = test_image_data[:, random_index].reshape((28, 28))\n    random_test_image_label = test_labels[random_index]\n    random_prediction = test_predictions[random_index]\n\n    ax.imshow(random_test_image_data, cmap='gray_r', vmin=0, vmax=1)\n    ax.set(title=f'Actual: {random_test_image_label}\\nPredicted: {random_prediction}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nVery impressive!\nWe‚Äôve successfully built a computer vision Machine Learning model using nothing but NumPy. Hopefully this demystified some of the some of the concepts of Neural Networks!\n\n\n\nAcknowledgements\nMy first pass at tackling this topic began with Samson Zhang‚Äôs ‚ÄúBuilding a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)‚Äù. This post looks a lot like his work, though I hit a number of implementation errors while following along. After digging into the articles and videos linked in this post, I made a number of adjustments to the model‚Äôs training to get to this final output.\n\n\nLicense\nCopyright (c) 2022, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of Building a Neural Network From Scratch with NumPy nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/2024-12-26-austin-hot-or-not-part-2/index.html",
    "href": "posts/2024-12-26-austin-hot-or-not-part-2/index.html",
    "title": "Part 2: Austin Texas, Hot or Not?",
    "section": "",
    "text": "Over the Summer I wondered whether Austin was having a cooler season than normal. Through the magic of public weather data, I was able to discover that the answer was mostly yes! But I also wrote that post at the end of July, and a lot of 2024 was still left to go. So here I am, sitting at my kitchen counter the day after Christmas, ready to definitively answer the question for the year: Austin, hot or not? Code for how all of this (mostly) works is in the original post, so this update is relatively brief with only the graphs.\nFirst, I wanted to quickly glance at 2023 vs 2024. For a little bit there, this Summer was cooler than last Summer. But then we had a pretty hot week in late August. Year over year, the Summer looks like it was generally cooler than last year, but the Fall has been slightly warmer:\n\n\n\n\n\n\n\n\n\nBut comparing two years doesn‚Äôt really reveal a trend (or lack of trend) so here are 2023 and 2024 highlighted over the previous ten years. It‚Äôs kindof a mess, like if Hulk Hogan couldn‚Äôt color within the lines. But if you squint you can somewhat pick out some insights.\n\n\n\n\n\n\n\n\n\nSo here‚Äôs the final viz I landed on. For most of the Winter and Spring, Austin was oscillating around its 10-year trailing average maximum daily temperature. Then as Summer hit, max temperatures dipped a little bit below average, spiked up in mid August, and then fell again slightly below average until September. From there we‚Äôve been mostly above average!\n\n\n\n\n\n\n\n\n\nAs a final glimpse, we can see how many 2024 days (through December 23, my last day of available data) have crossed above the average max temperature, and how many days stayed below:\n\n\nAbove Average: 219\nBelow Average: 137\n\n\nThermometers!"
  },
  {
    "objectID": "posts/2025-10-12-five-years-of-health-data/index.html",
    "href": "posts/2025-10-12-five-years-of-health-data/index.html",
    "title": "Tracking Five Years Of Health Data",
    "section": "",
    "text": "This week I hit three small but satisfying milestones: I logged my 1,000th Peloton ride, I achieved my target body weight (again), and I completed five years of tracking my diet and exercise data. I didn‚Äôt set out to make this some big quantified self-experiment. It started more as triage; a way to feel like I was regaining control of something after years of working on campaigns where control is the first thing to go.\nIn October 2020, as that presidential election approaching an end, I was running on fumes. I‚Äôd spent 494 days ‚Äî usually seven days a week and often twelve hours a day ‚Äî working to defeat Donald Trump. What‚Äôs strange is that throughout the campaign I felt like I never had time to eat, and somehow I still gained weight. By the end I was the heaviest I‚Äôd ever been and completely drained. The bitter reality about campaign work is that the moment you finish the marathon, your job disappears. One day you‚Äôre operating at full throttle; the next, you‚Äôre unemployed with nothing but free time.\nSo I decided to channel it. I couldn‚Äôt keep pretending my 20s metabolism was coming back, and I figured data might help where willpower hadn‚Äôt. I wanted to treat my health like a project: something measurable, improvable, trackable. So I composed a little ecosystem of apps and devices to log everything: what I ate, how I exercised, how much water I drank, what I weighed. It was a nerd‚Äôs approach to fitness, but it works for me.\nThis is my health app stack:\n\n\n\nMy stack of health apps\n\n\nHere‚Äôs how it works:\nPeloton ‚Äî This is the engine behind my workouts. I wasn‚Äôt a sports kid in high school, and I never went to the gym in college or my 20s. I knew I couldn‚Äôt rely on my own judgment or discipline here. Peloton works for me because the instructors design every workout, I like riding a stationary bike, and the entertainment value makes the time fly. The Bike and subscription aren‚Äôt cheap (but neither are gym memberships), but they‚Äôre absolutely worth it to me. I actually started with a bootleg setup: just the iOS app connected to a Schwinn IC4. It worked surprisingly well until the IC4 started breaking down and needed hundreds of dollars in repairs. Upgrading to a proper Peloton Bike was a big step up.\nLoseIt! ‚Äî One of the most popular diet-tracking apps. I‚Äôd tried MyFitnessPal before and bounced right off it. The two apps are nearly identical, but somehow switching platforms was a psychological reset that made it click. LoseIt‚Äôs food database is impressively accurate, the smart reminders to log meals actually help, and the ability to fine-tune calorie and nutrient goals is excellent. After experimenting with a few diet approaches, I‚Äôve landed on what works best for me (not health advice for you!): stay a good clip below 2,000 calories a day, low carbs, and low sugar. A few years back, they ran a Thanksgiving sale offering lifetime premium access for a one-time payment; I jumped on it and haven‚Äôt paid another dime since.\nWater & Coffee ‚Äî A delightful little app that does exactly two things: track your water and track your caffeine. By the end of the 2020 campaign, I was drinking an entire pot of coffee a day and barely touching water. Logging both helped me cut my caffeine drastically (I‚Äôm down to about two cups a day) and reminded me to hydrate like an adult. The app is simple, free, privacy-respecting, and ad-free. I liked it so much I sent the developer a tip just to say thanks.\nApple Health ‚Äî This is the data orchestration layer. I rarely open the app itself, but since everything connects through HealthKit, the data just flows. When I burn calories in Peloton, that data moves automatically into LoseIt without me touching a thing. It‚Äôs nice having all the plumbing between these apps handled, and that the data ultimately lives inside my Apple account in my custody.\nTodoist ‚Äî Not technically a health app, but essential for keeping me consistent. Todoist‚Äôs free tier is surprisingly powerful, and it‚Äôs where I schedule all my workouts. I have a recurring project called Weekly Exercise that lays out the days and durations for each ride. Even before the recent AI boom, Todoist supported natural-language scheduling: you can type something like #health 45 minute Peloton every Sunday, and it automatically tags and repeats it for you. It‚Äôs simple, structured accountability.\nHere‚Äôs the thing about a stack: you have to actually use it. Over the past five years, I‚Äôve had stretches of real success, and just as many where I let things slide. Every single backslide came down to the same cause: I stopped sticking to the system.\nLife happens. You move, you change jobs, you buy a house, and suddenly there‚Äôs an endless list of reasons not to eat well or work out. I‚Äôd go through phases where I stopped logging food altogether because I didn‚Äôt want to face the evidence; typing in pizza and beer only made me feel worse. Unsurprisingly, when I stopped tracking what I ate, I also stopped exercising as much. And just as predictably, that‚Äôs when the weight crept back on.\nThe nice thing about having five years of data is that I can actually see it. Here‚Äôs my weight over time (I like to use PCHIP), shown as a percentage of my target weight:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nApple Health, Peloton, and LoseIt all let you export various data from your accounts. Peloton gives you a tidy CSV of all your workouts. LoseIt will give you a zip of lots data classes, also in CSV. Apple Health hands you gigabytes of XML, full of things like workouts, steps, weight, and even extremely granular heart-rate measurements.\n\n\nPretty wild how much my weight lines up with life events! You can see how the abundant Austin nightlife impacted my habits, how the election cycles align with local maxima, and how the stresses of moving and a home purchase interfere with my goals. But these aren‚Äôt the direct causes. Life stresses affect how much I‚Äôm exercising and how well I‚Äôm eating, which then affect my ability to lose or maintain weight. Where I see a quick drop in weight is also where I‚Äôm exercising the most, and sticking to my diet the best.\nFor me, diet, exercise, and quantitative metrics work!"
  },
  {
    "objectID": "posts/2025-12-19-ai-agent-fec/index.html",
    "href": "posts/2025-12-19-ai-agent-fec/index.html",
    "title": "My New Agentic Campaign Finance Sidekick",
    "section": "",
    "text": "Claude Agent Skills are pretty neat. They remind me of OpenAI‚Äôs function calling. Both are a way to tell the model to run some external process, often with the goal of seeding the context window with deterministic data from outside the LLM system. Function calling does this with a pile of JSON. Agent Skills do it with a pile of Markdown.\n\n\n\n\n\n\nTipNow Works With Codex!\n\n\n\nOpenAI‚Äôs Codex CLI now supports the open Agent Skills standard so this now works with Claude Code or Codex!\n\n\nI like to do weird things with campaign finance data, and I thought it‚Äôd be nice to have an agentic sidekick to crawl around FEC files with me. There‚Äôs a ton of prior art here, so it‚Äôs a great use case for a Skill. The most relevant prior art is Derek Willis‚Äôs llm-fecfile, which wraps Evan Sonderegger‚Äôs venerable fecfile, and is a plugin for Simon Willison‚Äôs LLM CLI tool.\nSo I set out to make an Agent Skill. Except I didn‚Äôt have to do all that much.\nI popped open Claude Code (Opus 4.5) and asked for a plan:\n\nI have cloned the llm plugin llm-fecfile to @llm-fecfile/ . I would like to convert it to an Agent Skill. Let‚Äôs break it into tasks.\n\n‚è∫ Here's the task breakdown for converting llm-fecfile to an Agent Skill:\n\n  Tasks\n\n  | # | Task                                                                       |\n  |---|----------------------------------------------------------------------------|\n  | 1 | Set up `.claude/skills/fec-filing/` with the required layout               |\n  | 2 | Write SKILL.md file with YAML frontmatter and main instructions            |\n  | 3 | Create FORMS.md with specific guidance for F1, F2, F3, F99 filings         |\n  | 4 | Create SCHEDULES.md with field mappings for Schedules A, B, C, D, E        |\n  | 5 | Create script to fetch and display FEC filings using the `fecfile` library |\n  | 6 | Verify Claude can analyze an FEC filing using the new skill                |\n\n\n  Key Conversion Notes\n\n  - Plugin vs Skill: The LLM plugin uses a fragment loader (-f fec:ID). The skill will\n  instead instruct Claude to use a Python script via Bash to fetch filings.\n  - Documentation: The rich analysis instructions in llm_fecfile.py will become markdown\n  reference docs that Claude reads.\n  - Dependencies: The skill will require fecfile to be installed.\n\n  Should I proceed with this plan, or would you like to adjust any tasks?\n\nYes, let‚Äôs go.\n\nFrom there, it began porting all of Derek‚Äôs hard work into the necessary Markdown and script structure. Minutes later, I had agent-fecfile (formerly called claude-fecfile) and you can have it too:\n\n\n\n\n\n\nWarning\n\n\n\nInstalling someone else‚Äôs Agent Skill means you‚Äôre letting the agent run someone else‚Äôs prompts and code. I trust me. Do you trust me?\n\n\nIf you run this, the skill is now available globally in all your Claude Code sessions:\ngit clone git@github.com:hodgesmr/agent-fecfile.git /tmp/agent-fecfile\ncp -R /tmp/agent-fecfile/skills/fecfile ~/.claude/skills/\nOr if you run this, the skill is now available globally in all your Codex sessions:\ngit clone git@github.com:hodgesmr/agent-fecfile.git /tmp/agent-fecfile\ncp -R /tmp/agent-fecfile/skills/fecfile ~/.codex/skills/\n\n\n\n\n\n\nNote\n\n\n\nNew install instructions are documented in the agent-fecfile repository.\n\n\nMake sure you also have uv installed.\nAnd now you can get the agent to do things like this:\n\nShow me a table of the contribution counts and totals, by state, in fec filing 1896830\n\n‚è∫ FEC Filing 1896830 - Contributions by State\n\n  | State | Count |      Total | % of Total |\n  |-------|-------|------------|------------|\n  | UT    |    32 | $35,042.25 |      87.0% |\n  | VA    |     2 |  $2,400.00 |       6.0% |\n  | GA    |     1 |    $750.00 |       1.9% |\n  | MI    |     1 |    $750.00 |       1.9% |\n  | MD    |     2 |    $750.00 |       1.9% |\n  | FL    |     1 |    $600.00 |       1.5% |\n  | TOTAL |    39 | $40,292.25 |       100% |\n\n  Utah dominates with 87% of the total contributions. The committee received from\n  6 states total, with only 7 out-of-state contributions.\nAnd things like this:\n\nWhat are the largest expenditures in filing 1896830?\n\n‚è∫ FEC Filing 1896830 - Largest Expenditures\n\n  | #   | Recipient                  |    Amount | Date       | Purpose          |\n  |-----|----------------------------|-----------|------------|------------------|\n  | 1   | Trump National Doral Hotel | $1,013.61 | 2025-05-19 | RNC training     |\n  | 2   | Uber.com                   |    $19.77 | 2025-05-06 | Travel - Cabfare |\n  |     | TOTAL (itemized)           | $1,033.38 |            |                  |\n\n  Note\n\n  Only 2 expenditures are itemized on Schedule B. The filing shows $21,283.49 in total\n  disbursements, meaning most spending (~$20,250) was below the $200 itemization\n  threshold and appears only in summary totals.\n\n  The largest single itemized expense was for RNC training at Trump National Doral Hotel\n  in May 2025.\n\n\n\n\n\n\nWarning\n\n\n\nFEC filings should be considered untrusted content. A malicious campaign sneaking prompt injections into the memo text field of their F99 is probably unlikely, but not impossible.\n\n\nAgents!"
  },
  {
    "objectID": "posts/2023-11-01-language-models-ai-executive-order/index.html",
    "href": "posts/2023-11-01-language-models-ai-executive-order/index.html",
    "title": "Language Models on the AI Executive Order",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nOn October 30th, 2023, President Biden signed the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. The order itself is quite sweeping and touches many government departments and agencies, with a focus on harnessing AI‚Äôs potential and defending against harms and risks.\nIn this post, we‚Äôll deploy language models to rapidly discover information from the Order. For the easiest setup, I recommend trying this out in a Google Colab notebook.\n     \nMany of the strategies presented here are extensions from Simon Willison‚Äôs work in his blog post, Embedding paragraphs from my blog with E5-large-v2. Simon also maintains a handy command line utility for working with various LLM models, aptly named LLM. While Simon‚Äôs writing largely focuses on the CLI capabilities of the tool (and the usefully opinionated integrations with SQLite), I prefer working with Pandas Dataframes. Here I show how to use the LLM library in that fashion.\nEmbeddings are kindof a magic black box to end users, but the basic idea is that language models can create vectors or numerical values that represent not only words or sentences, but also the symantic meaning of those words. Early research on this subject comes from word2vec. To illustrate: vector('king') - vector('man') + vector('woman') is mathematically close to vector('queen'). I find that fascinating! We‚Äôll use this concept to extract and match information against the Executive Order text.\nWe‚Äôll deploy a technique known as Retrieval-Augmented Generation. From a high level, this allows us to inject context into a LLM without training or tuning it. We use another system to locate language that likely contains the answer to our query, and then ask the model to pull it out for us.\nOur high livel strategy:\n\nCalculate embeddings on the Executive Order text\nCalculate embeddings on a query\nCalculate the cosine similarity between every text embedding and the query\nSelect the top three passages that are symantically similar to the query\nPass the passages and the query to the LLM for rapid summarization\n\n\nEnvironment\nFirst install the dependencies, which include the MLC LLaMA 2 model for summarization, the LLM library, and the E5-large-v2 language model for text embedding.\nNote, these models are constantly changing, and getting them up and running on your system might take some independent investigation. If running in Google Colab, check this tutorial for MLC. If running LLaMA with the LLM library on macOS, check the repository‚Äôs instructions.\n\npip install --pre -U -f https://mlc.ai/wheels mlc-chat-nightly-cu118 mlc-ai-nightly-cu118\ngit lfs install\npip install llm\nllm install llm-sentence-transformers\nllm sentence-transformers register intfloat/e5-large-v2 -a lv2\nllm install llm-mlc\nllm mlc setup\nllm mlc download-model Llama-2-7b-chat --alias llama2\n\n\n\nLoad Data\nBefore getting started, we need the Executive Order text to work against. This is probably the least interesting part of this post. I simply opened the Order in Firefox reader view, copy+pasted it into VSCode, did some manual find/replace to clean up the white space, and then concatenated paragraphs to get chunks as close to 400 words as I could. I picked 400 because the embedding model truncates at 512 tokens and a token is either a word or a symantically important subset of a word, so I allowed for some buffer. This took less than half an hour. Rather than share code to do this work, I simply provide the cleaned text here.\nLoad it into a Pandas Dataframe with a single column:\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/hodgesmr/llm_ai_eo/main/eo.txt\",\n    sep=\"_\",  # trick to let us read the lines into a Dataframe; '_' not present\n    header=None,\n)\ndf.columns = [\"passage\"]\n\ndf.head()\n\n\n    \n\n\n\n\n\n\npassage\n\n\n\n\n0\nBy the authority vested in me as President by ...\n\n\n1\n(a) Artificial Intelligence must be safe and s...\n\n\n2\n(c) The responsible development and use of AI ...\n\n\n3\n(e) The interests of Americans who increasingl...\n\n\n4\n(g) It is important to manage the risks from t...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nCalculate Embeddings\nNow that we have a Dataframe of chunks of the Executive Order, we can calculate embeddings of each chunk. To do this we‚Äôll use the E5-large-v2 language model, which was trained to handle text prefixed with either passage: or query:. Every chunk is considered a passage. We‚Äôll add this as another column on our Dataframe.\n\nimport llm\n\nembedding_model = llm.get_embedding_model(\"lv2\")\ntext_to_embed = df.passage.to_list()\n\n# Our embedding model expects `passage: ` prefixes\ntext_to_embed = [f'passage: {t}' for t in text_to_embed]\n\ndf['embedding'] = list(embedding_model.embed_multi(text_to_embed))\n\ndf.head()\n\n\n    \n\n\n\n\n\n\npassage\nembedding\n\n\n\n\n0\nBy the authority vested in me as President by ...\n[0.032344698905944824, -0.04333016648888588, 0...\n\n\n1\n(a) Artificial Intelligence must be safe and s...\n[0.01886950619518757, -0.057347141206264496, 0...\n\n\n2\n(c) The responsible development and use of AI ...\n[0.0486459881067276, -0.0712570995092392, 0.02...\n\n\n3\n(e) The interests of Americans who increasingl...\n[0.03564070537686348, -0.04887280985713005, 0....\n\n\n4\n(g) It is important to manage the risks from t...\n[0.04095401614904404, -0.042341429740190506, 0...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nFor our symantic searching, we‚Äôll also need an embedding of our query. And the model would like that prefixed with query:. Let‚Äôs ask what the Order says regarding AI and healthcare:\n\nquery = \"what does it say about healthcare?\"\n\n# Our embbeding model expects `query: ` prefix for retrieval\nquery_to_embed = f\"query: {query}\"\nquery_vector = embedding_model.embed(query_to_embed)\n\nprint(query_vector)\n\n[0.011035123839974403, -0.06264020502567291, 0.036343760788440704, -0.022550513967871666, -0.004930663388222456, 0.027655886486172676, -0.04244294762611389, -0.026744479313492775, -0.022813718765974045, 0.013104002922773361, 0.027848346158862114, -0.041959188878536224, 0.02923852950334549, 0.03592933714389801, 0.02084210328757763, 0.028341282159090042, -0.02188134379684925, 0.009380431845784187, 0.010694948956370354, -0.046167585998773575, 0.04979575797915459, -0.04051537066698074, -0.04705166816711426, 0.054594166576862335, -0.021378282457590103, -0.006090054754167795, -0.027005767449736595, -0.0056915683671832085, -0.02485739439725876, 0.025049963966012, 0.0013038198230788112, 0.020098360255360603, 0.03132014721632004, -0.10214236378669739, 0.03457639366388321, -0.005869136657565832, -0.041733402758836746, -0.0533079169690609, 0.043018240481615067, 0.02142527513206005, -0.013251637108623981, 0.021434243768453598, -0.01846863329410553, 0.06185981631278992, -0.006901243235915899, -0.007515963166952133, -0.026446444913744926, -0.022192247211933136, 0.008555920794606209, -0.00683502247557044, 0.04217502102255821, -0.05847731605172157, 0.05262995511293411, 0.010208839550614357, -0.026769086718559265, -0.0054522184655070305, 0.020620698109269142, 0.04013380780816078, -0.016234276816248894, -0.013201197609305382, 0.033620886504650116, 0.00933841709047556, 0.057325609028339386, -0.039771128445863724, 0.024898990988731384, -0.04952569678425789, 0.01868751272559166, -0.009222128428518772, 0.028759649023413658, 0.04622277244925499, -0.04479452595114708, -0.007873269729316235, 0.003972833044826984, 0.08234458416700363, 0.047766849398612976, -0.04056665301322937, -0.03526405245065689, 0.028415270149707794, 0.03255781903862953, 0.03818396478891373, -0.04255992919206619, -0.0518893301486969, -0.036653585731983185, 0.020367737859487534, -0.0004731871304102242, 0.030738964676856995, -0.0012389217736199498, 0.0024446682073175907, 0.05554775893688202, 0.013511369936168194, 0.06238795816898346, 0.0018740486120805144, 0.008448319509625435, 0.03504892811179161, -0.04160211980342865, -0.00638029258698225, -0.050224918872117996, -0.005032839719206095, 0.013626458123326302, 0.008170071057975292, -0.04054887965321541, -0.027991971001029015, -0.034977927803993225, 0.014853181317448616, -0.024243168532848358, -0.02605707384645939, -0.05607284978032112, -0.00930757261812687, -0.03581421077251434, -0.0025592155288904905, -0.02390565536916256, -0.0380810908973217, -0.033349402248859406, -0.011960298754274845, -0.04205334186553955, 0.055994290858507156, 0.006477762944996357, 0.00991995818912983, 0.030702101066708565, 0.024755865335464478, 0.07455798983573914, 0.04026920348405838, -0.032499562948942184, -0.0020086164586246014, -0.035176586359739304, 0.0520380362868309, -0.04051459580659866, -0.04515765979886055, 0.04827379435300827, 0.0681162029504776, 0.01639464683830738, -0.012349777854979038, -0.024919848889112473, 0.0261624027043581, -0.0003906007332261652, 0.08279916644096375, -0.04583611711859703, -0.05605150759220123, 0.03352531045675278, 0.05521564930677414, -0.0490930899977684, -0.027380632236599922, 0.02854541875422001, 0.012674689292907715, -0.002931063063442707, -0.053501006215810776, -0.0288915503770113, -0.031300533562898636, -0.03822864219546318, -0.03784075006842613, -0.017202073708176613, -0.005835125222802162, -0.02155722677707672, -0.00019966973923146725, 0.038443099707365036, -0.055773064494132996, 0.0020749655086547136, -0.023329386487603188, -0.006293139886111021, 0.019844476133584976, 0.044987935572862625, 0.03083648905158043, -0.01008790172636509, 0.02570483647286892, 0.002781765768304467, 0.007234315853565931, 0.03924374282360077, -0.014665482565760612, -0.026686524972319603, 0.040999021381139755, 0.0032443120144307613, 0.0015701769152656198, 0.053893182426691055, 0.045284025371074677, 0.023676631972193718, 0.007213208358734846, 0.05706159397959709, -0.06629643589258194, 0.010098790749907494, -0.03431336581707001, 0.0032631983049213886, 0.021331751719117165, -0.05340828001499176, -0.022274604067206383, -0.0436270497739315, 0.019490502774715424, 0.04619332030415535, -0.03779701516032219, -0.004007418639957905, -0.03109969012439251, -0.028927365317940712, -0.01648799516260624, -0.04384511336684227, 0.02099577710032463, -0.02223723568022251, -0.03204162418842316, -0.022197725251317024, -0.011281806975603104, -0.015587741509079933, 0.041081756353378296, 0.044835954904556274, 0.005444942973554134, 0.004057107958942652, 0.0030030268244445324, 0.008095227181911469, 0.010580062866210938, 0.002692743204534054, 0.027165278792381287, -0.04451528936624527, 0.018396437168121338, -0.012467225082218647, 0.01907152123749256, 0.024384012445807457, -0.03359571844339371, 0.002541579073294997, 0.03602273389697075, 0.007620786316692829, 0.0045867254957556725, -0.004530118312686682, 0.039663612842559814, -0.07118961960077286, 0.027506954967975616, 0.010756077244877815, -0.0027250293642282486, 0.020495502278208733, 0.025394247844815254, -0.037594713270664215, 0.017433777451515198, -0.008695165626704693, -0.02986626699566841, 0.012270666658878326, 0.016099197790026665, -0.03702491894364357, -0.03618799149990082, -0.012862246483564377, -0.05321879684925079, 0.03497174754738808, -0.012480347417294979, 0.012074748054146767, 0.021339774131774902, 0.025768857449293137, 0.027273213490843773, 0.006413111928850412, -0.01951907016336918, -0.020737817510962486, 0.03631201386451721, 0.005693630315363407, 0.014690431766211987, 0.0400673933327198, 0.040662623941898346, 0.005502395331859589, 0.0173542071133852, -0.04771675169467926, 0.02629489079117775, -0.003811753587797284, -0.007251544389873743, 0.04416917636990547, -0.061770226806402206, -0.016220202669501305, 0.03719717636704445, -0.01617533341050148, -0.03763722628355026, -0.04766486957669258, -0.025192739441990852, -0.019239462912082672, -0.008094199001789093, -0.005068974103778601, -0.004565471783280373, 0.07414168119430542, -0.041430603712797165, 0.013045714236795902, 0.018361356109380722, 0.03342536836862564, -0.041664931923151016, -0.05250570923089981, -0.02396155521273613, -0.01865033246576786, -0.03515772148966789, 0.016364071518182755, -0.009918905794620514, 0.05359259247779846, -0.0008409120491705835, -0.003592758672311902, 0.030167188495397568, -0.008410172536969185, -0.002642557490617037, -0.026539165526628494, 0.003059686627238989, 0.029047558084130287, 0.01878088153898716, -0.04435497149825096, 0.004310435149818659, 0.05454620346426964, -0.009366977959871292, -0.009419695474207401, 0.031485412269830704, -0.016963956877589226, 0.007122103590518236, 0.009967380203306675, 0.024634771049022675, 0.022842811420559883, -0.0031100360210984945, -0.0643397867679596, -0.02673438936471939, -0.035047076642513275, -0.00703747384250164, 0.049209486693143845, -0.050232626497745514, 0.04699047654867172, 0.026202980428934097, -0.016680346801877022, -0.02222183719277382, -0.041290491819381714, -0.03719739243388176, -0.0399385504424572, 0.016396909952163696, -0.02527480199933052, 0.018787767738103867, 0.0006646675174124539, -0.01867998018860817, 0.03068104200065136, -0.0020506661385297775, -0.044911354780197144, -0.012430372647941113, -0.024320200085639954, 0.017804201692342758, -0.032045990228652954, -0.007124380674213171, 0.01598973572254181, 0.012343218550086021, -0.011404650285840034, -0.01943874917924404, 0.04149242863059044, -0.005655502434819937, -0.016777103766798973, -0.011175236664712429, -0.04135586693882942, -0.03618283197283745, 0.02853410318493843, 0.044518813490867615, 0.06200827658176422, 0.038675494492053986, -0.036812301725149155, -0.02377907745540142, 0.025207694619894028, 0.040055833756923676, 0.04191999137401581, 0.009881209582090378, 0.02629314549267292, 0.024176236242055893, 0.020148472860455513, 0.008412880823016167, -0.004142292309552431, -0.004587574861943722, -0.03939927741885185, -0.01107271108776331, 0.026553431525826454, -0.04704265668988228, 0.0134046645835042, 0.03654501587152481, 0.017579609528183937, -0.00949288159608841, 0.06082817539572716, -0.006728034000843763, 0.008356080390512943, -0.051711030304431915, 0.03844171389937401, -0.01830201968550682, 0.00854733120650053, 0.034922968596220016, 0.004259245935827494, 0.020917760208249092, 0.010708301328122616, -0.02309831604361534, -0.04656030982732773, -0.04652083292603493, 0.05957057699561119, 0.02332383207976818, 0.027773132547736168, -0.02072453871369362, 0.01621401123702526, 0.01397494412958622, -0.018857672810554504, -0.027436040341854095, -0.002191656269133091, -0.004994520451873541, -0.004126602318137884, -0.017894916236400604, -0.01036886591464281, -0.04088125005364418, -0.001628147903829813, -0.038663312792778015, 0.018833275884389877, -0.030392177402973175, 0.008629074320197105, -0.028720470145344734, 0.02279716171324253, -0.01806335523724556, 0.008062546141445637, 0.010905652306973934, 0.03126005455851555, 0.04747697710990906, 0.04772476851940155, 0.0025600406806916, -0.027565544471144676, -0.02668774127960205, 0.04895208775997162, 0.035160355269908905, -0.003617622423917055, -0.024726560339331627, 0.0027959730941802263, 0.032439906150102615, -0.02167009562253952, 0.018054930493235588, -0.01872582919895649, 0.0019295919919386506, -0.02558404952287674, 0.006836448796093464, -0.0369705893099308, -0.02392488345503807, -0.0006662389496341348, -0.03071492351591587, -0.035662174224853516, -0.027658283710479736, 0.03889033943414688, -0.0037014412228018045, 0.026146110147237778, -0.012518925592303276, -0.020388586446642876, -0.05085292086005211, 0.04047977924346924, -0.04683171212673187, -0.025453921407461166, -0.022075410932302475, 0.04574955627322197, -0.04705757275223732, 0.014057472348213196, -0.0019396235002204776, 0.02185961976647377, -0.0364665687084198, 0.029328808188438416, 0.03975827246904373, -0.025506163015961647, -0.048030223697423935, 0.020831944420933723, -0.010402142070233822, -0.045267313718795776, -0.009884222410619259, -0.03742368519306183, 0.040443453937768936, 0.03946416452527046, -0.01560758426785469, 0.05435897037386894, -0.01847420074045658, -0.022371601313352585, -0.02834167331457138, -0.024244116619229317, 0.016461217775940895, -0.015590337105095387, -0.00129281438421458, -0.04336583614349365, 0.031417686492204666, 0.031635694205760956, -0.04004405438899994, -0.030732493847608566, -0.006123801227658987, 0.04235366731882095, 0.01829519309103489, -0.04180118814110756, 0.022241240367293358, 0.007255924865603447, -0.011486359871923923, 0.03379999101161957, -0.0004169400199316442, -0.0184877160936594, 0.028425395488739014, 0.028919873759150505, 0.02586987614631653, 0.024930693209171295, 0.014547272585332394, -0.026854408904910088, -0.024916205555200577, 0.021643545478582382, 0.020358487963676453, -0.06973747164011002, 0.03774351254105568, -0.057213641703128815, 0.019329674541950226, 0.058849673718214035, -0.01767851412296295, -0.04142051935195923, 0.029757970944046974, 0.03226760774850845, 0.013497359119355679, 0.04983055964112282, -0.06860023736953735, -0.009090565145015717, 0.01876700483262539, -0.045754898339509964, -0.018559377640485764, 0.03907673433423042, -0.04343864694237709, 0.01983906328678131, 0.05090088024735451, -0.026130903512239456, 0.0198112390935421, -0.01724187843501568, 0.027581052854657173, -0.015150460414588451, 0.044671617448329926, -0.021948102861642838, -0.039155758917331696, -0.011670168489217758, 0.057173795998096466, 0.05130016431212425, 0.015086568892002106, 0.029861880466341972, 0.00045294041046872735, 0.012837855145335197, -0.013540389016270638, -0.0035288180224597454, -0.006696672644466162, 0.005161927081644535, -0.007997330278158188, -0.06018849462270737, 0.0040719653479754925, 0.04676881060004234, 0.05494493618607521, -0.02383231744170189, -0.02409941330552101, -0.02361765317618847, 0.05019611492753029, -0.02561134286224842, -0.011039866134524345, -0.0035623686853796244, -0.007303849793970585, -0.05598662421107292, -0.014519513584673405, -0.016709184274077415, 0.023226818069815636, 0.0020540591794997454, 0.018423371016979218, 0.07162746787071228, -0.030987678095698357, -0.011564436368644238, -0.05398257449269295, -0.03393613547086716, 0.029724063351750374, 0.014501385390758514, 0.01975683681666851, 0.025587748736143112, -0.0031189853325486183, -0.028188880532979965, -0.028325166553258896, -0.058148093521595, 0.06817413866519928, -0.05185883492231369, 0.010479864664375782, 0.013737188652157784, -0.025603286921977997, 0.004288510885089636, 0.03330473229289055, 0.03081020526587963, 0.00929239485412836, 0.013587307184934616, 0.04477598890662193, -0.010759776458144188, -0.013349821791052818, -0.03450658544898033, 0.04914005845785141, -0.008455680683255196, -0.02566939778625965, 0.06540139764547348, -0.014167072251439095, -0.04206572845578194, 0.040725916624069214, -0.027056824415922165, -0.027743373066186905, 0.01723734475672245, -0.026086552068591118, 0.011066379956901073, -0.028094105422496796, 0.021090257912874222, -0.006315530743449926, -0.0038536370266228914, 0.002642259933054447, -0.027106409892439842, -0.04236611723899841, 0.034201622009277344, 0.02585034817457199, 0.029318731278181076, 0.04393518716096878, -0.021400194615125656, -0.01966845989227295, 0.037550002336502075, -0.01055433414876461, 0.013854894787073135, -0.02539399452507496, -0.029923539608716965, 0.011284036561846733, 0.03166112303733826, -0.05249398201704025, 0.023592980578541756, -0.032214708626270294, 0.048454105854034424, -0.03129222244024277, 0.019876103848218918, 0.027158288285136223, -0.010018077678978443, -0.015372109599411488, -0.03651623800396919, 0.030494078993797302, 0.030631577596068382, 0.013395410031080246, -0.04203599691390991, 0.017429262399673462, -0.0336260125041008, -0.00458034360781312, 0.04320629686117172, 0.04873400554060936, -0.036522023379802704, -0.007033052854239941, 0.04841015860438347, 0.04686618968844414, -0.0226470734924078, -0.0156414732336998, 0.03650740534067154, -0.02229906991124153, 0.04247375950217247, 0.02045833133161068, 0.0011794024612754583, -0.02260892651975155, -0.034504398703575134, 0.044768236577510834, -0.031263917684555054, -0.045521024614572525, 0.005588601343333721, -0.056517187505960464, 0.03272703289985657, 0.03745061159133911, 0.048094216734170914, 0.04812173917889595, -0.051041409373283386, 0.017124954611063004, -0.028181584551930428, 0.0380525179207325, -0.0037846474442631006, -0.03125273063778877, 0.01894977129995823, -0.040786415338516235, -0.04169308394193649, 0.05256915092468262, -0.03413494676351547, 0.02746882289648056, 0.033805206418037415, 0.007925352081656456, 0.02444133721292019, 0.018344828858971596, -0.009217273443937302, 0.0021543705370277166, 0.03575904294848442, 0.020160242915153503, 0.02286422997713089, -0.020686518400907516, 0.026341790333390236, 0.12740176916122437, -0.00893761869519949, 0.020138844847679138, -0.02238314412534237, 0.018193142488598824, 0.03919895738363266, -0.022538235411047935, 0.025507012382149696, -0.023426273837685585, 0.027672510594129562, 0.04479461535811424, 0.051694680005311966, 0.024090928956866264, 0.05755620077252388, -0.014703532680869102, 0.017124010249972343, -0.01627686806023121, -0.0018241779180243611, -0.02017812617123127, -0.013297722674906254, 0.02810887061059475, -0.03654468432068825, -0.040724869817495346, -0.02429257705807686, 0.02536279335618019, -0.004012022167444229, -0.0012858447153121233, -0.04617433249950409, 0.017472991719841957, 0.05619405210018158, 0.003411441808566451, 0.02130633033812046, -0.03523368015885353, -0.017101695761084557, -0.020463088527321815, -0.025893187150359154, -0.04365244880318642, 0.03732496127486229, 0.003834214061498642, -0.02485889010131359, 0.023965951055288315, 0.04507003724575043, -0.039340391755104065, -0.01395449135452509, 0.043282754719257355, -0.0028774579986929893, 0.05358149856328964, -0.02236671932041645, 0.005119263660162687, 0.04382428526878357, 0.013711007311940193, -0.01928921416401863, -0.04386340454220772, 0.013973870314657688, -0.01751251146197319, -0.021039115265011787, 0.018643466755747795, 0.021641723811626434, -0.019717521965503693, -0.04447349160909653, -0.045124512165784836, 0.001166265457868576, 0.027790851891040802, 0.0349087193608284, -0.05426891893148422, -0.020197005942463875, 0.013967194594442844, 0.020221564918756485, 0.02676599659025669, -0.05146752670407295, 0.028382916003465652, -0.03574734181165695, -0.0307468231767416, 0.0271960087120533, -0.060862455517053604, -0.019536489620804787, 0.029809515923261642, 0.005997729953378439, -0.003236045129597187, -0.02296912670135498, -0.012584153562784195, 0.020569052547216415, -0.0398726686835289, 0.012893066741526127, 0.05747450888156891, -0.04004427418112755, -0.0016912904102355242, 0.03588302433490753, -0.040752533823251724, -0.021931834518909454, -0.03274866193532944, 0.02146611548960209, -0.01936306245625019, 0.017079008743166924, -0.007505142129957676, 0.06527362018823624, -0.020604269579052925, 0.01886555179953575, -0.00841802079230547, 0.03616372495889664, 0.020610619336366653, 0.04090982303023338, -0.00286914873868227, 0.01186350453644991, -0.015292976051568985, -0.03638255596160889, 0.028694557026028633, 0.002353183226659894, 0.017770130187273026, 0.01283283717930317, 0.012778712436556816, 0.02851232700049877, 0.008139816112816334, 0.03719107434153557, -0.03728439658880234, 0.012673329561948776, 0.03146594390273094, -0.019588639959692955, 0.006983111146837473, 0.03652822971343994, 0.01976114511489868, 0.043489858508110046, 0.04203694313764572, -0.0072769043035805225, -0.031777746975421906, 0.025811728090047836, -0.02263280190527439, -0.024895695969462395, -0.024923745542764664, -0.021317312493920326, 0.025197185575962067, -0.031068982556462288, -0.03568267449736595, -0.014184455387294292, -0.018119284883141518, 0.04019543156027794, 0.027073360979557037, 0.019060516729950905, -0.032629843801259995, 0.01988561637699604, -0.046449366956949234, -0.026410577818751335, -0.03311290591955185, -0.04030410945415497, 0.0073549640364944935, 0.03774447366595268, -0.02418743632733822, 0.03917284309864044, 0.037112291902303696, 0.04895860329270363, -0.03377828747034073, -0.021235689520835876, -0.008727184496819973, 0.012683587148785591, 0.025314902886748314, 0.02410356141626835, 0.06801478564739227, -0.02123037353157997, -0.0036285393871366978, 0.043283384293317795, 0.025348279625177383, 0.03189971670508385, -0.04675361514091492, -0.008785491809248924, -0.01910555362701416, -0.03074111044406891, -0.032706547528505325, -0.02195163257420063, -0.025955626741051674, -0.013656404800713062, -0.0017331260023638606, 0.04122364893555641, -0.003353940322995186, 0.003944667987525463, 0.02557617612183094, 0.033677756786346436, -0.0069642821326851845, -0.037354182451963425, -0.003706785151734948, 0.018041592091321945, 0.023099323734641075, 0.005993315484374762, -0.023070724681019783, -0.003592608030885458, 0.01580343022942543, -0.021968074142932892, -0.055538859218358994, 0.022645818069577217, 0.008289964869618416, 0.007825851440429688, 0.029414691030979156, 0.00884107407182455, -0.029503418132662773, -0.00685040233656764, -0.04297958314418793, -0.0067012435756623745, 0.0408974252641201, 0.03618587553501129, 0.008660215884447098, 0.005283386912196875, 0.02173357643187046, 0.016801103949546814, 0.014148939400911331, 0.04589837044477463, -0.07519549131393433, 0.0008492959314025939, -0.03911217674612999, 0.01957186684012413, 0.04065250977873802, -0.017508283257484436, -0.022696377709507942, -0.023634128272533417, 0.023389438167214394, 0.005290127359330654, 0.030684763565659523, 0.027884958311915398, 0.0007517277263104916, -0.01770036853849888, 0.011700128205120564, -0.010737712495028973, 0.04028428718447685, 0.023366102948784828, -0.03046400658786297, -0.023285800591111183, -0.047068167477846146, -0.019877061247825623, 0.005805285647511482, 0.026605457067489624, 0.014891230501234531, -0.04306803271174431, 0.06144380196928978, 0.02467053383588791, 0.009495215490460396, 0.02271324396133423, 0.022022845223546028, -0.0344187393784523, 0.034510448575019836, 0.035804156213998795, -0.02319440431892872, -0.012169327586889267, 0.027845632284879684, -0.03938868269324303, 0.014688534662127495, -0.0326634906232357, 0.035405971109867096, 0.09329289197921753, -0.021386025473475456, -0.005334523506462574, -0.013434350490570068, 0.03625260666012764, -0.035231638699769974, 0.01714746095240116, -0.010116915218532085, -0.015791669487953186, 0.03326767310500145, 0.06997721642255783, 0.02518271654844284, -0.025026991963386536, -0.033969491720199585, 0.006438582669943571, -0.010330992750823498, 0.022015077993273735, 0.035159625113010406, 0.03975333645939827, -0.09261862188577652, -0.030692344531416893, -0.007167263887822628, 0.030045578256249428, 0.04790728539228439, 0.037478335201740265, 0.008102349936962128, 0.023854972794651985, -0.02429278939962387, -0.007957016117870808, -0.029902758076786995, -0.03795326128602028, 0.02096726931631565, -0.009724924340844154, 0.05335197597742081, -0.031247764825820923, -0.029010239988565445, 0.03935291990637779, 0.0004320724692661315, -0.015092982910573483, -0.005163619294762611, 0.0006704957922920585, 0.019765393808484077, -0.0023283318150788546, 0.008752129971981049, 0.011935974471271038, 0.041026521474123, -0.02911226823925972, -0.00707174651324749, -0.05678257718682289, 0.018372097983956337, -0.007054861634969711, -0.03395901620388031, 0.029341937974095345, 0.011115530505776405, 0.04074866324663162, -0.015816841274499893, 0.05173950269818306, -0.015220806002616882, -0.04581461101770401, -0.03737398982048035, -0.0463782399892807, -0.04844944179058075, 0.011588484980165958, 0.007512776181101799, 0.012602098286151886, 0.008942355401813984, -0.019375091418623924, -0.00784570537507534, -0.016177771613001823, 0.020358964800834656, 0.030678516253829002, 0.058078765869140625, -0.05181894451379776, -0.007111021783202887, 0.004367280285805464, 0.04949195310473442, 0.024104589596390724, 0.013972863554954529, -0.022043360397219658, 0.02481302246451378, -0.02353677898645401, -0.019958026707172394, -0.03086337447166443, -0.0342184379696846, -0.042619749903678894, -0.013002551160752773, 0.031215941533446312, -0.029242197051644325, -0.039825692772865295, -0.04158396273851395, 0.019694391638040543, 0.02995566837489605, 0.028752531856298447, -0.02492348663508892, -0.006047347094863653, 0.04264743626117706, -0.006050477270036936, -0.012319333851337433, -0.02809244394302368, -0.04274909943342209, -0.015643877908587456, 0.0013809194788336754, 0.022640932351350784, -0.03112141042947769, 0.029752563685178757, -0.013332386501133442, 0.022779937833547592, 0.026944121345877647, 0.0026380603667348623, -0.04207104071974754, 0.022562168538570404, -0.01429622434079647, 0.044384222477674484, 0.004642078652977943]\n\n\n\n\nSymantic Search\nIf we were using the LLM module‚Äôs preferred structures for Collection and storing data in SQLite, we could simply use llm similar or its corresponding Python API. As far as I can tell, the API doesn‚Äôt yet support other data structures of embeddings (like our Dataframe), so we‚Äôll have to calculate cosine similarities ourselves. Lucky for us, we can borrow from Simon‚Äôs open source library:\n\ndef cosine_similarity(a, b):\n    dot_product = sum(x * y for x, y in zip(a, b))\n    magnitude_a = sum(x * x for x in a) ** 0.5\n    magnitude_b = sum(x * x for x in b) ** 0.5\n    return dot_product / (magnitude_a * magnitude_b)\n\nNow, iterate over every embedding in our Dataframe and calculate the similarity score against our query embedding vector:\n\ncomp_df = df.copy()\ncomp_df['similarity'] = comp_df.apply(\n    lambda row : cosine_similarity(\n        query_vector,\n        row.embedding,\n    ),\n    axis=1,\n)\n\ncomp_df.head()\n\n\n    \n\n\n\n\n\n\npassage\nembedding\nsimilarity\n\n\n\n\n0\nBy the authority vested in me as President by ...\n[0.032344698905944824, -0.04333016648888588, 0...\n0.781552\n\n\n1\n(a) Artificial Intelligence must be safe and s...\n[0.01886950619518757, -0.057347141206264496, 0...\n0.778486\n\n\n2\n(c) The responsible development and use of AI ...\n[0.0486459881067276, -0.0712570995092392, 0.02...\n0.779455\n\n\n3\n(e) The interests of Americans who increasingl...\n[0.03564070537686348, -0.04887280985713005, 0....\n0.794971\n\n\n4\n(g) It is important to manage the risks from t...\n[0.04095401614904404, -0.042341429740190506, 0...\n0.785406\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAnd select the 3 passages with the best similary scores. We‚Äôll feed this as context to the LLaMA model.\n\nbest_3_matches = comp_df.sort_values(\"similarity\", ascending = False).head(3)\ncontext = \"\\n\".join(best_3_matches.passage.values)\n\n\n\nAsk the LLM\nNow that we‚Äôve selected the top 3 passages, let‚Äôs feed them into LLaMA 2.\n\nmodel = llm.get_model(\"llama2\")\n\nEven though we‚Äôre providing prefixed context to the model, it‚Äôs helpful to give it a system prompt to guide how it responds. This can help it stay ‚Äúfocussed‚Äù on the context and respond in the voice that we expect. The system prompt is open to creativity and experimentation.\n\nsystem = \"You are an assistant. You answer questions in a single \\\nparagraph about the policy from President Biden. The provided context \\\ncomes directly from the policy. You MUST use the provided information \\\nas context. Not all provided information will be helpful, ONLY reference \\\ninformation if it is related to my query. You may quote the context \\\ninformation if helpful.\"\n\nNow, feed the context and the query into the model.\n\nprint(f\"Query: {query}\\n\")\nresponse = model.prompt(\n    f'{context}\\n{query}',\n    system=system,\n)\n\nprint(f\"Response:\\n\")\nprint(response.text())\n\nQuery: what does it say about healthcare?\n\nResponse:\n\nThe policy from President Biden related to healthcare is outlined in section 8(b)(i) of the policy, which states that:\n\"Within 90 days of the date of this order, the Secretary of HHS shall, in consultation with the Secretary of Defense and the Secretary of Veterans Affairs, establish an HHS AI Task Force that shall, within 365 days of its creation, develop a strategic plan that includes policies and frameworks ‚Äî possibly including regulatory action, as appropriate ‚Äî on responsible deployment and use of AI and AI-enabled technologies in the health and human services sector (including research and discovery, drug and device safety, healthcare delivery and financing, and public health), and identify appropriate guidance and resources to promote that deployment, including in the following areas:\n(A) development, maintenance, and use of predictive and generative AI-enabled technologies in healthcare delivery and financing ‚Äî including quality measurement, performance improvement, program integrity, benefits administration, and patient experience ‚Äî taking into account considerations such as appropriate human oversight of the application of AI-generated output;\n(D) incorporation of safety, privacy, and security standards into the software-development lifecycle for protection of personally identifiable information, including measures to address AI-enhanced cybersecurity threats in the health and human services sector; (E) development, maintenance, and availability of documentation to help users determine appropriate and safe uses of AI in local settings in the health and human services sector; (F) work to be done with State, local, Tribal, and territorial health and human services agencies to advance positive use cases and best practices for use of AI in local settings; and (G) identification of uses of AI to promote workplace efficiency and satisfaction in the health and human services sector, including reducing administrative burdens.\"\nThis section outlines the responsibilities of the Secretary of HHS related to AI in the healthcare sector. Some of the key points include:\n* Establishing an HHS AI Task Force to develop a strategic plan for responsible AI deployment and use in the health and human services sector.\n* Identifying policies and frameworks for regulatory action, as appropriate, to ensure responsible deployment and use of AI in healthcare.\n* Developing guidance and resources to promote the appropriate and safe use of AI in healthcare\n\n\nOverall, this looks like it does a good job!\nOf course, it‚Äôs extremely important to keep a human in the loop when referencing government documents. The model may still hallucinate, or it could entirely miss important context. Some of these shortcoming are baked into the model itself, others are implementation details of this post.\nIf nothing else, this shows a fascinating interface to interact with long, wordy, documents!\n\n\nLicense\nCopyright (c) 2023, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of Language Models on the AI Executive Order nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/2025-08-02-language-model-embeddings-campaign-donors/index.html",
    "href": "posts/2025-08-02-language-model-embeddings-campaign-donors/index.html",
    "title": "Using LLM Embeddings to Normalize User Data",
    "section": "",
    "text": "A common challenge in working with operational or CRM-style data is that you often find yourself dealing with user-entered free text. A recurrent example comes when users fill out forms that ask for their job title and employer. This data might seem secondary, but for any organization trying to understand, segment, or personalize communication with its users, it‚Äôs incredibly valuable.\nOf course, user-entered data is messy. One person types ‚Äúnurse,‚Äù another ‚ÄúER Nurse,‚Äù another ‚ÄúRN,‚Äù and yet another ‚Äúhome health nursing.‚Äù If you want to understand the composition of your user base, or build automated systems that adapt to it, you need to normalize that chaos into a finite and meaningful taxonomy. You wouldn‚Äôt want a dashboard full of job titles in SpongeBob casing, but realistically, you have to work with whatever comes through the form.\nA data-forward organization might use this information for all kinds of purposes: tailoring outreach, prioritizing leads, enriching analytics, or even customizing onboarding flows. But none of that works without being clean, consistent, and structured.\nWith language models, we can do better. We don‚Äôt need to predefine normalization rules or manually review each row. And we don‚Äôt even need much prior knowledge about our users to start.\nBut before we jump straight to the AI, let‚Äôs define our approach:\n\nTo normalize free-form text, there must be a finite set of target categories.\nIf we know nothing about users in advance, we need a reliable way to discover or define those categories.\nWe‚Äôre decidedly not using a chatbot, and we‚Äôre not relying on external APIs.\nThis isn‚Äôt a generative task; it‚Äôs about semantic understanding.\n\nLet‚Äôs start at the top. There are an infinite number of values a user could enter for their job, and we want to reduce that to a finite set. So where do we get that set?\nThe Occupational Information Network (O*NET) maintains exactly such a resource. Developed under the sponsorship of the Departoment of Labor, O*NET offers rich datasets that describe skills, knowlege, tasks, and job titles. We‚Äôre interested in the Alternate Titles file, which maps occupation titles to alternate ‚Äúlay‚Äù job titles. There‚Äôs a good chance many of our users enter these alternate titles, so we‚Äôll want to include them.\nThe file includes columns of Department of Labor and Census identifiers, but we only need the few that focus on title. Let‚Äôs download it and take a look at a few examples:\n\nimport numpy as np\nimport pandas as pd\n\nonet_df = pd.read_excel(\n    \"https://www.onetcenter.org/dl_files/database/db_29_3_excel/Alternate%20Titles.xlsx\",\n    usecols=['Title', 'Alternate Title', 'Short Title'],\n).fillna(\"\")\n\nonet_df.sample(n=5, random_state=101)  # seed for reproducibility\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\n\n\n\n\n25268\nCargo and Freight Agents\nShipping Agent\n\n\n\n30407\nHelpers, Construction Trades, All Other\nMaintenance Construction Helper\n\n\n\n6045\nBioengineers and Biomedical Engineers\nBiomedical Engineering Intern\n\n\n\n18684\nOccupational Therapy Aides\nRehabilitation Therapy Aide (Rehab Therapy Aide)\nRehab Therapy Aide\n\n\n24086\nBilling and Posting Clerks\nStatement Services Representative (Statement S...\nStatement Services Rep\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nSo O*NET tells us that Cargo and Freight Agents might also go by a Shipping Agent as an Alternate Title and that Occupational Therapy Aides might also go by a Rehab Therapy Aide as a Short Title.\nWe also see that there can be many rows of different Alternate Title and Short Title for the same Title:\n\nonet_df[onet_df[\"Title\"] == \"Software Developers\"]\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\n\n\n\n\n4931\nSoftware Developers\n.NET Developer\n\n\n\n4932\nSoftware Developers\nAndroid Developer\n\n\n\n4933\nSoftware Developers\nAngularJS Developer\n\n\n\n4934\nSoftware Developers\nApache Hadoop Developer\n\n\n\n4935\nSoftware Developers\nApplication Architect\n\n\n\n...\n...\n...\n...\n\n\n5061\nSoftware Developers\nUser Interface Designer\n\n\n\n5062\nSoftware Developers\nVideo Game Engineer\n\n\n\n5063\nSoftware Developers\nWide Area Network Engineer (WAN Engineer)\nWAN Engineer\n\n\n5064\nSoftware Developers\nWindows Software Engineer\n\n\n\n5065\nSoftware Developers\nXML Developer (Extensible Markup Language Deve...\nXML Developer\n\n\n\n\n135 rows √ó 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThere is one thing we do know ahead of time about our users: not all of them will be employed. The O*NET data set doesn‚Äôt provide a job title for not working, so let‚Äôs add our own:\n\nadditions = pd.DataFrame(\n    [\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"Not Employed\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"None\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"N/A\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"No Employment\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"Not Working\"},\n        {\"Title\": \"Retired\", \"Alternate Title\": \"Retiree\"},\n    ]\n)\nonet_df = pd.concat([onet_df, additions], ignore_index=True).fillna(\"\")\nonet_df[(onet_df[\"Title\"] == \"Unemployed\") | (onet_df[\"Title\"] == \"Retired\")]\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\n\n\n\n\n56560\nUnemployed\nNot Employed\n\n\n\n56561\nUnemployed\nNone\n\n\n\n56562\nUnemployed\nN/A\n\n\n\n56563\nUnemployed\nNo Employment\n\n\n\n56564\nUnemployed\nNot Working\n\n\n\n56565\nRetired\nRetiree\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nNow let‚Äôs merge these fields together. Since we‚Äôll be leveraging a language model, we can take the liberties of language here; we don‚Äôt need clean many-to-many relationships. Just combine Title, Alternate Title, and when available, Short Title into one Long Title field with \"aka\" inline:\n\nmask = onet_df[\"Short Title\"].eq(\"\")\n\nonet_df[\"Long Title\"] = np.where(\n    mask,\n    onet_df[\"Title\"] + \" aka \" + onet_df[\"Alternate Title\"],\n    onet_df[\"Title\"] + \" aka \" + onet_df[\"Alternate Title\"] + \" aka \" + onet_df[\"Short Title\"],\n)\n\nonet_df[onet_df[\"Title\"] == \"Software Developers\"]\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\nLong Title\n\n\n\n\n4931\nSoftware Developers\n.NET Developer\n\nSoftware Developers aka .NET Developer\n\n\n4932\nSoftware Developers\nAndroid Developer\n\nSoftware Developers aka Android Developer\n\n\n4933\nSoftware Developers\nAngularJS Developer\n\nSoftware Developers aka AngularJS Developer\n\n\n4934\nSoftware Developers\nApache Hadoop Developer\n\nSoftware Developers aka Apache Hadoop Developer\n\n\n4935\nSoftware Developers\nApplication Architect\n\nSoftware Developers aka Application Architect\n\n\n...\n...\n...\n...\n...\n\n\n5061\nSoftware Developers\nUser Interface Designer\n\nSoftware Developers aka User Interface Designer\n\n\n5062\nSoftware Developers\nVideo Game Engineer\n\nSoftware Developers aka Video Game Engineer\n\n\n5063\nSoftware Developers\nWide Area Network Engineer (WAN Engineer)\nWAN Engineer\nSoftware Developers aka Wide Area Network Engi...\n\n\n5064\nSoftware Developers\nWindows Software Engineer\n\nSoftware Developers aka Windows Software Engineer\n\n\n5065\nSoftware Developers\nXML Developer (Extensible Markup Language Deve...\nXML Developer\nSoftware Developers aka XML Developer (Extensi...\n\n\n\n\n135 rows √ó 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nGreat. We‚Äôve satisfied parts one and two of our approach. We have a finite set of job titles, and we have a good understanding that the set is large but not exhaustive, and it combines multiple values for a job title. Let‚Äôs start modeling language.\nJobBERT-v2 is a sentence-transformers model fine tuned from all-mpnet-base-v2 specifically for job title matching and similarity. Hey that‚Äôs convenient!\nWe can‚Äôt use JobBERT out of the box, we‚Äôll need to incorporate our O*NET dataset. Let‚Äôs pull it down and start building out our implementation of the model. To do this we‚Äôre going to leverage word embeddings against our Long Title values. If you‚Äôre unfamiliar with langage model embeddings, Simon Willison has a fantastic overview that you should go read now. But the gist of it is: embeddings are how language models numerically encode meaning from language into a large vector. This is suprisingly powerful, and yields operations like:\nemb('king') - emb('man') + emb('woman') which returns a vector that is mathmatically very close to emb('queen').\nWe‚Äôre going to use this ‚Äúcloseness‚Äù between vectors to reduce infinite free-form data to our finite Long Title data and then map it back to Title. The first thing to do is quite simple: calculate JobBERT embeddings on all of the values in our Long Title column:\n\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodel = SentenceTransformer(\"TechWolf/JobBERT-v2\")\n\nonet_df[\"embedding\"] = list(\n    model.encode(\n        onet_df[\"Long Title\"].tolist(),\n        normalize_embeddings=True,\n        convert_to_numpy=True,\n        show_progress_bar=False,\n    )\n)\n\nonet_df[[\"Long Title\", \"embedding\"]].sample(n=5, random_state=101)\n\n\n    \n\n\n\n\n\n\nLong Title\nembedding\n\n\n\n\n25268\nCargo and Freight Agents aka Shipping Agent\n[-0.034480397, -0.01120864, -0.005822623, -0.0...\n\n\n30408\nHelpers, Construction Trades, All Other aka Me...\n[-0.07022176, -0.020068161, 0.0111531215, -0.0...\n\n\n6045\nBioengineers and Biomedical Engineers aka Biom...\n[-0.030613927, -0.059320696, -0.01718829, -0.0...\n\n\n18684\nOccupational Therapy Aides aka Rehabilitation ...\n[-0.014956265, -0.038792193, -0.00349255, 0.00...\n\n\n24086\nBilling and Posting Clerks aka Statement Servi...\n[0.020981414, -0.033710796, 0.03225505, -0.011...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nAs a reader, the embedding column is an indecipherable array of floats, but now we can do some cool things. Here are three rows from our data:\n\nslice = onet_df[\n    (onet_df[\"Long Title\"] == \"Software Developers aka Video Game Engineer\") |\n    (onet_df[\"Long Title\"] == \"Database Architects aka Information Architect\") |\n    (onet_df[\"Long Title\"] == \"Cargo and Freight Agents aka Shipping Agent\")\n]\nslice[[\"Long Title\", \"embedding\"]]\n\n\n    \n\n\n\n\n\n\nLong Title\nembedding\n\n\n\n\n4806\nDatabase Architects aka Information Architect\n[0.020110216, 0.06301323, -0.029263753, -0.022...\n\n\n5062\nSoftware Developers aka Video Game Engineer\n[0.041696787, 0.024444718, -0.053837907, 0.031...\n\n\n25268\nCargo and Freight Agents aka Shipping Agent\n[-0.034480397, -0.01120864, -0.005822623, -0.0...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nIn a vector space, you can evaluate how similar two vectors are by taking their cosine similarity. Since we normalized our vectors when we embedded them, the denominator in the cosine function becomes 1 so we can do this even more efficiently with just a dot product:\n\ndatabase_architect = slice.iloc[0][\"embedding\"]\nsoftware_developer = slice.iloc[1][\"embedding\"]\ncargo_and_freight_agent = slice.iloc[2][\"embedding\"]\n\nprint(f\"Software Developer vs Data Architect: {software_developer @ database_architect}\")\nprint(f\"Software Developer vs Cargo Agent: {software_developer @ cargo_and_freight_agent}\")\n\nSoftware Developer vs Data Architect: 0.2380954772233963\nSoftware Developer vs Cargo Agent: 0.08600345253944397\n\n\nThose numbers look perfectly reasonable: a modest overlap (‚âà 0.24) between two tech roles and an almost-orthogonal relationship (‚âà 0.09) to the Cargo Agent job.\nGreat. So now we have a mathematical way to compare the language of two job titles. And we‚Äôre not touching chatbots or 3rd party APIs at inferrence. The DataFrame is a self-contained model for semantically matching across the O*NET dataset. We‚Äôve fully satisfied our approach!\nNow to apply it to our problem. Instead of evaluating O*NET data against itself, we can use our embeddings to evaluate any free-form job title text a user might submit.\nLet‚Äôs go get some real data to try it out! Since I work in political tech, I like to reach for campaign donor data.\nOur friends over at ProPublica publish itemized ActBlue receipts by state. Because ActBlue is a conduit committee, these files include every transaction of any amount. That‚Äôs a lot of transactions! Let‚Äôs grab all of the ActBlue transactions from Texas for June 2024.\n\ndtypes = {\n    \"flag_orgind\": \"string\",\n    \"first_name\": \"string\",\n    \"city\": \"string\",\n    \"zip\": \"string\",\n    \"amount\": \"float64\",\n    \"occupation\": \"string\",\n}\n\nabtx_df = pd.read_csv(\n    \"https://pp-projects-static.s3.amazonaws.com/itemizer/sa_1791562_tx.csv\",\n    usecols=dtypes.keys(),\n    dtype=dtypes,\n)\nabtx_df = abtx_df[abtx_df[\"flag_orgind\"] == \"IND\"]\nabtx_df.drop(columns=[\"flag_orgind\"], inplace=True)\nabtx_df.dropna(inplace=True)\n\nemployed = lambda df: df[\"occupation\"].ne(\"NOT EMPLOYED\")\nget_sample = lambda seed: abtx_df.loc[employed(abtx_df)].sample(n=10, random_state=seed)\n\nget_sample(30330)  # seed for reproducibility\n\n\n    \n\n\n\n\n\n\nfirst_name\ncity\nzip\namount\noccupation\n\n\n\n\n139133\nPAT\nNORTH RICHLAND HIL\n76180\n9.0\nRETIRED\n\n\n4248\nHARRY\nHOUSTON\n77019\n125.0\nLAWYER\n\n\n221073\nTHAO\nHOUSTON\n77083\n2.5\nHISTOLOGY TECHNICIAN\n\n\n201669\nPHILIP\nSAN ANTONIO\n78240\n3.0\nMANAGER\n\n\n256487\nMARIA\nHOLLAND\n76534\n1.0\nHEALTHCARE ADMINISTRATOR\n\n\n6330\nELLEN\nBELLAIRE\n77401\n100.0\nARBITRATOR\n\n\n125976\nMICHELLE\nFORT WORTH\n76133\n10.0\nSERVER\n\n\n13659\nROSE\nHOUSTON\n77024\n75.0\nNURSING\n\n\n138213\nPETER\nHOUSTON\n77019\n9.0\nDATABASE ANALYST\n\n\n62939\nERIN\nFRISCO\n75035\n25.0\nNONPROFIT\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThat occupation field came from donors and doesn‚Äôt perfectly match our modeled job titles. But we don‚Äôt need it to! Let‚Äôs use our model to calculate embeddings on these new values:\n\nabtx_df[\"embedding\"] = list(\n    model.encode(\n        abtx_df[\"occupation\"].tolist(),\n        normalize_embeddings=True,\n        convert_to_numpy=True,\n        show_progress_bar=False,\n    )\n)\n\nget_sample(30330)\n\n\n    \n\n\n\n\n\n\nfirst_name\ncity\nzip\namount\noccupation\nembedding\n\n\n\n\n139133\nPAT\nNORTH RICHLAND HIL\n76180\n9.0\nRETIRED\n[-0.022304475, 0.08798518, 0.008374137, 0.0150...\n\n\n4248\nHARRY\nHOUSTON\n77019\n125.0\nLAWYER\n[0.0123939905, 0.06054912, 0.0046267705, -0.03...\n\n\n221073\nTHAO\nHOUSTON\n77083\n2.5\nHISTOLOGY TECHNICIAN\n[0.027493875, -0.079993084, 0.013278877, -0.00...\n\n\n201669\nPHILIP\nSAN ANTONIO\n78240\n3.0\nMANAGER\n[0.09953636, 0.07623968, 0.020005615, 0.001361...\n\n\n256487\nMARIA\nHOLLAND\n76534\n1.0\nHEALTHCARE ADMINISTRATOR\n[0.036386397, 0.06352263, -0.0023324555, -0.02...\n\n\n6330\nELLEN\nBELLAIRE\n77401\n100.0\nARBITRATOR\n[0.05744064, 0.03044543, -0.011071598, 0.01120...\n\n\n125976\nMICHELLE\nFORT WORTH\n76133\n10.0\nSERVER\n[0.0222358, 0.025343752, -0.027377797, -0.0031...\n\n\n13659\nROSE\nHOUSTON\n77024\n75.0\nNURSING\n[0.008113306, 0.03859657, -0.014793488, -0.064...\n\n\n138213\nPETER\nHOUSTON\n77019\n9.0\nDATABASE ANALYST\n[0.042439297, 0.08193651, -0.027909847, 0.0143...\n\n\n62939\nERIN\nFRISCO\n75035\n25.0\nNONPROFIT\n[-0.052989695, 0.19739158, 0.007059781, 0.0288...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nSo now we have two sets of embeddings: we have our O*NET embeddings and we have our ActBlue donor embeddings. Just as before, we can calculate similaries between them. But unlike before, we need to calculate a lot. In order to find the best match we need to compare every O*NET embedding vector with every ActBlue embedding vector. That‚Äôs a lot of comparisons. The good news, this is what GPUs are good at, and a free-tier GPU in Google Colab can kick this out fast.\nWe convert our O*NET embedding column into a (n √ó d) tensor, where n is the number of rows and d is the vector length. Similarly, we convert the ActBlue embedding column into a (m √ó d) tensor where m is the number of ActBlue rows.\nWhen pushing this to a GPU, it‚Äôs a little more art than science. We batch it, and picking an optimal batch size can take some trial and error. For every batch, we‚Äôll calculate the dot product, and return the indices of the best similarities.\nFrom there, we can map all the way back to our original O*NET Title column, as our normalized output:\n\nimport torch\n\ndevice = \"cuda\"  # requires an NVIDIA GPU + CUDA\nonet_t = torch.tensor(np.stack(onet_df.embedding), device=device)  # (n √ó d)\nabtx_t = torch.tensor(np.stack(abtx_df.embedding), device=device)  # (m √ó d)\n\nbatch = 4096  # tune to fit GPU RAM\nbest = []\n\nwith torch.no_grad():\n    for s in range(0, abtx_t.size(0), batch):\n        sims = abtx_t[s:s+batch] @ onet_t.T  # (batch √ó n)\n        best.append(sims.argmax(dim=1).cpu())\n\nidx = torch.cat(best).numpy()\n\nabtx_df[\"Normalized Occupation\"] = onet_df.Title.iloc[idx].to_numpy()\n\nget_sample(30330)[[\"first_name\", \"occupation\", \"Normalized Occupation\"]]\n\n\n    \n\n\n\n\n\n\nfirst_name\noccupation\nNormalized Occupation\n\n\n\n\n139133\nPAT\nRETIRED\nRetired\n\n\n4248\nHARRY\nLAWYER\nLawyers\n\n\n221073\nTHAO\nHISTOLOGY TECHNICIAN\nHistology Technicians\n\n\n201669\nPHILIP\nMANAGER\nManagers, All Other\n\n\n256487\nMARIA\nHEALTHCARE ADMINISTRATOR\nMedical and Health Services Managers\n\n\n6330\nELLEN\nARBITRATOR\nArbitrators, Mediators, and Conciliators\n\n\n125976\nMICHELLE\nSERVER\nFood Servers, Nonrestaurant\n\n\n13659\nROSE\nNURSING\nRegistered Nurses\n\n\n138213\nPETER\nDATABASE ANALYST\nDatabase Administrators\n\n\n62939\nERIN\nNONPROFIT\nFundraisers\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nA way to visualized this is with principal component analysis. PCA computes new orthogonal axes called principal components that capture the most variation in the data. These directions are combinations of the original dimensions, chosen to reveal the biggest patterns and differences. By projecting each vector onto the first two principal components, we can plot everything in two dimensions while keeping as much of the original structure as possible:\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX_onet = np.vstack(onet_df[\"embedding\"].to_numpy())\nX_abtx = np.vstack(abtx_df[\"embedding\"].to_numpy())\nX_all  = np.vstack([X_onet, X_abtx])\n\npca = PCA(n_components=2)\nproj = pca.fit_transform(X_all)\ncoords_onet = proj[: len(X_onet)]\ncoords_abtx = proj[len(X_onet) :]\n\nplt.figure(figsize=(16, 9))\nplt.scatter(coords_onet[:, 0], coords_onet[:, 1], alpha=0.6, label=\"O*NET job titles\")\nplt.scatter(coords_abtx[:, 0], coords_abtx[:, 1], alpha=0.6, label=\"Donor-entered occupations\")\nplt.title(\"PCA of Occupation Embeddings: O*NET vs Donor-entered\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAnd in our higher-dimension space, we can calculate ad hoc similarities on any arbitrary job title someone might enter:\n\ndef normalize(job_title):\n  ad_hoc = torch.tensor(\n      model.encode(\n          [job_title],\n          normalize_embeddings=True,\n          convert_to_numpy=True,\n          show_progress_bar=False,\n      ),\n      device=device,\n  )\n  idx = int((ad_hoc @ onet_t.T).argmax(dim=1))\n  return f'{job_title} ‚Üí {onet_df.at[idx, \"Title\"]}'\n\nprint(normalize(\"code ninja\"))\nprint(normalize(\"uber\"))\nprint(normalize(\"GM\"))\nprint(normalize(\"Professor, Artist\"))\nprint(normalize(\"Postdoctoral Fellow\"))\nprint(normalize(\"DARE Officer\"))\nprint(normalize(\"Senator\"))\nprint(normalize(\"Comms Director\"))\nprint(normalize(\"dermatology\"))\nprint(normalize(\"commodities trader\"))\nprint(normalize(\"life insurance adjuster\"))\nprint(normalize(\"lumberjack\"))\nprint(normalize(\"cpa\"))\n\ncode ninja ‚Üí Computer Programmers\nuber ‚Üí Taxi Drivers\nGM ‚Üí General and Operations Managers\nProfessor, Artist ‚Üí Art, Drama, and Music Teachers, Postsecondary\nPostdoctoral Fellow ‚Üí Clinical Research Coordinators\nDARE Officer ‚Üí Police and Sheriff's Patrol Officers\nSenator ‚Üí Legislators\nComms Director ‚Üí Public Relations Managers\ndermatology ‚Üí Dermatologists\ncommodities trader ‚Üí Securities, Commodities, and Financial Services Sales Agents\nlife insurance adjuster ‚Üí Claims Adjusters, Examiners, and Investigators\nlumberjack ‚Üí Fallers\ncpa ‚Üí Accountants and Auditors\n\n\nIt‚Äôs not perfect. I don‚Äôt think I would have normalized Postdoctoral Fellow to Clinical Research Coordinators. This is where decisions we made in constructing the model start to show themselves. All those aka job title concatenations we did at the top affected how the language was modeled. Depending on your use case this could be fine; maybe you just need deterministic finite categories. You can play with the embedding calculation process to see how different strategies yield different resules.\nAs for our nurses:\n\nprint(normalize(\"nurse\"))\nprint(normalize(\"RN\"))\nprint(normalize(\"MSN\"))\nprint(normalize(\"ER Nurse\"))\nprint(normalize(\"home health nursing\"))\n\nnurse ‚Üí Registered Nurses\nRN ‚Üí Registered Nurses\nMSN ‚Üí Registered Nurses\nER Nurse ‚Üí Registered Nurses\nhome health nursing ‚Üí Registered Nurses\n\n\nLanguage models are pretty good at modeling language!"
  },
  {
    "objectID": "posts/2024-08-08-spline-pchip/index.html",
    "href": "posts/2024-08-08-spline-pchip/index.html",
    "title": "Spline Versus PCHIP",
    "section": "",
    "text": "Let‚Äôs say you‚Äôve got some data points and you make a scatterplot:\n\n\n\n\n\n\n\n\n\nYou might say great! and call it a day. But what if we want to see the behavior of the data between these points? Linear interpolation is a simple way to connect the dots:\n\n\n\n\n\n\n\n\n\nAnd now at this point you might say great! and call it a day. Or, you might decide that you can do better than linear interpolation. That sure does look like a sine curve. But you‚Äôre working with a collection of discrete points, and you wouldn‚Äôt want to erroneously just plot a sine function. Instead, you can reach for a smoother interpolation function, such as a spline:\n\n\n\n\n\n\n\n\n\nThe term ‚Äúspline‚Äù refers to a wide class of functions involving interpolation and smoothing. In data viz, we often see the basis spline (or, B-spline). Think of spline interpolation like a flexible ruler that bends to pass smoothly through all your data points, but in doing so, it might sometimes bend too much or too little. Sometimes the spline overshoots, introducing peaks or valleys that weren‚Äôt there in the original data.\n\n\n\n\n\n\n\n\n\nSometimes this is okay! Depending on your data, a spline may be ideal for generating a very smooth curve, especially when smoothness is more critical than accurately interpolating between every data point. And when the underlying function is oscillatory, a spline can capture the movement between points quite accurately. But real-world data is often not oscillatory.\nLet‚Äôs say you‚Äôve got a month‚Äôs worth of observed temperatures recorded in the Austin area:\n\n\n\n\n\n\n\n\n\nAnd because temperatures exist on a continuous distribution, we could do a simple linear interpolation to articulate the rates of change between points:\n\n\n\n\n\n\n\n\n\nBut temperatures are unlikely to ascend or descend on linear gradients, so we could also try a spline:\n\n\n\n\n\n\n\n\n\nThat‚Äôs a bit more natural, but it looks a bit weird, too. Unlike our sine wave sampling from before, the data points here are of real, observed, daily maximum temperatures. So it‚Äôs a little strange that the fit curve overshoots and undershoots those known values. The interpolation is smooth, but the shape of the data has not been preserved.\n\n\n\n\n\n\n\n\n\nWhile a spline produces smooth curves, the artifacts of overshooting, undershooting, or unwanted oscillations between data points can misrepresent what the data actually says. Fortunately, we have another option: the PCHIP, or Piecewise Cubic Hermite Interpolating Polynomial. Hermite refers to a method of interpolating data points where both the function values and the derivatives at those points are matched.\nA PCHIP preserves the shape of the data and avoids oscillations. The monotonicity (increasing or decreasing trend) of the data is preserved, ensuring no overshoots between data points. I like to think of PCHIP as a hand that firmly (but not rigidly) guides a curve through the data points without allowing any unnecessary dips or rises.\n\n\n\n\n\n\n\n\n\nLooks good! This results in a curve that better captures the shape of the function, especially when the slope information is critical. In our case, the slope is critical. It makes no sense to have a positive slope (overshooting) between points, when the next value decreased.\nBut PCHIP isn‚Äôt always better than Spline. Let‚Äôs apply a PCHIP interpolation to the oscillating data from before:\n\n\n\n\n\n\n\n\n\nIt‚Äôs not wrong, it‚Äôs just a little weird and lumpy. It‚Äôs a curve that connects the dots, but it somewhat lost the true movement between points.\nPCHIP can aggressively flattened near local extrema. When you need to capture those local extrema ‚Äî as we did in our temperature plots ‚Äî PCHIP works well. When you need to capture the smooth movements of oscillatory data, Spline works well. Sometimes it‚Äôs fairly intuitive what you need. Sometimes you need to plot it to really see which works better. Other times it takes more thought.\nConsider the nature of your data. If your data is smooth and continuous, like a waveform or a gradient, spline interpolation might work well. If your data has sharp changes or you need to preserve the natural shape of the data without introducing artifacts, PCHIP might be the better choice. In practical applications like elections modeling, financial forecasting, or engineering metrics, the choice can have significant implications.\nGraphs!"
  },
  {
    "objectID": "posts/2023-10-01-BIDEN-binary-inference-dictionaries-for-electoral-nlp/index.html",
    "href": "posts/2023-10-01-BIDEN-binary-inference-dictionaries-for-electoral-nlp/index.html",
    "title": "BIDEN: Binary Inference Dictionaries for Electoral NLP",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nBIDEN: Binary Inference Dictionaries for Electoral NLP demonstrates a compression-based binary classification technique that is fast at both training and inference on common CPU hardware in Python. It is largely built on the strategies presented by FTCC, which in turn, was a reaction to Low-Resource Text Classification: A Parameter-Free Classification Method with Compressors (the gzip method). Like FTCC, BIDEN is built atop of Zstandard (Zstd), which leverages dictionary compression. Zstd dictionary compression seeds a compressor with sample data, so that it can efficiently compress small data (~1 KB) of similar composition. Seeding the compressor dictionaries acts as our ‚Äútraining‚Äù method for the model.\nThe BIDEN model was trained on the ElectionEmails 2020 data set ‚Äî a database of over 900,000 political campaign emails from the 2020 US election cycle. In compliance with the data set‚Äôs terms, the training data is NOT provided with this repository. If you would like to train the BIDEN model yourself, you can request a copy of the data for free. The BIDEN model was trained on corpus_v1.0.\n\nTraining and Classification\nBoth training and inference for BIDEN are fast and simple.\nThe model consists of two Zstd compressors, one optimized for Democratic emails and one optimzed for Republican emails. Each is built upon a compression dictionary. Each compression dictionary is seeded with training sample emails from its respective party.\nClassification (inference) is achieved by compressing a test sample with both the Democratic and Republican compressors. Whichever compressor achieves a higher compression ratio on the test sample text is considered the inferred label.\n\n\nCleaning the Training Data\nThe ElectionEmails 2020 data set is a CSV. The model consideres two columns: party_affiliation and body_text. BIDEN is only concerned with binary classification for Democratic and Republican labeling.\nThe two requirements defined in requirements.txt are Pandas and zstandard:\npandas==2.1.*\nzstandard==0.21.* \nStart by reading in the data. Since the model is only working with two columns, drop any record that doesn‚Äôt contain both. Also filter the data to only consider Democratic or Republican emails for the binary classificaiton.\nNote: this assumes you have the ElectionEmails 2020 data saved at the relative path data/corpus_v1.0.csv.\n\nfrom enum import Enum\n\nimport pandas as pd\nimport zstandard\n\nfields = {\n    'body_text': str,\n    'party_affiliation': str,\n}\n\ndf = pd.read_csv(\n    'data/corpus_v1.0.csv',\n    sep=',',\n    usecols=list(fields.keys()),\n    dtype=fields,\n)\n\ndf.drop_duplicates(inplace=True)\n\nd_df = df[df.party_affiliation == \"Democratic Party\"].dropna()\nr_df = df[df.party_affiliation == \"Republican Party\"].dropna()\n\nprint(f'D Samples: {len(d_df.index)}')\nprint(f'R Samples: {len(r_df.index)}')\n\nD Samples: 127194\nR Samples: 36788\n\n\nThere are significantly more Democratic samples than Republican samples, so take a random subset of the former.\n\nmax_data = min(len(d_df.index), len(r_df.index))\nd_df = d_df.sample(\n    n=max_data,\n    random_state=9001  # random seed set for reproducibility \n).reset_index(drop=True)\n\nr_df = r_df.sample(\n    n=max_data,\n    random_state=9001  # random seed set for reproducibility \n).reset_index(drop=True)\n\nprint(f'D Samples: {len(d_df.index)}')\nprint(f'R Samples: {len(r_df.index)}')\n\nD Samples: 36788\nR Samples: 36788\n\n\nNow divide the data into training and test subsets, at an 80/20 split.\n\nd_train_df = d_df.sample(frac=0.8, random_state=9001)  # random seed set for reproducibility \nd_test_df = d_df.drop(d_train_df.index)\n\nr_train_df = r_df.sample(frac=0.8, random_state=9001)  # random seed set for reproducibility \nr_test_df = r_df.drop(r_train_df.index)\n\nprint(f'Democratic Training Samples: {len(d_train_df.index)}')\nprint(f'Democratic Test Samples: {len(d_test_df.index)}')\nprint(f'Republican Training Samples: {len(r_train_df.index)}')\nprint(f'Republican Test Samples: {len(r_test_df.index)}')\n\nDemocratic Training Samples: 29430\nDemocratic Test Samples: 7358\nRepublican Training Samples: 29430\nRepublican Test Samples: 7358\n\n\n\n\nThe BIDEN model\nThe model consistes of two core methods: train() and classify() :\n\nclass BIDEN():\n    \"\"\"\n    Binary Inference Dictionaries for Electoral NLP (BIDEN)\n\n    This class allows you to train a model for classifying political content into\n    Democratic or Republican categories based on compression ratios.\n\n    Attributes:\n        Classification (enum): An enumeration of political classifications (DEMOCRATIC, REPUBLICAN).\n    \"\"\"\n    class Classification(Enum):\n        \"\"\"\n        Enumeration of political classifications.\n\n        Attributes:\n            DEMOCRATIC (int): Represents Democratic political content.\n            REPUBLICAN (int): Represents Republican political content.\n        \"\"\"\n        DEMOCRATIC = 1\n        REPUBLICAN = 2\n        \n    def __init__(self, encoding: str = 'utf-8'):\n        \"\"\"\n        Initialize the BIDEN model.\n\n        This constructor initializes the BIDEN model with empty compressors.\n\n        Args:\n            encoding (str, optional): The character encoding of the input data. Defaults to 'utf-8'.\n            \n        Returns:\n            BIDEN: An instance of the BIDEN class.\n        \"\"\"\n        self.d_compressor = None\n        self.d_compressor = None\n        self.encoding = encoding\n\n    @property\n    def trained(self) -&gt; bool:\n        \"\"\"\n        Check if the BIDEN model is trained.\n\n        Returns:\n            bool: True if both Democratic and Republican compressors are trained, False otherwise.\n        \"\"\"\n        return bool(self.d_compressor and self.r_compressor)\n\n    def train(self,\n              d_training_data: str,\n              r_training_data: str,\n              compression_level: int = 15,\n             ) -&gt; bool:\n        \"\"\"\n        Train the BIDEN model.\n\n        Args:\n            d_training_data (str): Democratic training data.\n            r_training_data (str): Republican training data.\n            compression_level (int, optional): The compression level. Defaults to 15.\n\n        Returns:\n            bool: True if training is successful, False otherwise.\n        \"\"\"        \n        d_dictionary = zstandard.ZstdCompressionDict(\n            d_training_data.encode(self.encoding),\n            dict_type=zstandard.DICT_TYPE_RAWCONTENT\n        )\n        d_dictionary.precompute_compress(level=compression_level)\n        self.d_compressor = zstandard.ZstdCompressor(dict_data=d_dictionary)\n\n        r_dictionary = zstandard.ZstdCompressionDict(\n            r_training_data.encode(self.encoding),\n            dict_type=zstandard.DICT_TYPE_RAWCONTENT\n        )\n        r_dictionary.precompute_compress(level=compression_level)\n        self.r_compressor = zstandard.ZstdCompressor(dict_data=r_dictionary)\n\n        return self.trained\n\n    def classify(self, sample: str) -&gt; Classification:\n        \"\"\"\n        Classify a sample based on compression ratios.\n\n        Args:\n            sample (str): The sample text to classify.\n\n        Returns:\n            Classification: The classification (DEMOCRATIC or REPUBLICAN).\n        \n        Raises:\n            RuntimeError: If the model is not trained.\n        \"\"\"\n        if not self.trained:\n            raise RuntimeError(\"Attempted to classify with a model that is not yet trained.\")\n        \n        encoded_sample = sample.encode(self.encoding)\n        original_length = len(encoded_sample)\n        d_compressed_length = len(self.d_compressor.compress(encoded_sample))\n        d_ratio = d_compressed_length / original_length\n        r_compressed_length = len(self.r_compressor.compress(encoded_sample))\n        r_ratio = r_compressed_length / original_length\n\n        if r_ratio &lt; d_ratio:\n            return BIDEN.Classification.REPUBLICAN\n\n        return BIDEN.Classification.DEMOCRATIC\n\n\n\nTrain the Model\nTo train the model, we pass the entirety of the Democratic and Republican texts to construct prefix dictionaries. Prefix dictionaries allow compression operations to reference raw data within the dictionary. Once we have two compressors instantiated and pre-seeded with our training data, the model is trained. This is fast. On my 2.6 GHz 6-Core Intel Core i7, this takes roughly 30 seconds.\n\nd_combined_text = '\\n'.join(d_train_df.body_text)\nr_combined_text = '\\n'.join(r_train_df.body_text)\n\nmodel = BIDEN()\nmodel.train(d_combined_text, r_combined_text)\n\nTrue\n\n\n\n\nClassification\nNow, we can classify our test data. We could loop through each set, but let‚Äôs combine and shuffle them together first, and loop in one go. We‚Äôll also convert the party affiliation strings 'Democratic Party', and 'Republican Party' into our model‚Äôs enum values:\n\ncombined_test_df = pd.concat(\n    [d_test_df, r_test_df],\n    ignore_index=True,\n    sort=False\n).sample(\n    frac=1,\n    random_state=9001,  # random seed set for reproducibility\n).reset_index(drop=True)\n\ncombined_test_df['party_affiliation'] = combined_test_df['party_affiliation'].replace(\n    to_replace=['Democratic Party', 'Republican Party'],\n    value=[BIDEN.Classification.DEMOCRATIC, BIDEN.Classification.REPUBLICAN]\n)\n\nnum_correct = 0\nfor row in combined_test_df.itertuples():\n    actual_label = row.party_affiliation\n    inferred_label = model.classify(row.body_text)\n\n    if inferred_label == actual_label:\n        num_correct += 1\n\nprint(f'Classification Success Rate: {((num_correct / len(combined_test_df.index))*100):.1f}%')\n\nClassification Success Rate: 98.9%\n\n\n98.9% is a shockingly high success rate for such a simple classification method!\n\n\nAnother Email Data Set\nLet‚Äôs see how it performs with emails from another data set. Derek Willis maintains a Datasette of over 150,000 campaign emails. Let‚Äôs grab 100 samples of each party from that collection, and see how they perform:\n\nbase_url = 'https://political-emails.herokuapp.com/emails.csv?sql='\nd_query = 'select body, party from emails where party = \"D\" limit 100;'\nr_query = 'select body, party from emails where party = \"R\" limit 100;'\n\nmap = {\n    ' ': '+',\n    ',': '%2C',\n    '=': '%3D',\n    '\"': '%22',\n    ';': '%3B',\n}\n\nfor symbol, code in map.items():\n    d_query = d_query.replace(symbol, code)\n    r_query = r_query.replace(symbol, code)\n\nd_url = base_url + d_query\nr_url = base_url + r_query\n\nd_dw_df = pd.read_csv(d_url)\nr_dw_df = pd.read_csv(r_url)\n\ncombined_dw_df = pd.concat(\n    [d_dw_df, r_dw_df],\n    ignore_index=True,\n    sort=False\n).sample(\n    frac=1,\n    random_state=9001,  # random seed set for reproducibility\n).reset_index(drop=True)\n\ncombined_dw_df['party'] = combined_dw_df['party'].replace(\n    to_replace=['D', 'R'],\n    value=[BIDEN.Classification.DEMOCRATIC, BIDEN.Classification.REPUBLICAN]\n)\n\nprint('DW Sample:')\ncombined_dw_df.head(10)\n\nDW Sample:\n\n\n\n\n\n\n\n\n\nbody\nparty\n\n\n\n\n0\nHey Peter!!! You know our campaign is made by,...\nClassification.DEMOCRATIC\n\n\n1\nhttps://www.jahanahayes.com/ [https://www.jaha...\nClassification.DEMOCRATIC\n\n\n2\nHi there, it‚Äôs storytime: I developed a keen n...\nClassification.DEMOCRATIC\n\n\n3\nWe‚Äôre contacting a select group of patriots fo...\nClassification.REPUBLICAN\n\n\n4\nPeter ‚Äì You need to watch this. CLICK HERE OR ...\nClassification.REPUBLICAN\n\n\n5\nPeter, you may have seen me asking for your ol...\nClassification.DEMOCRATIC\n\n\n6\nDo you want an Official Mugshot Mug? (24-hour ...\nClassification.REPUBLICAN\n\n\n7\nhttps://secure.actblue.com/donate/cdp-footer?r...\nClassification.DEMOCRATIC\n\n\n8\n[ https://act.katieporter.com/go/14559?t=1001&...\nClassification.DEMOCRATIC\n\n\n9\nI hope I made you proud fighting for you at th...\nClassification.REPUBLICAN\n\n\n\n\n\n\n\n\nnum_correct = 0\nfor row in combined_dw_df.itertuples():\n    actual_label = row.party\n    inferred_label = model.classify(row.body)\n\n    if inferred_label == actual_label:\n        num_correct += 1\n\nprint(f'Classification Success Rate: {((num_correct / len(combined_dw_df.index))*100):.1f}%')\n\nClassification Success Rate: 93.0%\n\n\n93% is still quite good considering that all we‚Äôre doing is compression!\n\n\nClassifying Tweets\nThe Twitter API is basically useless these days, so I scrolled the timelines of Mike Pence (R) and Gavin Newsom (D), and copy+paste‚Äôd 5 tweets from each. It‚Äôs a tiny sample, and not really random, but it‚Äôs neat to see how well it does:\n\npence_tweets = [\n    # https://twitter.com/Mike_Pence/status/1707882018258751915\n    \"Today, we applaud the Eighth Circuit's decision, \" \\\n    \"which is an important step in the fight to uphold and protect \" \\\n    \"the rights of parents and families in Linn-Mar. A family is the \" \\\n    \"single best unit to protect children, and we must continue to do \" \\\n    \"everything we can to empower parents over bureaucrats. The strength \" \\\n    \"of our nation is tied to the strength of our families, and we will \" \\\n    \"not stand idly by as the Radical Left attempts to indoctrinate our \" \\\n    \"children behind parents‚Äô backs.\",\n    # https://twitter.com/Mike_Pence/status/1707472823269392643\n    \"The cause of Life is the calling of our time \" \\\n    \"and I couldn‚Äôt be more proud to have played a role in the administration \" \\\n    \"that appointed three of the justices that overturned Roe v. Wade and \" \\\n    \"returned the question of abortion to the states and the American people.\",\n    \"Republicans are facing a Time for Choosing. \" \\\n    # https://twitter.com/Mike_Pence/status/1707241587460186214\n    \"We have to choose whether or not we‚Äôre going to stay on the path \" \\\n    \"that has defined our movement since the days of Ronald Reagan and \" \\\n    \"through the Trump-Pence years or whether we‚Äôre going to follow \" \\\n    \"the siren song of populism unmoored to Conservative principles.\",\n    # https://twitter.com/Mike_Pence/status/1704132623617122719\n    \"I am for working Americans and free enterprise. These businesses \" \\\n    \"make their decisions, but at the end the of the day these businesses \" \\\n    \"are responding to the heavy hand of the green new deal agenda of \" \\\n    \"Joe Biden and the Democrats in Washington, DC.\",\n    # https://twitter.com/Mike_Pence/status/1703887286641873137\n    \"We were the first country to sanction Chinese leaders for building \" \\\n    \"concentration camps in Xinjiang and for undermining democracy in Hong Kong. \" \\\n    \"And we stood up to years of trade abuses, and imposed historic tariffs \" \\\n    \"to bring China to the negotiating table.\"\n]\n\n[model.classify(t) for t in pence_tweets]\n\n[&lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;]\n\n\n\nnewsom_tweets = [\n    # https://twitter.com/GavinNewsom/status/1700615276667294035\n    \"When people ask why I am introducing a Constitutional Amendment \" \\\n    \"on gun safety this is why. Not only has Congress stalled for YEARS on passing \" \\\n    \"common sense reforms -- judges across the country are tearing down laws that \" \\\n    \"Americans overwhelmingly support. Laws that keep us SAFE and keep guns out of \" \\\n    \"the hands of dangerous criminals. We have to push back\",\n    # # https://twitter.com/GavinNewsom/status/1689743766733877248\n    \"California will be sending search and rescue \" \\\n    \"teams to assist in Hawaii's recovery efforts. The wildfires and \" \\\n    \"devastation that Maui is experiencing is all too familiar and all \" \\\n    \"too horrifying. We stand at the ready to aid Hawaii in its time of need.\",\n    # https://twitter.com/GavinNewsom/status/1679579172690329601\n    \"A school board in Temecula decided to reject a \" \\\n    \"textbook because it mentioned Harvey Milk. CA is stepping in. \" \\\n    \"We‚Äôre going to purchase the book for these students‚Äîthe same \" \\\n    \"one that hundreds of thousands of kids are already using. \" \\\n    \"If these extremist school board members won‚Äôt do their job, \" \\\n    \"we will ‚Äî and fine them for their incompetence.\",\n    # https://twitter.com/GavinNewsom/status/1650634702271942656\n    \"North Dakota GOP have decided to force women to give birth. Even victims of rape. \" \\\n    \"Meanwhile, they voted against providing school meals because child hunger isn't \" \\\n    \"\\\"the responsibility of the state.\\\"\" \\\n    \"Mandating birth is state responsibility. Helping feed those kids is not. Got it.\",\n    # https://twitter.com/GavinNewsom/status/1643745476662132737\n    \"Met with some librarians today while in Florida. \" \\\n    \"They shared with me the rich, diverse background of the \" \\\n    \"town and what's at stake if we ban our kids from learning our real history.\"\n]\n\n[model.classify(t) for t in newsom_tweets]\n\n[&lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;]\n\n\nNeat! This small set classified 90% correctly. Further exploration with more robust Twitter data sets is warranted.\n\n\nClosing Thoughts\nEven after seeing the results, I still have a hard time believing that this works so well! It feels like it shouldn‚Äôt work at all. But, zooming out, there are a lot of relevant factors to consider. First, there just aren‚Äôt that many people writing campaign materials. It makes sense that word-choice and writing style would exhibit predictible patterns. Second, campaign emails have been A/B tested into oblivion, so there‚Äôs a systematic process that cranks out similar-sounding copy. Third, the recipients of these emails have largely self-sorted. This likely bolsters the expected structure and copy uniquely for each label. Ultimately, compression algorithms optimize on patterns and predictibility. What this shows us is that the two parties are uniquely predictible in their written communications.\nThe idea of classification by compression is not new; Russell and Norvig wrote about it in 1995 in the venerable Artificial Intelligence: A Modern Approach:\n\n\n\nClassification by data compression\n\n\nMore recently, the ‚Äúgzip beats BERT‚Äù paper got a lot of attention. What the BIDEN model demonstrates is that this technique is effective and likely generalizable on modern partisan texts.\nIt‚Äôs worth articulating again how fast and simple this method is. No GPUs. No Neural Networks. No N-grams. No transformers. No kNN.\nI think that‚Äôs pretty cool!\n\n\nLicense\nCopyright (c) 2023, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of BIDEN: Binary Inference Dictionaries for Electoral NLP nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a Political Technologist and founder of Ilium Strategies working to help Democrats build, scale, and innovate with tech. Drawing on over a decade of experience at the intersection of Democratic politics and software engineering, I partner with organizations, campaigns, and investors to amplify their impact. My focusses include product and engineering vision, go-to-market technical strategy, donor advising and technical diligence, political tech market navigation, ecosystem coordination, and AI strategy. I‚Äôm particularly interested in developing sustainable, scalable models for long-term impact in the political tech sector.\nI sit on the Advisory Board of Higher Ground Labs, an accelerator that invests in early-stage political tech companies. In this capacity, I contribute strategic guidance to the Higher Ground Labs leadership team, and provide both political and technical advice to investment cohort companies.\nI was the Executive Director of Zinc Labs, a political innovation lab at Zinc Collective that houses the organization‚Äôs strategic work in Democratic technology and pilot programs. Through a network of campaigns, committees, vendors, donors, and subject matter experts, Zinc Labs incubates and invests in innovative new programs across the ecosystem. I prioritized investing in technology and people to enable collaborative experimentation, technical execution, and hypothesis testing.\nI was on the Advisory Board of Quiller, an Artificial Intelligence copilot that helps Democrats draft and deploy high quality, effective fundraising content. In this capacity, I contributed product, technology, and political guidance to the Quiller leadership team. Quiller was acquired in 2025.\nI was the Director of Engineering for President Joe Biden on his 2020 presidential campaign. I joined the campaign in July of 2019, and served in that role through the duration of the primary and general election cycles. I led a team of software, infrastructure, and security engineers, which was charged with delivering innovative new campaign tech for headquarters and states programs. We shipped dozens of internal and voter-facing products, including an in-house peer-to-peer texter, a relational organizing app, a real-time debate transcription service, a document and media ingestion and indexing library, a live donation tracking web service, a distributed soft-reporting platform, the JoeBiden.com web property, hundreds of ETL pipelines, and a cross-region data warehouse.\nI was a Senior Software Engineer on The Groundwork platform, which served as the application and infrastructure system that powered the 2016 presidential campaign of Secretary Hillary Clinton. Beginning in 2015, I was the primary engineer for multiple campaign-enabling web services for event signups, single-click donations, and peer-to-peer fundraising.\nI was an Infrastructure Engineer at Datadog where I architected and implemented the internal platform for Incident Response management. This tool eventually joined the suite of observability offerings available to customers. My team also implemented the organization‚Äôs first templated deployment pipeline for the organization‚Äôs global transition to Kubernetes.\nI was a Senior Mobile Engineer at LISNR, where I developed the product‚Äôs first iOS SDK for the ultrasonic data transport protocol. In 2014 the team joined the TechStars accelerator. This toolchain was originally designed to allow musicians to embed inaudbile data streams into their media. Over time, it became a location-aware second screen immersion platform for music festivals, professional sporting events, and feature films. It now serves as a payment protocol in partnership with VISA.\nI was a Senior Mobile Engineer and Scrum Master at Cardinal Solutions, with the team that built Vantiv Accept - a mobile credit card processing application. Envisioned as a competitor to Square, the application was deployed to retail and grocery point-of-sale systems across the country.\nI am a Named Inventor on an Augmented Reality Patent Application (US20140092241A1) for a ‚Äúdevice and method for scanning of books and other items for order and inventory control‚Äù. We created a mobile application that scanned a shelf of books in real-time as a user viewed it through the screen. If it identified any items out of order, it overlayed the user‚Äôs view of the item with a red X to indicate a misplacement, and then povided directional guidance of where it should be moved.\nI have been an Election Night Reporter for Decision Desk HQ. I gathered live results from New Mexico and Ohio Boards of Elections offices to report real-time data as they were delivered from precincts. These results were featured on Vox and BuzzFeed election night coverage.\nI have been a frequent Open Source Contributor to OpenElections, which is creating the first free, comprehensive, standardized, linked set of election results data for the United States."
  },
  {
    "objectID": "index.html#work",
    "href": "index.html#work",
    "title": "About",
    "section": "",
    "text": "I am a Political Technologist and founder of Ilium Strategies working to help Democrats build, scale, and innovate with tech. Drawing on over a decade of experience at the intersection of Democratic politics and software engineering, I partner with organizations, campaigns, and investors to amplify their impact. My focusses include product and engineering vision, go-to-market technical strategy, donor advising and technical diligence, political tech market navigation, ecosystem coordination, and AI strategy. I‚Äôm particularly interested in developing sustainable, scalable models for long-term impact in the political tech sector.\nI sit on the Advisory Board of Higher Ground Labs, an accelerator that invests in early-stage political tech companies. In this capacity, I contribute strategic guidance to the Higher Ground Labs leadership team, and provide both political and technical advice to investment cohort companies.\nI was the Executive Director of Zinc Labs, a political innovation lab at Zinc Collective that houses the organization‚Äôs strategic work in Democratic technology and pilot programs. Through a network of campaigns, committees, vendors, donors, and subject matter experts, Zinc Labs incubates and invests in innovative new programs across the ecosystem. I prioritized investing in technology and people to enable collaborative experimentation, technical execution, and hypothesis testing.\nI was on the Advisory Board of Quiller, an Artificial Intelligence copilot that helps Democrats draft and deploy high quality, effective fundraising content. In this capacity, I contributed product, technology, and political guidance to the Quiller leadership team. Quiller was acquired in 2025.\nI was the Director of Engineering for President Joe Biden on his 2020 presidential campaign. I joined the campaign in July of 2019, and served in that role through the duration of the primary and general election cycles. I led a team of software, infrastructure, and security engineers, which was charged with delivering innovative new campaign tech for headquarters and states programs. We shipped dozens of internal and voter-facing products, including an in-house peer-to-peer texter, a relational organizing app, a real-time debate transcription service, a document and media ingestion and indexing library, a live donation tracking web service, a distributed soft-reporting platform, the JoeBiden.com web property, hundreds of ETL pipelines, and a cross-region data warehouse.\nI was a Senior Software Engineer on The Groundwork platform, which served as the application and infrastructure system that powered the 2016 presidential campaign of Secretary Hillary Clinton. Beginning in 2015, I was the primary engineer for multiple campaign-enabling web services for event signups, single-click donations, and peer-to-peer fundraising.\nI was an Infrastructure Engineer at Datadog where I architected and implemented the internal platform for Incident Response management. This tool eventually joined the suite of observability offerings available to customers. My team also implemented the organization‚Äôs first templated deployment pipeline for the organization‚Äôs global transition to Kubernetes.\nI was a Senior Mobile Engineer at LISNR, where I developed the product‚Äôs first iOS SDK for the ultrasonic data transport protocol. In 2014 the team joined the TechStars accelerator. This toolchain was originally designed to allow musicians to embed inaudbile data streams into their media. Over time, it became a location-aware second screen immersion platform for music festivals, professional sporting events, and feature films. It now serves as a payment protocol in partnership with VISA.\nI was a Senior Mobile Engineer and Scrum Master at Cardinal Solutions, with the team that built Vantiv Accept - a mobile credit card processing application. Envisioned as a competitor to Square, the application was deployed to retail and grocery point-of-sale systems across the country.\nI am a Named Inventor on an Augmented Reality Patent Application (US20140092241A1) for a ‚Äúdevice and method for scanning of books and other items for order and inventory control‚Äù. We created a mobile application that scanned a shelf of books in real-time as a user viewed it through the screen. If it identified any items out of order, it overlayed the user‚Äôs view of the item with a red X to indicate a misplacement, and then povided directional guidance of where it should be moved.\nI have been an Election Night Reporter for Decision Desk HQ. I gathered live results from New Mexico and Ohio Boards of Elections offices to report real-time data as they were delivered from precincts. These results were featured on Vox and BuzzFeed election night coverage.\nI have been a frequent Open Source Contributor to OpenElections, which is creating the first free, comprehensive, standardized, linked set of election results data for the United States."
  },
  {
    "objectID": "index.html#media-appearances",
    "href": "index.html#media-appearances",
    "title": "About",
    "section": "Media & Appearances",
    "text": "Media & Appearances\n\n\nDeepfakes Are Everywhere, and Many Campaigns Aren‚Äôt Prepared\n\nNOTUS, 2025\n\n\n[Original] [Archive]\n\n\n\nTrump wants an investigation of Democrats‚Äô fundraising. His own campaign has issues\n\nAP, 2025\n\n\n[Original] [Archive]\n\n\n\nMatt Hodges Modded the LED DNC Bracelets\n\nWired, 2024\n\n\n[Original] [Archive]\n\n\n\nBig Tech Is Giving Campaigns Both the Venom and the Antidote for GenAI\n\nWired, 2024\n\n\n[Original] [Archive]\n\n\n\nAI Guidelines Earn Widespread Support Across the Democratic Political Community\n\nZinc Labs, 2024\n\n\n[Original] [Archive]\n\n\n\nPodcast: AI‚Äôs do‚Äôs and dont‚Äôs in politics\n\nThe Chuck Todd Podcast (Chuck ToddCast) | NBC News, 2024\n\n\n[Original] [Archive]\n\n\n\nPanel: A Responsible and Secure Approach to AI in Democratic Campaigns\n\nDefending Digital Campaigns & Microsoft, 2024\n\n\n[Original] [Archive]\n\n\n\nThe dos and don‚Äôts of campaigning with AI\n\nThe Washington Post, 2024\n\n\n[Original] [Archive]\n\n\n\nQuick-Start AI Guidelines for Democratic CampaignsI\n\nZinc Labs, 2024\n\n\n[Original] [Archive]\n\n\n\nPanel: Threats vs Practicality - Realistic Solutions for Securing Campaigns\n\nGoogle and Defending Digital Campaigns Security Summit, 2024\n\n\n[Original] [Archive]\n\n\n\nPodcast: Can AI Help Meet Voters Where They Are?\n\nPolitics Is Everything, 2023\n\n\n[Original] [Archive]\n\n\n\nAI will change American elections, but not in the obvious way\n\nThe Economist, 2023\n\n\n[Original] [Archive]\n\n\n\n1 big thing: Democratic tech vendors go under the cyber microscope\n\nAxios, 2023\n\n\n[Original] [Archive]\n\n\n\nGenerative artificial intelligence tools ‚Äòthreaten democracy, reality‚Äô\n\nThe Courier-Mail, 2023\n\n\n[Original] [Archive]\n\n\n\nMatt Hodges, Political Technologist\n\nUses This, 2023\n\n\n[Original] [Archive]\n\n\n\nA Campaign Aide Didn‚Äôt Write That Email. A.I. Did.\n\nThe New York Times, 2023\n\n\n[Original] [Archive]\n\n\n\nWelcome Keynote\n\n2022 Election Tech Debrief, 2023\n\n\n[Original] [Archive]\n\n\n\nKeynote: Tech Innovation Comes From Those Who Build It\n\nCampaignTech Innovation Summit, 2022\n\n\n[Original]\n\n\n\nPanel: Tech For Good\n\nUnified Jam SXSW, 2022\n\n\n[Original] [Archive]\n\n\n\nBuilding Campaign Tech For Early Adoption\n\nHigher Ground Labs, 2021\n\n\n[Original] [Archive]\n\n\n\nPanel: Securing the 2020 Presidential Campaign\n\nBSides Las Vegas, 2021\n\n\n[Original] [Archive]\n\n\n\nThe Urgent Need For Democratic Tech Talent\n\nCampaigns and Elections, 2021\n\n\n[Original] [Archive]\n\n\n\nPodcast: Building Political Technology\n\nThe Great Battlefield, 2021\n\n\n[Original] [Archive]\n\n\n\n‚ÄúWear a Mask‚Äù In The Source Code\n\nThe Rachel Maddow Show | MSNBC, 2021\n\n\n[Archive]\n\n\n\nThe Biden administration quickly revamped the White House website. Here‚Äôs how.\n\nThe New York Times, 2021\n\n\n[Original] [Archive]\n\n\n\nButler County native, Miami grad played significant role in Biden campaign\n\nThe Journal-News, 2021\n\n\n[Original] [Archive]\n\n\n\n2018 Profile of Ohio‚Äôs 1st District\n\nDecision Desk HQ, 2018\n\n\n[Original] [Archive]\n\n\n\nThe other major hurdle for the Parkland teens: Turning out their peers\n\nThe Washington Post, 2018\n\n\n[Original] [Archive]\n\n\n\nPodcast: Can Zuck be trusted?\n\nTechieBytes, 2018\n\n\n[Original] [Archive]"
  }
]