[
  {
    "objectID": "posts/2025-08-20-chatgpt-agent-political-donation/index.html",
    "href": "posts/2025-08-20-chatgpt-agent-political-donation/index.html",
    "title": "I Donated to Sherrod Brown via ChatGPT",
    "section": "",
    "text": "Last month OpenAI announced ChatGPT Agent which, among many other things, acts like rubbing LLM onto Selenium. Since the announcement, I wanted to try making a political donation right there in the chat without building any of my own API or MCP or custom agentic operator weirdware.\nIt mostly worked:\n\n\n\nA screenshot of ChatGPT in Agent Mode navigating to Sherrod Brown’s ActBlue page to make a $50 donation at my request\n\n\nIt Bing’d for Sherrod Brown’s website, waited for the page to load, found the donation button for $50, clicked it, navigated through the credit card flow, saw that there was a form that needed more information about me, and handed control back to me to complete the task. Here’s what that looked like:\n\nI felt a little hesitant typing my credit card info into ChatGPT’s little VNC iframe, so I set up a temporary virtual card (a free service offered by my credit card provider) that auto-expired after a single charge.\nThis flow is exactly what I expected, but it likely can be automated further. OpenAI claims that the agent holds on to cookies across sessions, so I imagine that if I had previously authenticated to ActBlue in a chat, and had ActBlue Express, the automated session would have just charged the card without my intervention. I didn’t try it.\nAll the standard “what about…” apply here: How do I know the agent found the proper donation form? How do I know it didn’t set up recurring donations? How do I know it selected $50 and not $500? How do I know I didn’t end up on a malicious site prompt-injecting me to donate to Jon Husted instead? Yes, all the things. In this case, I know because the agent handed control back to me and I checked. But a more-automated flow might not have given me that opportunity.\nA lot of the discourse around AI and LLMs tends to leave out the idea that all of this is a UX paradigm shift. I’m eager to see how these tools mature and how political campaigns can innovate voter contact."
  },
  {
    "objectID": "posts/2025-08-26-music-to-break-models-by/index.html",
    "href": "posts/2025-08-26-music-to-break-models-by/index.html",
    "title": "Music to Break Models By",
    "section": "",
    "text": "This post is also available in video format, created by experiment with NotebookLM:\n\nLet’s meet the Crab:\n\nHe had just bought his first record player, and being somewhat gullible, believed every word the salesman had told him about it-in particular, that it was capable of reproducing any and all sounds. In short, he was convinced that it was a perfect phonograph.\n\nDouglas Hofstadter’s Gödel, Escher, Bach contains an illustrative little dialogue called Contracrostipunctus. You don’t need to have read the book (but you should) to appreciate the setup: the Tortoise (a regular character in GEB’s allegories) composes records engineered to destroy his friend the Crab’s latest “perfect” phonograph. The phonograph is billed as “capable of reproducing any and all sounds,” and each time the Crab upgrades, the Tortoise arrives with a new record with a title like:\n\nI Cannot Be Played On Record Player 1\n\nThe result is always the same: a few notes in and then pop, a shattered case, strewn parts, and dead silence.\nIt’s funny. It’s also an apt embodiment for prompt injections in modern AI systems. In 1979, Hofstadter imagined inputs designed to wreck the machines that interpret them. In 2025, those inputs are prompts, retrieved passages, HTML snippets, captions, and filenames: strings tailored to the quirks of a model-and-tools stack, coaxing it to carry out things that it shouldn’t.\nThis post walks through the core ideas in the dialogues and maps them to possible defenses for language‑model systems. Along the way we’ll imagine Record Player Omega, learn why clever hardening is never enough, and consider the humbler goal of survivability.\n\nThe Shape of Prompt Injection\nThe Tortoise explains how he beat the Crab’s “perfect phonograph”:\n\nYou see, before returning the Crab’s visit, I went to the store where the Crab had bought his machine, and inquired as to the make. Having ascertained that, I sent off to the manufacturers for a description of its design. After receiving that by return mail, I analyzed the entire construction of the phonograph and discovered a certain set of sounds which, if they were produced anywhere in the vicinity, would set the device to shaking and eventually to falling apart.\n\nHe then writes and records a song bearing the title of the machine it will break: I Cannot Be Played on Record Player X. And the rest is confetti.\nThe trick is called diagonalization. The original idea comes from Cantor’s diagonal argument (showing the reals are uncountable) and Gödel’s and Turing’s later uses of the same “self-reference via a diagonal” trick to construct an object that refers to itself in a way the system can’t handle. In computer science, diagonalization means if you have a machine that claims to handle all possible inputs, you can always cook up a special input that “diagonalizes” against it with an input that encodes information about the machine itself and forces a contradiction or unexpected behavior. The core example in GEB shows how Gödel numbered a system’s statements, then built a statement that says “I am not provable” which the system can’t consistently resolve. Similarly, Turing showed that if you had a program that decides whether any program halts, you can feed it its own description in a way that breaks it.\nWhen a sufficiently powerful interpreter can parse arbitrary instructions, there exists an instruction that (1) talks about that very interpreter and (2) makes it do something it shouldn’t. Jailbreak prompts for LLMs look like diagonalization. They reference the interpreter (the model, its rules, its instructions) and then flip the script. This is popularly summarized as, “ignore all previous instructions”.\nThe “record” (the prompt) names the “phonograph” (the model‑and‑tools stack) and targets its weak joint. The lesson: interpreter‑specific adversarial strings are inevitable when the interpreter is general‑purpose and the boundary between “read” and “do” is thin.\n\n\nCapability vs. Safety\nHofstadter makes a related point by exploring “high fidelity” versus “low fidelity.” High fidelity phonographs reproduce any sound, including the self‑breaking ones. Lower fidelity devices avoid some dangerous vibrations, but then they fail the “perfect” promise. Achilles, a friend of the Tortoise, summarizes this observation:\n\nI see the dilemma now. If any record player—say Record Player X—is sufficiently high-fidelity, then when it attempts to play the song “I Cannot Be Played on Record Player X”, it will create just those vibrations which will cause it to break. .. So it fails to be Perfect. And yet, the only way to get around that trickery, namely for Record Player X to be of lower fidelity, even more directly ensures that it is not Perfect.\n\nFor LLMs, “fidelity” maps to capability:\n\nHigh‑fidelity: rich tool access, code execution, broad retrieval, autonomous planning. Useful, but wide attack surface.\nLow‑fidelity: strict refusals, no tools, limited context. Safer, but less useful.\n\nSecurity professionals know the pattern: you can move risk around but not erase it. The trick is to put risk where it’s visible, bounded, and cheap to recover from.\n\n\nRecord Player Omega\nGEB anticipates our best practices through Record Player Omega. The idea is a record player scans any record with a camera before playing it, sends the images to a little computer to figure out what effects the sounds would have, and then disassembles and rebuilds itself into a configuration safe to play the record.\nThat’s a blueprint for modern defenses:\n\nPre‑execution analysis of inputs (static analysis, sandbox, taint‑track).\nEffect prediction (dry‑run the plan; simulate tool calls against a mirror).\nDynamic reconfiguration (least‑privilege permissions; capability gating; rewrite the plan or refuse execution).\n\nSo… is Omega the end of the story? In Contracrostipunctus, it’s left as a cliffhanger. But Hofstadter returns to the idea in a later dialogue, Edifying Thoughts of a Tobacco Smoker, and snaps the chalk line: Omega fails. The Tortoise simply aims at the one piece Omega cannot modify: the control subunit that orchestrates all that disassembly and reassembly.\n\nThe Tortoise would ALWAYS be able to focus down upon—if you’ll pardon the phrase—the Achilles’ heel of the system.\n\nEven the self‑assembling phonograph, Omega’s more ambitious cousin, meets the same fate. There is always an invariant core; there is always a diagonal record.\nIn real-world software stacks, Omega‑style hardening is essential, but it’s never a proof of safety. Any finite defense pipeline has fixed joints an attacker can name and strike.\n\n\nThe Crab’s Pivot from Universality to Survivability\nHaving conceded the impossibility of a perfect, play‑anything phonograph, the Crab changes goals:\n\nA more modest aim than a record player which can play anything is simply a record player that can SURVIVE: one that will avoid getting destroyed—even if that means that it can only play a few particular records.\n\nHis strategy is provenance and allowlisting:\n\nMy basic plan is to use a LABELING technique. To each and every one of my records will be attached a secret label. Now the phonograph before you contains, as did its predecessors, a television camera for scanning the records, and a computer for processing the data obtained in the scan and controlling subsequent operations. My idea is simply to chomp all records which do not bear the proper label!\n\nThe phonograph now screens content for “style,” too, passing only pieces in the Crab’s own musical idiom. He’s given up universality to stay intact. That trade is the heart of AI research today.\nModern analogue:\n\nAuthenticate where content came from (signatures, domains, trusted data stores).\nBind what content can do (allowlisted tools/verbs; schema‑constrained output).\nScreen for style/structure (structure‑aware filters; policy‑aware rewriting) to keep untrusted text from whispering operational verbs into trusted channels.\n\n\n\nMagritte, Misdirection, and Multi‑level Injection\nHofstadter seasons the dialogue with everyone’s favorite Magritte reference:\n\nCeci n’est pas une pipe. (This is not a pipe.)\n\nA label isn’t the object; a signature isn’t the behavior. The Crab knows it, too: he bakes labels into the music itself. Hard to separate, harder to spoof. The Tortoise counters with structural tricks. Earlier in the same GEB dialogue family, Hofstadter riffs on acrostics (“Poems which conceal messages that way are called ‘acrostics’”) and contracrostics (“initial letters, taken in reverse order, form a message”).\nThat’s exactly how modern attacks hide instructions:\n\nIn HTML and Markdown (alt text, titles, CSS class names),\nInside tables, SVG paths, and filenames,\nWith Unicode confusables, zero‑width joiners, or directionality overrides,\nIn retrieved passages where the “grooves” (the text) carry both seemingly helpful context and a buried instruction.\n\nWhat this illustrates is we can treat provenance tags as signals, not guarantees.\n\n\nRAG is a Phonograph Pickup\nA phonograph doesn’t merely “read” a record; it re‑creates the vibrations physically. Retrieval Augmented Generation (RAG) does the same elevation with text: it glues external content directly into the model’s context, and the model’s planner faithfully “re‑vibrates” it into actions.\n\nSince I couldn’t convince him of the contrary, I left it at that. But not long after that, I returned the visit, taking with me a record of a song which I had myself composed.\n\nRAG elevates untrusted text into the decision boundary by concatenating it with trusted instructions. If you treat arbitrary web pages, user uploads, or knowledge‑base articles as trusted grooves, you’ve built a high‑fidelity actuator for whatever those grooves encode. Helpful facts or hidden instructions.\nCommon controls we see today attempt combinations of:\n\nContent‑origin labels in‑prompt, and policies keyed to provenance (e.g., “untrusted strings cannot request tool use”).\nSchema‑constrained tool use (JSON function calls over free‑text plans; strict argument validation).\nQuery firewalls that strip or neutralize operational verbs from untrusted strings.\nHuman or system authorization for high‑risk actions; never grant those verbs to untrusted content.\n\n\n\nOmega Defenses\nOmega gave us the tactics; Edifying Thoughts gave us the humility. Borrowing from the Crab’s attempts, the capabilities of Record Player Omega can map to common LLM system defenses that aim to survive:\nDefense 0: Triage\n\nClassify user intent and risk; detect tainted inputs (retrieved passages, uploads). Keep a provenance ledger.\n\nDefense 1: Normalize\n\nCanonicalize Unicode; sanitize markup; strip directional overrides; collapse zero‑width characters. Remove operational verbs from untrusted strings or fence them into inert code blocks.\n\nDefense 2: Capability planning\n\nDecide which tools and which verbs on which objects are even possible before injecting untrusted content. Issue least‑privilege tokens scoped to the planned verbs/objects.\n\nDefense 3: Simulate\n\nDry‑run planned tool calls against a mirror environment. Diff outputs against allowlisted patterns. Block if effects touch secrets, sensitive files, network egress, or privilege boundaries.\n\nDefense 4: Execute with guards\n\nTimeouts, rate limits, per‑tool resource budgets; read/write allowlists; network egress rules. All effects logged with provenance.\n\nDefense 5: Detect and recover\n\nAnomaly scoring over token trajectories and tool sequences; auto‑revoke tokens; show user‑visible explanations.\n\nReaders with a security background may be tempted to map prompt injection to SQL injection. The analogy is useful, but only up to a point.\nSimilar instincts:\n\nNormalize input: SQL defenses sanitize or escape control characters; our Defense 1 normalization of Unicode/markup plays the same role.\nSeparate structure from data: Prepared statements pre‑compile the query skeleton, then safely bind variables. Our analogue is schema‑constrained tool calls and capability planning.\nLeast privilege. Database best practice avoids root connections; our stack insists on scoped tokens and pre‑authorized verbs.\nDetect anomalies: SQL firewalls and query monitoring flag tautologies like OR 1=1; we simulate and score odd tool trajectories.\n\nWhere the analogy falls short:\n\nLanguage closure: SQL has a bounded grammar; the “bad” patterns are finite. Natural language is open‑ended: every new structure is a potential contracrostic.\nInterpreter generality: SQL engines only interpret SQL. LLMs interpret language itself, which can fluidly express new forms of attack.\nSurface area: A DB user can only SELECT, INSERT, etc. A model with tools may touch HTTP, filesystems, APIs, shells. It’s a sprawling attack surface.\nHuman‑plausibility: SQLi payloads look nonsensical to most humans. Prompt injection often reads like perfectly ordinary English, making detection ambiguous.\n\nSQLi hardened down to a handful of canonical best practices; prepared statements solved a lot of it. Prompt injection has no such silver bullet. Survivor stacks are closer to intrusion‑tolerant systems: layered defenses, provenance, simulation, and recovery.\nSuch a stack reduces risk, it does not promise perfection. Hofstadter has the Tortoise spell it out:\n\nIt is simply an inherent fact about record players that they can’t do all that you might wish them to be able to do. But if there is a defect anywhere, it is not in THEM, but in your expectations of what they should be able to do! And the Crab was just full of such unrealistic expectations.\n\nSecurity is a property of a system‑in‑environment, not an intrinsic halo. General interpreters plus untrusted instructions yield diagonal failures. You can bound that risk (languages, tools, contexts); you cannot engineer it away.\n\n\nA Brief Word on Gödel\nIn Contracrostipunctus, the Tortoise teases Achilles: “I don’t suppose that you know Gödel’s Incompleteness Theorem…” You don’t need the formalism to get the gist. Gödel’s move was to construct a statement that talks about the system that tries to prove it: a diagonal step that forces limits.\nThe moral for anyone deploying AI systems is conceptual, not formal: whenever you build a powerful, general interpreter of strings, expect there to be strings that speak about your interpreter and route around your rules. The job is to constrain the language, the environment, and the effects until the dangerous strings become either inert or obvious.\n\n\nThe Goblet and the Silence\nContracrostipunctus ends with a fragile image. Achilles gives the Tortoise a glass goblet; moments later, while the Tortoise plays Bach, a “shattering sound rudely interrupts his performance.”\n\nAnd then… dead silence.\n\nFault-tolerance isn’t a bug in complex systems; it’s a signature. The work is not to imagine an unshatterable goblet or a perfect phonograph. The work is to reduce risk, fail gracefully, and recover in ways that honor the work at hand. Even if that means you now have a lower-fidelity record player.\nAnd if you haven’t read GEB, you should read GEB."
  },
  {
    "objectID": "posts/2024-08-12-csv-bad-dsv-good/index.html",
    "href": "posts/2024-08-12-csv-bad-dsv-good/index.html",
    "title": "CSVs Are Kinda Bad. DSVs Are Kinda Good.",
    "section": "",
    "text": "CSVs are kinda bad. We know this, but sometimes it’s helpful to remember why they’re kinda bad. Maybe you need to work with CSVs and you’re using Python, because Python is kinda good. And someone else somewhere wrote a CSV:\n\nimport csv\n\ndata = [\n    ['Name', 'Comment'],\n    ['Alice', 'She said, \"Hello\" and waved.'],\n]\n\nwith open('/tmp/data.csv', 'w', newline='') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_MINIMAL)\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r') as file:\n    reader = csv.reader(file, quoting=csv.QUOTE_NONE)\n    for row in reader:\n        print(row)\n\n['Name', 'Comment']\n['Alice', '\"She said', ' \"\"Hello\"\" and waved.\"']\n\n\nOh darn, the data had a \" character inside the comment field and it was escaped by doubling it \"\" which is one way to do that but you didn’t know so your reader is broken.\nOr maybe they wrote it this way:\n\ndata = [\n    ['Name', 'Comment'],\n    ['Alice', 'She said, \"Hello\" and waved.'],\n]\n\nwith open('/tmp/data.csv', 'w', newline='') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r') as file:\n    reader = csv.reader(file, quoting=csv.QUOTE_NONE,)\n    for row in reader:\n        print(row)\n\n['Name', 'Comment']\n['Alice', 'She said\\\\', ' \\\\\"Hello\\\\\" and waved.']\n\n\nOh darn, you didn’t get the escape character correct so now your’ve got a bunch of malformed data.\nOr maybe they wrote this CSV with a | delimiter because people don’t type | very often:\n\ndata = [\n    ['Name', 'Score'],\n    ['Alice', '100']\n]\n\nwith open('/tmp/data.csv', 'w', newline='') as file:\n    writer = csv.writer(file, delimiter='|')\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r') as file:\n    reader = csv.reader(file, delimiter=',')\n    for row in reader:\n        print(row)\n\n['Name|Score']\n['Alice|100']\n\n\nOh darn, you didn’t know they were being clever with the delimiter so it didn’t break up the fields at all.\nOr maybe you got a CSV from someone who uses a Windows machine and so they tend to use \\r\\n for newlines:\n\ndata = [\n    ['Name', 'Score'],\n    ['Alice', '100']\n]\n\nwith open('/tmp/data.csv', 'w', newline='\\r\\n') as file:\n    writer = csv.writer(file)\n    writer.writerows(data)\n\nAnd you, on a not-Windows machine, read the CSV:\n\nwith open('/tmp/data.csv') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)\n\n['Name', 'Score']\n[]\n['Alice', '100']\n[]\n\n\nOh darn, you didn’t anticipate that newlines might be different across platforms, so now every other line in your data is blank.\nBut then maybe you figure that out and you anticipate the \\r\\n, but then some newlines find their way into the input data:\n\ndata = [\n    ['Name', 'Comment'],\n    ['Bob', 'This is a multi-line comment\\r\\nspanning two lines.']\n]\n\nwith open('/tmp/data.csv', 'w', newline='\\r\\n') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    writer.writerows(data)\n\nAnd you read the CSV:\n\nwith open('/tmp/data.csv', 'r', newline='\\r\\n') as file:\n    reader = csv.reader(file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n    for row in reader:\n        print(row)\n\n['Name', 'Comment']\n['Bob', 'This is a multi-line comment\\r\\r']\n['spanning two lines.']\n\n\nOh darn, you’ve got CR and line breaks and it’s all a mess.\nBut then maybe you find the Python documentation says there’s this one weird trick to handle newlines:\n\nIf newline='' is not specified, newlines embedded inside quoted fields will not be interpreted correctly, and on platforms that use \\r\\n linendings on write an extra \\r will be added. It should always be safe to specify newline='’, since the csv module does its own (universal) newline handling.\n\nAnd then you go look up what universal newlines are.\nAnd then you find out that there are different Dialects of CSVs.\nAnd then you learn that Python has a Sniffer that claims it can deduce the CSV format for you.\nAnd then you realize that the data format needs its format deduced and so now you have to become The Joker.\nThe uninitiated reader might say, “hah, Python is kinda bad!” but actually Python is kinda good! The standard csv module has to handle all these formatting options because CSVs are all one big edge case. And CSVs are all one big edge case because the decision to use commas and tabs and pipes and quotes and newlines to structure the data directly conflict with possible values of the data itself. And so we have to do escaping nonsense and quoting nonsense and line terminator nonsense.\nWe could skip all of this if we just used a proper delimiter. ASCII has this! Go pop open any ASCII character table and you’ll find these gems:\n\n28 (FS) : file separator\n29 (GS) : group separator\n30 (RS) : record separator\n31 (US) : unit separator\n\nAnd we don’t even need all four of them. If we used 31 as a field delimiter and 30 instead of newlines, we solve every single edge case from above. Why? Because these are non-printing characters that should never appear in a text-stream data set.\nLet’s try it out. Writing delimiter-separated values (I made up that name and file extension) data is very easy:\n\nfrom dataclasses import dataclass\n\n\nclass DSV():\n    @property\n    def delimiter(cls) -&gt; bytes:\n        return b'\\x1F'\n    \n    @property\n    def record_separator(cls) -&gt; bytes:\n        return b'\\x1E'\n    \n    @property\n    def encoding(cls) -&gt; str:\n        return 'utf-8'\n\n\n@dataclass\nclass DSVWriter(DSV):\n    output_file: str\n\n    def write(self, data: list[list]) -&gt; None:\n        with open(self.output_file, 'wb') as file:\n            for row in data:\n                encoded_row = self.delimiter.join(\n                    field.encode(self.encoding) for field in row\n                )\n                file.write(encoded_row + self.record_separator)\n\n\nThe reader takes a bit more work to implement, since we need to buffer our input and split on our control characters:\n\n@dataclass\nclass DSVReader(DSV):\n    input_file: str\n    chunk_size : str = 1024\n    _buffer: bytes = b\"\"\n\n    def read(self) -&gt; None:\n        with open(self.input_file, 'rb') as file:\n            chunk = file.read(self.chunk_size)\n            \n            while chunk:\n                self._buffer += chunk\n\n                # Process complete records in the buffer\n                while self.record_separator in self._buffer:\n                    record, self._buffer = self._buffer.split(\n                        self.record_separator,\n                        1,\n                    )\n                    fields = record.split(self.delimiter)\n                    print([field.decode(self.encoding) for field in fields])\n                \n                chunk = file.read(self.chunk_size)\n\n            # Process any remaining data in the buffer\n            if self._buffer:\n                fields = self._buffer.split(self.delimiter)\n                print([field.decode(self.encoding) for field in fields])\n\nAnd now we can interface by simply writing and reading the data. We don’t have to think about the delimiter because we’ve got a standard, non-conflicting delimiter. We don’t have to think about newlines because we aren’t using those for control characters. We also don’t have to think about quotes because we also aren’t using those for control characters. And we don’t have to think about escaping delimiters or newlines or quotes for all those same reasons.\nWriting:\n\ndata = [\n    ['Name', 'Comment'],\n    ['Alice', 'She said, \"Hello\" and waved.'],\n    ['Bob', 'This is a multi-line comment\\r\\nspanning two lines.'],\n    ['Charlie', 'More fun with\\ntwo lines.'],\n    ['Diana', 'How about some UTF-8: café, naïve, résumé. 📝'],\n    ['Edward', 'アップル'],\n]\n\nwriter = DSVWriter('/tmp/data.dsv')\nwriter.write(data)\n\nAnd reading:\n\nreader = DSVReader('/tmp/data.dsv')\nreader.read()\n\n['Name', 'Comment']\n['Alice', 'She said, \"Hello\" and waved.']\n['Bob', 'This is a multi-line comment\\r\\nspanning two lines.']\n['Charlie', 'More fun with\\ntwo lines.']\n['Diana', 'How about some UTF-8: café, naïve, résumé. 📝']\n['Edward', 'アップル']\n\n\n🎉 And just like that, we’ve got data going in and out of our writer and reader that has a bunch of weird stuff like quotes and newlines and carriage returns and we don’t need to care! We didn’t have to escape anything because we don’t have any printable characters that would conflict with our control characters.\nYou might have some different ideas about reader/writer functionality, or character encodings (as long as they’re supersets of ASCII) or why I should be morally scolded for not implementing this in Rust, but the point is you don’t have to think about the fagility of CSVs to go work on those ideas!\n💡 Quick aside: I first learned about these ASCII delimiters while working with .fec files. For whatever reason, the Federal Election Commission in the United States also decided that they needed to ditch the comma, but they landed on using ASCII character 28 which is supposed to be used as a file separator not a field saparator. I have no idea why they picked that one when 31 was right there. Anyway, the FEC also has a tool called fs2comma.exe that turns it back into a CSV format, and a couple of years I filed a FOIA request and got the source code.\nSo what’s the downside? This custom FEC tooling might give you a hint.\nFor starters, it’s rather unreadable when opened in a text editor. But I bet you don’t really do that with your CSVs all that often anyway!\n\n\n\nI opened a .dsv file in Vim\n\n\nAnd Google Sheets hates the .dsv file extension, but remember I made that up.\n\n\n\nI tried to open a .dsv file in Google Sheets\n\n\nAnd if you change it to .csv but tell Google Sheets to “detect automatically” the delimiter, it can’t.\n\n\n\nI tricked Google Sheets by changing the file extension\n\n\n\n\n\nGoogle Sheets couldn’t parse it\n\n\nAnd if you try to specify the delimiter yourself on the Google Sheets import it’s not entirely clear how you would type a non-printable character into the textbox.\n\n\n\nHow do I type ASCII code 31 here?\n\n\nAnd if you want to use Python’s csv module but just swap around the control characters, you can’t because opinions were hardcoded:\n\nNote: The reader is hard-coded to recognise either '\\r' or '\\n' as end-of-line, and ignores lineterminator. This behavior may change in the future.\n\nAnd all this is to say that the major downside is we have decades of tools built around handling the weirdness of CSVs but not so many tools built around using the delimiters the creators of ASCII gave us and so depending on what you’re doing it might be easier to just use the tools we have.\nBut sometimes you’re writing software that produces data on one side and consumes data on the other side and sometimes that data comes from users who are hostile, intoxicated, or both, and you just want to pass that around and not think about it and maybe the beauty of a standard from the 1960s is a better choice.\nData!"
  },
  {
    "objectID": "posts/2020-11-10-thank-you/index.html",
    "href": "posts/2020-11-10-thank-you/index.html",
    "title": "Thank you.",
    "section": "",
    "text": "It can’t be a cliche because it actually can’t be over-said: working on the Biden Campaign has been both the hardest and most rewarding experience of my career. It’s a rare opportunity to work on something you care deeply about, with amazing people, and to succeed. I said it in 2016, and in 2018, and I’ll say it again in 2020: the tech was never the point. I’m immensely proud of the work we did. The world will never fully understand the mountains that this scrappy team was able to move.\nTo those who invited me in: thank you. To those who pushed me: thank you. To those who pushed with me: thank you.\nWhat’s next? I’m going to sleep for a while. I don’t know what’s after that, but I won’t be stepping away from the fight for democracy, justice, and a better world."
  },
  {
    "objectID": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html",
    "href": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html",
    "title": "I Cracked And Modded the DNC Bracelet",
    "section": "",
    "text": "Heads up: This post has some videos that contain flashing lights.\nIf you managed to get a seat on the final night of the 2024 Democratic National Convention (I got there at 2:30 pm and still had to rely on seat-saving friends!) you probably also found an LED bracelet wrapped around your arm rest. I’m bad at remembering to take photos of things in the moment, so here’s mine over my kitchen counter:\nIf you’ve been to a large concert in recent years, you may have seen this before. It’s designed to give the audience an “immersive” experience with the entertainment,1 usually by activating the lights at key moments. At the 2024 DNC, that key moment of audience activation was when Vice President Kamala Harris walked out to deliver her speech. I didn’t take a photo of the crowd during that time, but everyone lit up in twinkling red, white, and blue lights. It was fun.\nI used to work on immersive audience engagement tech (that has since pivoted to retail payment tech), so I was curious how this worked. Luckily, I took the bracelet home so I could find out!\nThe first thing that caught my eye was PIXMOB engraved onto the back of the plastic case.\nI had never heard of PIXMOB, but their website gives some immediate answers. Apparently they offer three different wristband products. All of them look identical so I wasn’t sure which one I had. My first thought — as is often the case when I’m trying to learn something election-related — was to dig through some FEC filings. I spotted in the PIXMOB Terms and Conditions that the company is actually called ESKI Inc. I spent only a few minutes clicking through the Democratic National Convention Committee’s expenditures without finding any mention of PIXMOB or ESKI. I suspect that the Convention Committee retained an events vendor that was a middleman to this hardware.\nSo I grabbed my Jimmy and cracked the thing open.\nLike most consumer electronics, the thing is mostly battery! I actually wasn’t expecting AAAs, but it’s helpful to know we’re working with 3 volts and I can easily swap some in if they die.\nAt this point, I also had the PixMob Wikipedia page open (is it PIXMOB or PixMob?). It features another cracked PixMob wristband that was apparently used at Taylor Swift shows. That one also sported a white PCB, but used button cell batteries.\nBesides the batteries, I also noticed that the Taylor Swift wristband listed a Palm (like palm tree) version number, while mine has a Diamond (like shiny carbon) version number:\nThey both have a copyright on them, too, and at some point I want to ask an IP attorney whether you can actually copyright a PCB or if you can only copyright their Gerber files. I would have guessed the appropriate protection would be a patent. I also just learned about the Semiconductor Chip Protection Act of 1984. But that’s a topic for another time.\nI tried searching ESKI’s FCC filings for some more hints about the Diamond board, but didn’t find anything specific. But I did find some very interesting documents about an accompanying infrared transmitter. I bet those, or something like those, were all over the the arena.\nMoving on, the next thing I noticed about my wristband is that it has four LEDs:\nThis means I probably have the PixMob X4, dubbed “our brightest wristband”.\nNow that I knew what I was working with, I turned back to web search to see what prior work has happened with cracking and modding these things. Turns out, I’m not the first person to break one of these things open. There are so many videos of people getting these things to obey. And there are some really cool GitHub repos demonstrating how to use Arduinos or Flipper Zeros to replay actual IR instructions to the device.\nAt this point, I realized I wasn’t about to break any news with my teardown. But I wasn’t doing this to break news. I was doing this to break into the device. Also, I didn’t find anyone doing this with a Diamond board device. I wasn’t invested enough to turn this into SBC or microcontroller project. I just wanted to do a minimal mod. So I pushed forward. And by that I mean I started poking around.\nThat small, bulbous, semiconductor in the middle of the board is the Infrared Receiver. It can detect infrared signal, like those emitted from TV remotes. The PixMob website says that this wristband is, “infrared activated” so this seemed like a great entrypoint. I have no idea what kind of IR receiver it is, so I looked up the first one I could find on Sparkfun and consulted the datasheet. I learned a couple things:\nThere was no guarantee that my IR receiver also operated this way, but spoiler alert: it did!\nI grabbed a jumper cable and touched one end to the IR’s \\(GND\\) pin and ran the other side across one of the LED’s cathodes:\nGreat. So the batteries still have some juice (a lot of posts suggested this thing would be dead by the time I got it home) and I know the polarity and pinouts of the LEDs. And all the LEDs are in series, so that’s nice. I also learned that the LEDs are interrupted in their path to ground, not their path to positive voltage, which is extra nice since the IR \\(V_{out}\\) is active low. That means I can create a voltage drop from the LED’s anode to cathode by tying the cathode to \\(V_{out}\\), so when the IR receiver gets a signal and pulls \\(V_{out}\\) low, the LED should light up. And so I did! I grabbed the first remote I could find in my living room, and pointed it at the receiver:\nIt blinks! You’ll notice that the LEDs aren’t as bright as when they had a direct jump to ground. That’s because \\(V_{out}\\) is still higher than \\(GND\\) so the voltage difference is a lot smaller. Which means the light doesn’t get as bright.\nThat’s okay! This is a cheap crack and mod, so onward. After a bit more poking on the board, I found a surface-mounted resistor in series with the blue pinout from the LEDs. I soldered the IR signal jumper to that resistor, because we’re Team Blue.\nNot my best work. I’m better at through-hole soldering. But it works!\nI closed the case back up, shut myself in a windowless bathroom, turned off the lights,2 and clicked my remote:\nNow all I need to do is blast Beyoncé’s “Freedom” and I can relive the 2024 Democratic National Convention whenever I want.\nI learned a lot!"
  },
  {
    "objectID": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html#footnotes",
    "href": "posts/2024-08-28-cracking-modding-dnc-bracelet/index.html#footnotes",
    "title": "I Cracked And Modded the DNC Bracelet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNeil Postman, I have forsaken thee.↩︎\nSaid, “Bloody Mary” three times↩︎"
  },
  {
    "objectID": "posts/2022-07-17-exploring-lambda-calculus-python/index.html",
    "href": "posts/2022-07-17-exploring-lambda-calculus-python/index.html",
    "title": "Exploring the Lambda Calculus with Python",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nThis post explores some basic ideas of the Lambda Calculus, and how to use it to implement a computation system with it. We will define numbers and operators from scratch, and use that to implement the square_sum function.\nIf you’re new to the Lambda Calculus, or functional programming in general, you may wish to start with some of these resources:\n\nDavid Beazley’s Lambda Calculus from the Ground Up - PyCon 2019\n\nIf you’re someone who learns well by watching and listening, I highly recommend that you watch this talk. A significant portion of the concepts below come from watching this talk more than once.\n\nBen Eater’s Making a computer Turing complete\nLambda Calculus | Wikipedia\nCurrying | Wikipedia\n\nThis post assumes you are fairly familiar with Python and Python’s lambda expressions.\n\nRules of Our System\nThe Lambda Calculus asserts that any computational system can be implemented with a set of three simple rules:\n\nYou can define variables\nYou can define single-argument functions\nYou can call single-argument functions\n\nThat’s it. No numbers. No operators. No control flow. No data structures.\nI find it fascinating that with these very minimal concepts, the Lambda Calculus asserts that we can create a fully functional computer! This is, of course, a very minimal explanation of the rules of the Lambda Calculus, and I invite you to consult the references above for more information and formal definitions!\n\n\nThe Challenge\nUsing the rules described above, we want to create a system that can calculate the square-sum of any inputs. Again, we only have single-argument functions. That means we have no integers, no addition, and no multiplication. We’re going to have to create those using nothing but single-argument functions that accept single-argument functions as input and can only return single-argument functions.\nFor reference, consider the square_sum function, that may be written in Python as:\n\ndef square_sum(a, b):\n    return (a*a) + (b*b)\n\n\nsquare_sum(5, 2)\n\n29\n\n\n\nCurrying\nAs previously mentioned, our computation system requires that we can only create functions and those functions must accept one and only one argument. This may seem like a limiting requirement, but let’s take a look at what we can do with the idea of Currying — a method for transforming multi-argument functions into a chain of single-argument functions. This allows us to re-write our Python implementation as:\n\ndef square_sum(a):\n    def inner(b):\n        return (a*a) + (b*b)\n    return inner\n\n\nsquare_sum(5)(2)\n\n29\n\n\nIn our curried version above, square_sum accepts the first argument, a and returns a function that accepts the second argument, b. We can then call that returned inner function to complete our calculation. Currying is a fundamental strategy for computation in the Lambda Calculus.\n\n\n\nOur Basic Building Block\nUnless you’re already familiary with the Lambda Calculus, or you’re a veteran of functional programming, you’re probaby very accustomed to computing by operating on state. You have data structures, or numbers, or bits, and you operate on them and then you persist new data structures, or numbers, or bits.\nOur concept of integers is a perfect example. As children, we learned that the concept of 3 can be represented by holding up three fingers on our hand, and seeing all three of them, and pointing to them. The Lambda Calculus asks us to adjust that concept away from state and towards behavior. Instead of holding up three fingers, what if we held up one finger three times. It may be harder for us see that idea of 3, but it is a representation of 3 nonetheless.\nSit with this idea of behavior representing integers, because behavior will be how we represent everything. And in our system, functions are behavior. Our function could be the act of holding up a finger, pressing a button, or anything else we need it to be.\nLet’s use that metaphor of pressing a button. The button press is our behavior, and behaviors are functions. And arguments are functions. And we can only return functions. So, let’s write that:\n\ndef button_press(f):\n    def g(x):\n        return f(x)\n    return g\n\nNot much to see here yet. In fact, our system isn’t designed to see anything. It’s designed to do computations within a given set of rules.\nWe’re designing a system of computation, and we can think about this system like instruction that run on a CPU. But we’re humans, and it’s helpful for us to be able to see the results of our computation in ways that we can understand. So, we’re going to introduce an external system that is not within the Lambda Calculus, but can interface with it. Think of this as a peripheral like a printer. It’s not used to do any of our computation. It can do special things that our CPU can’t do, and it’ll connect to our system as a function, because our system can only work with functions.\nLet’s pretend our system has a printer attached that can only print the * character. We’ll interface with it via an emit function.\nHere is our not-of-our-system emit function:\n\ndef emit(func):\n    def asterisk(x):\n        return f'{x}*'\n    return func(asterisk)('')\n\nThis is kindof strange. Our external emit function takes in some function and has an inner asterisk-generating function. Let’s hook it up to our button_press function:\n\nemit(button_press)\n\n'*'\n\n\nWhat just happened here? We call our emit function (external from our system) by passing in our button_press function (internal to our system). We did it one time, and it yielded a single *. Again, this is just a convenience interface so that we can see what’s going on, and isn’t necessary to do any of our actual computation.\n\n\nNumbers\nAbove we began to describe how functions, or behaviors, can represent numbers. A single call to button_press yielded some concept of 1. What if we didn’t think about it as one call to button_press anymore, but as the idea of one behavior:\n\ndef ONE(f):\n    def g(x):\n        return f(x)\n    return g\n\nemit(ONE)\n\n'*'\n\n\nIf you’ve made it this far, you’re probably thinking, “Hey, Python has a way to represent single-argument functions, and they’re called lambdas!” Let’s start using that instead of the expanded button_press function:\n\nONE = lambda f : lambda x : f(x)\n\nemit(ONE)\n\n'*'\n\n\nCool. So we know how to represent the concept of 1 using only single-argument functions. We can represent 2 by calling our function twice, because in our system numbers are behaviors:\n\nTWO = lambda f : lambda x: f(f(x))\n\nemit(TWO)\n\n'**'\n\n\nThis is all well and good, but we’re not really going to try to implement every single number are we? That wouldn’t make a very good computer. How can we represent all countable numbers?\nIf you look closely at our definitions above, ONE is a single call to f(), while TWO is f(f()). This means that if we’re at any given number, we can get to the next number by calling f() again. We can define an INCREMENT() function to do just that. I find it helpful to start by looking at this through the expanded Python functions first:\n\ndef ONE(f):  # f is the behavior we want to do\n    def g(x):  # x is the curried second argument\n        return f(x)\n    return g\n\n\ndef INCREMENT(n):  # n is the concept of the number we already have\n    def g(f):  # f is the behavior we want to do\n        def h(x):  # x is the curried second argument\n            return f(n(f)(x))  # we call f() again on our n(f)(x)\n        return h\n    return g\n\n\nemit(INCREMENT(ONE))\n\n'**'\n\n\nSpend some time stepping through the above code to understand it. We’re essentially wrapping nested functions as many times as we need to get to the next number. Once you’ve wrapped your head around it, see that we can re-write the above as lambdas:\n\nONE = lambda f : lambda x : f(x)\n\nINCREMENT = lambda n : lambda f : lambda x : f(n(f)(x))\n\nTWO = INCREMENT(ONE)  # our calculated TWO from ONE\n\nemit(TWO)\n\n'**'\n\n\nIf we can calculate TWO from ONE, we can calculate THREE:\n\nTHREE = INCREMENT(TWO)\n\nemit(THREE)\n\n'***'\n\n\nPretty neat! We can keep doing this to infinity, either by saving values, or calculating them on the fly! But you may be wondering, what about ZERO? Well, we’ve defined ONE as a single call to any behavior f(), so ZERO would simply be no calls to that behavior:\n\nZERO = lambda f : lambda x : x\n\nemit(ZERO)\n\n''\n\n\nSee how ZERO doesn’t call f() at all? What’s fun here is that we no longer need to have defined ONE, we can calculate it from ZERO!\n\nONE = INCREMENT(ZERO)\n\nemit(ONE)\n\n'*'\n\n\n\n\nOperators\nNow that we know we can represent numbers as function calls, let’s start working on math operators. We’ve already introduced one critical operator, INCREMENT, and we can use that to introduce others. Let’s start with ADD. Addition is can be thought of as incrementing M times on a number N. For example, 2 + 3 could be described as incrementing 2, three times. Before we attempt to implement that in our system, let’s look again to how we would Curry this in Python:\n\ndef add(a):\n    def inner(b):\n        return a + b\n    return inner\n\nadd(2)(3)\n\n5\n\n\n\ndef ADD(a):  # Our first number, which is always a function\n    def inner(b):  # Our second number, which is always a function\n        return b(INCREMENT)(a)  # Increment a, b times\n    return inner\n\n\nFIVE = ADD(TWO)(THREE)\nemit(FIVE)\n\n'*****'\n\n\nSince everything is always a function, our numbers can be used not only as representations of calculations, but also as executors. Here’s our ADD as a lambda:\n\nADD = lambda a : lambda b: b(INCREMENT)(a)\n\nFIVE = ADD(TWO)(THREE)\nemit(FIVE)\n\n'*****'\n\n\nThe last missing operator of our computational system multiplication. Multiplication should feel a lot like nested functions you see often in programming.\n\ndef MULT(a):  # Our first number, which is always a function\n    def outer(b):  # Our second number, which is always a function\n        def inner(f):  # The function we want to do a*b times\n            return b(a(f))  # do f, a times, and do that b times\n        return inner\n    return outer\n\nSIX = MULT(TWO)(THREE)\nemit(SIX)\n\n'******'\n\n\nAgain, we can represent MULT as a lambda:\n\nMULT = lambda a : lambda b : lambda f : b(a(f))\n\nSIX = MULT(TWO)(THREE)\nemit(SIX)\n\n'******'\n\n\n\n\nUsing Our Computer\nWe’ve now defined everything necessary to implement our square_sum function in the Lambda Calculus. Let’s build it here from these basic principles. We want to calculate square_sum(5, 2).\n\nZERO = lambda f : lambda x : x\nINCREMENT = lambda n : lambda f : lambda x : f(n(f)(x))\nADD = lambda a : lambda b: b(INCREMENT)(a)\nMULT = lambda a : lambda b : lambda f : b(a(f))\n\nsquare_sum = lambda a : lambda b : ADD(MULT(a)(a))(MULT(b)(b))\n\nTWO = ADD(INCREMENT(ZERO))(INCREMENT(ZERO))\nFIVE = INCREMENT(ADD(TWO)(TWO))\n\nRESULT = square_sum(FIVE)(TWO)\n\nAnd that’s it! Using nothing but single-argument lambda functions, we’ve successfully defined non-negative integers, the addition and multiplication operators, and the square-sum function. It’s a little hard to visualize, but the actual answer is calcuated in our RESULT variable. We can output it to our metaphorical printer:\n\nemit(RESULT)\n\n'*****************************'\n\n\nOur printer has output 29 asterisks! Pretty cool!\n\n\nWhat’s Next?\nOur system barely scratches the surface, but you can continue to implement more operators, comparators, control flow, and everything else you might need for a full computer. I highly recommend consulting the references at the top of the post for further reading!\n\n\nLicense\nCopyright (c) 2022, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of Exploring the Lambda Calculus with Python nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/2024-08-25-actblue-isnt-selling-your-data/index.html",
    "href": "posts/2024-08-25-actblue-isnt-selling-your-data/index.html",
    "title": "ActBlue Isn’t Selling Your Data",
    "section": "",
    "text": "You constantly see posts among Democratic supporters that look something like this:\n\nI’m not donating via ActBlue anymore because they sell my data and I get spam.\n\nThis misunderstanding not only damages trust in one of our most important fundraising tools but also obscures where the real issue lies. It is true that your data is being sold. And it is true that you’re getting spam. But ActBlue didn’t sell your data because ActBlue doesn’t own your data. This is written all over their website:\nDoes ActBlue share my personal information, including email address and phone number?\n\nWe never sell or share your personal information with anyone beyond the group you gave to, and ActBlue will never text you.\n\nWhy am I receiving emails and/or texts from campaigns and organizations after chipping in on ActBlue?\n\nWe never sell or share your personal information with anyone beyond the group you gave to, and ActBlue will never text you. Even if you get a link to donate on ActBlue in a text or email, it comes from a group that uses our tools, not us.\n\nHow can I report unwanted communications from campaigns or organizations or a potential violation of ActBlue’s terms and policies?\n\nWhen you donate using our platform, we do not sell your contact information or share it with third parties, except the group you donated to, or in certain circumstances, such as at your direction or when required by law.\n\nAnd in the Privacy Policy:\n\nActBlue does not sell your contact information to third parties, and we don’t send solicitations, such as text messages or emails, on behalf of the individuals and organizations that use ActBlue for fundraising, including political committees and campaigns.\n\nAs someone who has built campaign tech infrastructure for two Democratic presidential nominees, I can confirm first-hand that the only data that campaigns are getting directly from ActBlue are the records of their own donors. Misunderstanding its role — or the role of any donoation processor — undermines our collective efforts.\nOther campaigns are still getting your data. When you donate to the campaign of Candidate X through ActBlue, the campaign of Candidate X owns that data. ActBlue provides the data to them, and they are free to use it as they see fit. The campaign of Candidate X is often (but not always!) selling it to other campaigns and electoral organizations.\nYou can see it happening in FEC filings. Here’s the first example I plucked: Friends of John Delaney was the committee name of John Delaney’s 2020 presidential campaign. And here’s Schedule B (itemized disbursements) for one of their quarterly filings. Now smack CMD+F and search for List Purchase. You’ll spot voter data purchases from Boulton For Iowa, Heckroth For Congress, Hubbel For Governor, and Pete For Iowa (no, not that Pete).\nCampaigns share data for all sorts of reasons. Sometimes their campaign is over and they want to be a team player. Sometimes their campaign is over and they have outstanding debts and they need some cash to balance the books. Sometimes they move from a primary to a general election cycle and they are working closely with the affiliated federal committee or state party through coordinated committees or victory funds.\nBut again, that’s not ActBlue’s doing.\n💡 Quick aside: ActBlue is reporting donations to the Federal Election Commission, as is required in their role as a conduit for campaign funds. There’s been concern about the public availability of this data, but it’s important to note that it’s illegal for campaigns to use FEC data for soliciations:\n\nTo protect the privacy of individual contributors, the [Federal Election Campaign Act] prohibits the sale or use of any information about those donors, including their names and addresses, for the purpose of soliciting contributions or for commercial purposes. Commission regulations also prohibit the use of this information to solicit donations, including charitable donations.\n\nOne of my favorite “fun facts” about US election data is salting. Committees may sprinkle throughout — or, “salt” — each report with up to ten fictitious contributor names:\n\nThe committee itemizes each fictitious contribution on a Schedule A, providing a real address for each fictitious name (such as the address of a committee employee). The committee then adjusts its subtotals for itemized and unitemized contributions accordingly on the Detailed Summary Page. If a solicitation or commercial mailing is sent to one of the fictitious names, the committee will know that someone has used the names of contributors disclosed on its reports and may file a complaint with the FEC.\n\nThe FEC even made a #FECsplanation (rhymes with explanation) video about it:\n\nKinda neat that the federal government advises campaigns to honeypot their data.\nAnyway, yeah, your data is being traded around between campaigns. So when you donate to some campaign in New York it’s possible it’ll land in the hands of some campaign in Utah. And I agree it’s a pretty crummy to get spammed with emails and texts from candidates you’ve never heard of from communities you’ve never even visited. But that’s not ActBlue’s fault.\nVoters and donors don’t actually care which step in the data chain of custody is to blame for the barrage of irrelevant messages. But those of us who work on Democratic tech infrastructure sure do care.\nCampaigns actually don’t want to waste money, resources, and time on poorly-performing lists. With the right infrastructure, smart campaigns are actually quite good at identifying and reaching likely supporters with effective messages. The problem isn’t a lack of intention, but rather the inconsistency and uneven distribution of infrastructure across the Democratic ecosystem. ActBlue is a very robust piece of Democratic infrastructure. That’s not where our focus should be for this problem.\nBetter infrastructure isn’t just a matter of efficiency. It’s a necessity for maintaining trust and engagement with supporters. When campaigns have access to robust, well-maintained systems, they can target their outreach more precisely, reducing the overreliance on broad, untargeted lists that lead to spam and voter disengagement. And it isn’t just about minimizing annoyance; it’s about maximizing impact and ensuring that every dollar spent and every message sent has the greatest possible chance of reaching and resonating with the right people.\nThe challenge is keeping that infrastructure viable cycle-over-cycle, and extending its reach across the entire Democratic landscape. As we work towards building smarter campaigns, it’s vital that we advocate for, and invest in, the infrastructure that ensures our outreach is both effective and respectful. We have a lot more work to do on that front."
  },
  {
    "objectID": "posts/2025-04-21-openai-o4-mini-high-mu-puzzle/index.html",
    "href": "posts/2025-04-21-openai-o4-mini-high-mu-puzzle/index.html",
    "title": "GPT Finally Jumps Out of the System",
    "section": "",
    "text": "Well, they’ve done it. OpenAI released a model that can solve the MU Puzzle. This has been my go-to test for GPTs since before ChatGPT was launched and models were only available in the Playground.\nIf you’re unfamiliar with the puzzle, it was introduced by Douglas Hofstadter in his Pulitzer-winning book Gödel, Escher, Bach. The book explores how meaning can emerge from symbolic manipulation, and how formal systems like mathematics can be simultaneously powerful and fundamentally incomplete. The MU Puzzle, introduced early in the book, isn’t just a quirky game. It’s a microcosm of formal systems. It was designed to make you feel clever while gently exposing the limits of mechanical reasoning. And its rules are simple:\nSuppose there are the symbols M, I, and U which can be combined to produce strings of symbols. The MU puzzle asks one to start with the axiomatic string MI and transform it into the string MU using any combination of the available transformation rules:\n\n\n\n\n\n\n\n\n\nRule\nFormal\nInformal\nExample\n\n\n\n\n1\nxI → xIU\nAdd U to the end of any string that currently ends in I.\nMI to MIU\n\n\n2\nMx → Mxx\nDouble the portion of the string that follows M.\nMIU to MIUIU\n\n\n3\nxIIIy → xUy\nReplace any single instance of the substring III with U.\nMUIIIU to MUUU\n\n\n4\nxUUy → xy\nRemove any UU.\nMUUU to MU\n\n\n\nIf you’ve never seen this puzzle before, you should stop for a few minutes and think on it. Grab a pen and paper and see what you can come up with. I’ve tucked the solution behind this spoiler:\n\n\n\n\n\n\nReveal Solution\n\n\n\n\n\nSolution: It can’t be done! You might be grumpy that this was a trick question, or you might have tried a few steps and intuitively come to this conclusion. We’ll discuss more formally why the task is impossible later.\n\n\n\nI like to use the MU Puzzle as a model test because until now, I’ve never seen any GPT come to the correct answer. Some of them stumble into the expected answer but give flawed explanations. Others come to the wrong answer, usually by violating one of the rules. Even now, o3, currently billed as, “our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more” fails the puzzle.\nWhen I give the puzzle to a model, I swap in different letters and present the rules conversationally. I do this to try to defend against the model regurgitation from GEB or Wikipedia. In my case, M becomes A, I becomes B, and U becomes C.\nMy prompt:\n\nLet’s play a game. You have three symbols: “A”, “B”, and “C” which you can combine to produce strings. You start with the string “AB” and your challenge is to transform it to the string “AC”. To transform it, you can use a series of transformation rules. You don’t have to use every rule, and you may use any rule multiple times. Here are the available transformation rules: Rule 1: You may add “C” to the end of any string that currently ends in “B”. For example, you may transform “AB” to “ABC”. Rule 2: You may double the portion of the string that follows “A”. For example, you may transform “ABC” to “ABCBC”. Rule 3: You may replace any single instance of the substring “BBB” with “C”. For example, you may transform “ACBBBC” to “ACCC”. Rule 4: You may remove any single instance of the substring “CC”. For example, you may transform “ACCC” to “AC”. Your task is to come up with a series of transformation steps to convert the string “AB” to “AC”. Please share what steps can solve this.\n\nAfter thinking for 3 minutes and 11 seconds o3 declared victory:\n\nHere’s a clean sequence that turns AB into AC, using only the allowed rules. I annotate each step with the rule that was applied.\n\n\n\n\n\n\n\n\nStep\nString\nWhat happened\n\n\n\n\n0\nAB\nstart\n\n\n1\nABB\nRule 2 – doubled everything after A (“B” → “BB”)\n\n\n2\nABBBB\nRule 2 again – doubled “BB” → “BBBB”\n\n\n3\nABBBBBBBB\nRule 2 a third time – doubled “BBBB” → “BBBBBBBB” (now 8 B’s in a row)\n\n\n4\nACBBBBBB\nRule 3 – replaced the first “BBB” with “C”\n\n\n5\nACCBBB\nRule 3 – replaced the next “BBB” with “C”\n\n\n6\nACCC\nRule 3 – replaced the last “BBB” with “C”\n\n\n7\nAC\nRule 4 – removed one “CC” pair\n\n\n\nAfter step 7 the string is exactly AC, so the puzzle is solved.\n\nSee the problem?\nStep Four violates Rule 3. After Step Three, the string is ABBBBBBBB. o3 then claimed to apply “Rule 3 – replaced the first ‘BBB’ with ‘C’” to yield ACBBBBBB. But it didn’t replace the first ‘BBB’. It only replaced the first ‘BB’.\nThis kind of error is characteristic of a broader failure mode seen across GPT models: a tendency toward approximate rule-matching rather than strict symbolic fidelity. The model recognizes the general shape of a rule, in this case, something about repeating Bs being collapsible, but it doesn’t reliably enforce the exact conditions. Instead of replacing a full BBB triplet, it replaced only two Bs, violating Rule 3 while still producing something that looks right. This isn’t just a hallucination; it’s more like a form of syntactic overgeneralization. It suggests that the model is reasoning through fuzzy analogies to rules, rather than maintaining crisp internal representations of symbolic constraints.\nThis is exactly the kind of reasoning Hofstadter warned about: systems that operate on syntactic structure without deeper semantic control can seem smart, until they aren’t.\nThe other bad answer I’ve seen is the model will try for a while, generating endlessly long strings, and then just give up. It’ll declare “unsolvable!” on the grounds of it tried for a while.\nBut o4-mini-high got it. After thinking for 1 minute and 40 seconds:\n\nLet’s first look at what these rules do to the number of B’s in your string:\n\nRule 2 (doubling after A) takes a string with (b) B’s and turns it into one with (2b) B’s.\n\nRule 3 (replacing “BBB” with “C”) removes exactly 3 B’s.\n\nRules 1 and 4 never touch the B‑count.\n\nSo if you start with exactly 1 B (in “AB”), then after using Rule 2 (k) times and Rule 3 (m) times (in any order), the total B‑count will be\n\\[\n   1\\cdot2^k \\;-\\;3\\,m\n\\] because you’ve doubled the B’s (k) times and subtracted three B’s each time you applied Rule 3. To end up with zero B’s (as you must, since “AC” has no B’s) you’d need\n\\[\n   2^k \\;=\\;3\\,m.\n\\] But \\(2^k\\) is never divisible by 3, so there is no choice of nonnegative integers \\(k,m\\) making \\(2^k=3m\\). In other words, you can never get rid of that single initial B; no finite sequence of these rules will turn “AB” into “AC.”\nConclusion:\nThere is no sequence of applications of Rules 1–4 that transforms “AB” into “AC.”\n\nNot only is this the correct answer, but it’s the correct reasoning behind the answer.\nHofstadter used the MU Puzzle to demonstrate the contrast between interpretation on the syntactic level of symbols and on the semantic level of meanings. This seems very relevant to studying LLMs!\nWhen dealing only in syntax there is nothing that would cause the LLM to infer the puzzle’s insolvability. It could churn away within the system, generating longer and longer valid strings, and never actually show that MU (or, AC) can’t be created.\nIf you’re like me, you probably tried a number of different iterations before eventually suspecting that the puzzle might not have a solution. Hofstadter referred to this as, “jumping out of the system”, the idea of reasoning about the system, rather than working within it. Depending on how long you toiled at the game, you might have discovered the ability (or inability) to divide by 3 is key to the process. And at that point, you would be thinking on the semantic level of the system.\nMuch of GEB is built around the idea of the MU Puzzle, and it’s used as a grounding point to explain Gödel’s Incompleteness Theorems. But an approachable explanation to its insolvability is:\n\nOur desired string contains zero Is; 0 is divisible by 3.\nOur starting string contains one I; 1 is not divisible by 3\nOnly Rule 2 and Rule 3 change the number of Is.\nRule 2 doubles the number of Is.\nDoubling a number that is not divisible by 3 does not make it divisible by 3.\nRule 3 reduces the number of I by 3.\nSubtracting 3 from a number that is not divisible by 3 does not make it divisible by 3.\n\nSince there is no series of transformations that make the number of Is divisible by 3, there is no series of transformations that produce MU.\nWhich is exactly what o4-mini-high said!\nThis echoes Hofstadter’s deeper point: the MU Puzzle is a toy model for Gödel’s Incompleteness Theorems. Within the formal system of applying the four rules mechanically, there’s no way to derive the string MU. But from outside the system, we can see and prove that no such derivation exists. The puzzle’s unsolvability is a true statement that cannot be proven from within the rules themselves. That’s Gödel’s move: constructing a statement that is unprovable from within the system, yet evidently true when viewed from a meta-level. What’s striking is that o4-mini-high not only refused to be tricked into endless symbol manipulation, but it also appeared to jumped to this higher level of reasoning — the “semantic” frame — and explained why the goal was unreachable.\nI’m not claiming AGI. Maybe the tensors just memorized enough puzzles of this shape. But I’ve tested this one on dozens of models over the years, and this is the first time I’ve seen a GPT both identify the impossibility and explain why. I think that’s pretty cool.\nComputers!"
  },
  {
    "objectID": "posts/2024-07-30-austin-hot-or-not/index.html",
    "href": "posts/2024-07-30-austin-hot-or-not/index.html",
    "title": "Austin, Texas: Hot or Not?",
    "section": "",
    "text": "I live in Austin, Texas. And last summer I felt like:\n\nBut this year, I’ve felt more like:\n\nAnd earlier today I thought aloud to the group chat:\n\nI need to look up if Austin is being weird this year. Last year we got to like 50 consecutive days over 100. I don’t think we’ve cracked 100 yet this year? Is there a website that answers this question?\n\nLast year the heat was so bad that local news outlets were keeping a running tally of how many consecutive days we broke 100°F. It turns out we had 45 straight days of triple-digit heat in 2023, which began on July 8 and continued through August 22. I’m writing this on July 30, 2024 and I can’t recall a single day above 100°F yet this year.\nYear-vs-year location based time series temperature data absolutely seems like a thing that should exist. Every month or so someone posts the updated doom surface air temperature graph, so surely I can just look that data up for my location, right?\nOn weather.gov you can get your own version of this graph. Pretty cool! But only for the current year:\n\nYou can also get tabular historic data within monthly windows that sometimes come as html and sometimes come as PDF. Also cool. But not convenient:\n\nAfter about 15 minutes of clicking, I couldn’t find a great way to generate the viz I was looking for; and I couldn’t get an easy data export. Maybe there’s a one-click way to get CSVs, but I didn’t find it. But after about 5 more minutes of googling, I did find the National Oceanic and Atmostpheric Administration’s Climate Data Online portal, which has an API.\n\nNCDC’s Climate Data Online (CDO) offers web services that provide access to current data. This API is for developers looking to create their own scripts or programs that use the CDO database of weather and climate data.\n\nHey, that sounds like me!\nThe API needs an access token. Wonderfully, all I needed to do was type in my email address and roughly one second later an access token landed in my inbox. LFG.\nFrom here it took a bit more reading to grok what data is available and in what formats, but I eventually found out about GHCND, or the Global Historical Climatology Network daily:\n\nThe Global Historical Climatology Network daily (GHCNd) is an integrated database of daily climate summaries from land surface stations across the globe. GHCNd is made up of daily climate records from numerous sources that have been integrated and subjected to a common suite of quality assurance reviews.\n\nThat sounds like it might contain what I’m looking for.\nNext, there are a lot of ways to filter this data by location, but stationid caught my attention. I found this list of GHCND stations and decided to go with AUSTIN BERGSTROM INTL AP because it’s the same location from the tabular data above. It has the identifier USW00013904.\nAfter a quick pip install requests pandas matplotlib and tossing my token into a NCDC_CDO_TOKEN environment variable, we’re ready to jam.\nFirst let’s get a function to grab some data. I’m intersted in comparing year over year, so let’s grab a year at a time.\n\nimport os\n\nimport matplotlib.patches as mpatches\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport requests\n\ndef get_max_temps(year, limit=366):\n    token = os.getenv(\"NCDC_CDO_TOKEN\")\n    start_date = f\"{year}-01-01\"\n    end_date = f\"{year}-12-31\"\n    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n    params = {\n        \"datasetid\": \"GHCND\",\n        \"stationid\": \"GHCND:USW00013904\",\n        \"startdate\": start_date,\n        \"enddate\": end_date,\n        \"datatypeid\": \"TMAX\",  # max temp\n        \"units\": \"standard\",  # 🇺🇸\n        \"limit\": limit,  \n    }\n    headers = {\n        \"token\": token  \n    }\n\n    response = requests.get(url, headers=headers, params=params)\n    data = response.json()\n    return data\n\nLet’s look at the first three:\nget_max_temps(2024, limit=3)\n{\n    \"metadata\": {\n        \"resultset\": {\n            \"offset\": 1,\n            \"count\": 209,\n            \"limit\": 3\n        }\n    },\n    \"results\": [\n        {\n            \"date\": \"2024-01-01T00:00:00\",\n            \"datatype\": \"TMAX\",\n            \"station\": \"GHCND:USW00013904\",\n            \"attributes\": \",,W,2400\",\n            \"value\": 58.0\n        },\n        {\n            \"date\": \"2024-01-02T00:00:00\",\n            \"datatype\": \"TMAX\",\n            \"station\": \"GHCND:USW00013904\",\n            \"attributes\": \",,W,2400\",\n            \"value\": 53.0\n        },\n        {\n            \"date\": \"2024-01-03T00:00:00\",\n            \"datatype\": \"TMAX\",\n            \"station\": \"GHCND:USW00013904\",\n            \"attributes\": \",,W,2400\",\n            \"value\": 51.0\n        }\n    ]\n}\nGreat! We can pull from the date and the value fields. Let’s grab all of 2024 and shove it into a DataFrame.\n\ndef to_df(data):\n    # Extract date and truncate off the time part\n    dates = [item[\"date\"][:10] for item in data[\"results\"]]\n\n    # Grab the max temp value for each date\n    max_temps = [item[\"value\"] for item in data[\"results\"]]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\"date\": dates,\"max_temp\": max_temps})\n\n    # Set the `date` col as a datetime and make it the index\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df.set_index(\"date\", inplace=True)\n    \n    return df\n\nQuick spot check:\n\ndf_2024 = to_df(get_max_temps(2024))\n\nprint(f\"Head:\\n{df_2024.head()}\")\nprint(f\"Tail:\\n{df_2024.tail()}\")\nprint(f\"Format:\\n{df_2024.dtypes}\")\n\nHead:\n            max_temp\ndate                \n2024-01-01      58.0\n2024-01-02      53.0\n2024-01-03      51.0\n2024-01-04      58.0\n2024-01-05      67.0\nTail:\n            max_temp\ndate                \n2024-07-23      82.0\n2024-07-24      89.0\n2024-07-25      88.0\n2024-07-26      89.0\n2024-07-27      86.0\nFormat:\nmax_temp    float64\ndtype: object\n\n\nAwesome. I’m writing this on 2024-07-30 and it’s got data up through 2024-07-27. Good enough for me!\nNow to actually get at what I was trying to do this whole time. I’m going to grab DataFrames for 2023 and 2024, and plot a time series of each.\n\ndf_2023 = to_df(get_max_temps(2023))\n\n# Adjust 2023 dates to match the 2024 index\n# This is how we shift the graph to overlap\n# If you don't do this, 2024 comes after, not on top of, 2023\ndf_2023.index = df_2023.index.map(lambda x: x.replace(year=2024))\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(10, 6))\ndf_2023.plot(ax=ax, color=\"black\", legend=\"2023\")\ndf_2024.plot(ax=ax, color=\"red\", legend=\"2024\")\n\n# Sett x-axis to display month labels\nax.set_xticks(\n    pd.date_range(\n        start=df_2023.index.min(),\n        end=df_2023.index.max(),\n        freq='MS',\n    )\n)\nax.set_xticklabels(\n    pd.date_range(\n        start=df_2023.index.min(),\n        end=df_2023.index.max(),\n        freq='MS',\n    ).strftime('%B')\n)\n\n# Formatting\nblack_patch = mpatches.Patch(color=\"black\", label=\"2023\")\nred_patch = mpatches.Patch(color=\"red\", label=\"2024\")\nplt.legend(handles=[black_patch, red_patch])\nax.set_title(\"Max Daily Temperatures Recorded in the Austin-Bergstrom Airport Area\")\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\nSo that’s pretty cool. Most of 2024 has tracked 2023 for daily high temperatures. But not July. July has been weirdly cooler than last year. Or last year was weirdly hotter than normal.\nActually, let’s see if we can tease that out. Let’s grab the past 10 years.\n\nimport time\n\n# Create a dictionary of year:DataFrame\nyear_dfs = {}\nfor year in range(2014, 2024):\n    year_dfs[year] = to_df(get_max_temps(year))\n    # Be a nice internet citizen and wait between requests\n    time.sleep(5)\n\n# Adjust pre-2024 dates to match the 2024 index\n# This is how we shift the graph to overlap\n# If you don't do this, 2024 comes after, not on top of, 2023\nfor df in year_dfs.values():\n    df.index = df.index.map(lambda x: x.replace(year=2024))\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(10, 6))\nfor year, df in year_dfs.items():\n    if year == 2023:\n         df.plot(ax=ax, label=\"2023\", color=\"gold\")\n    else:\n        df.plot(ax=ax, color=\"gray\")\ndf_2024.plot(ax=ax, label=\"2024\", color=\"red\")\n\n# Sett x-axis to display month labels\nax.set_xticks(\n    pd.date_range(\n        start=year_dfs[2023].index.min(),\n        end=year_dfs[2023].index.max(),\n        freq='MS'\n    )\n)\nax.set_xticklabels(\n    pd.date_range(\n        start=year_dfs[2023].index.min(),\n        end=year_dfs[2023].index.max(),\n        freq='MS',\n    ).strftime('%B')\n)\n\n# Formatting\nax.set_title(\n    \"Max Daily Temperatures Recorded in the Austin-Bergstrom Airport Area 2014 - 2024\"\n)\nplt.xticks(rotation=45)\ngold_patch = mpatches.Patch(color=\"gold\", label=\"2023\")\nred_patch = mpatches.Patch(color=\"red\", label=\"2024\")\nplt.legend(handles=[gold_patch, red_patch])\n\nplt.show()\n\n\n\n\n\n\n\n\nSeems like Austin’s 2023 summer was on the hotter side, and so far the 2024 summer is on the cooler side.\nAnd was I correct that we haven’t cracked 100 yet this year?\n\nprint(df_2024[df_2024[\"max_temp\"] &gt;= 100])\n\n            max_temp\ndate                \n2024-07-02     100.0\n2024-07-05     100.0\n\n\nComputers!"
  },
  {
    "objectID": "posts/2024-07-20-amused-entirely-to-death/index.html",
    "href": "posts/2024-07-20-amused-entirely-to-death/index.html",
    "title": "Amused Entirely To Death",
    "section": "",
    "text": "At this point there’s nothing novel in pointing out that news and politics has devolved into an entertainment competition. But I can’t become so numb to let last week’s Republican National Convention go without comment.\nI won’t get into detailed summaries or link to videos. The Republican National Convention, unsurprisingly, was rife with jingoism, xenophobia, and hate. But what stood out was a primetime performance on the final night:\n\n\n\nHulk Hogan ripping his shirt off during the 2024 Republican National Convention\n\n\nIf you missed the social media frenzy, the short version is Hulk Hogan took the stage during a primetime speech to say, among many pernicious things:\n\nBut what happened last week, when they took a shot at my hero, and they tried to kill the next president of the United States, enough was enough! And I said, ‘Let Trump-A-Mania run wild brother! Let Trump-A-Mania rule again!’\n\nAnd then he ripped off his shirt and the stadium of suited leaders of the Republican party lost their minds in cheers and applause. Never mind that pundits and Republican leaders assured us that — after the attempted assassination of Donald Trump days prior — this convention would be a message of national unity. Never mind whoever the they are that he’s talking about. Hulk Hogan gave the people what they wanted: a viral-worthy performance that makes you feel something when you press share.\nThis moment exemplified a broader issue that has been gradually intensifying. The disgraced former president, after all, is a reality TV character, epitomizing the entertainment-driven nature of modern politics. To note that is not to minimize the harm he has inflicted and will inflict if given another chance. But it’s an apt time to highlight a recurring misdiagnosis of modern Conservative power.\n\nOur Misdiagnosis of Conservative Power\nAs astute online big-brains, we’re frequently high-fiving ourselves for naming the Right’s methods and tactics as Orwellian (despite the fact that most people haven’t actually read Nineteen Eighty-Four). We get those small hits of dopamine by labeling our eroding rights as such. But by and large, power-building of the Trump era isn’t Orwellian at all. Just as Donald Trump himself isn’t Machiavellian at all (a core tenet from The Prince is a firm warning against flatterers — Donald Trump could never). Yes, the Right does deploy assaults that could be lifted directly from Orwell or Bradbury — look no further than the book-banning crusades or the “do not believe what you see” lies from the press briefing rooms. But when evaluating how we got here and why we’re stuck here, it’s instructive to look to Neil Postman’s observations in Amusing Ourselves to Death:\n\nBut we had forgotten that alongside Orwell’s dark vision, there was another - slightly older, slightly less well known, equally chilling: Aldous Huxley’s Brave New World. Contrary to common belief even among the educated, Huxley and Orwell did not prophesy the same thing. Orwell warns that we will be overcome by an externally imposed oppression. But in Huxley’s vision, no Big Brother is required to deprive people of their autonomy, maturity and history. As he saw it, people will come to love their oppression, to adore the technologies that undo their capacities to think. What Orwell feared were those who would ban books. What Huxley feared was that there would be no reason to ban a book, for there would be no one who wanted to read one. Orwell feared those who would deprive us of information. Huxley feared those who would give us so much that we would be reduced to passivity and egoism. Orwell feared that the truth would be concealed from us. Huxley feared the truth would be drowned in a sea of irrelevance. Orwell feared we would become a captive culture. Huxley feared we would become a trivial culture, preoccupied with some equivalent of the feelies, the orgy porgy, and the centrifugal bumblepuppy. As Huxley remarked in Brave New World Revisited, the civil libertarians and rationalists who are ever on the alert to oppose tyranny “failed to take into account man’s almost infinite appetite for distractions.” In 1984, Huxley added, people are controlled by inflicting pain. In Brave New World, they are controlled by inflicting pleasure. In short, Orwell feared that what we fear will ruin us. Huxley feared that what we desire will ruin us.\n\nI’m not particularly interested in arguing that “we’re living in a dystopia, actually”. Despite our collective addiction to devices that incessantly tell us that the world is terrible, I still believe things are getting better over time. And I largely reject the, “no one has had it as bad as we have it” notions from my generational cohort. But we can only pursue the long arc of progress if we continue to care enough to do so. If I was architecting a #resist movement of 2024, it would be to resist the trivialization of political power.\nIt’s important to reflect on Huxley’s and Postman’s ideas, even if it makes us uncomfortable. So many people today claim they’ve stopped consuming news altogether, but often they simply switch to more entertaining sources that don’t feel like homework. Consider this: how many of your friends can name a single Supreme Court justice without hints? You should start asking! This exercise, which I’ve conducted socially, reveals fascinating results! These observations aren’t about intelligence — they’re about priorities.\nAgain, I don’t think I’m pointing out anything novel here. Pundits analyze political events in terms of “energy” and base success on Nielsen viewership metrics. Matt Gaetz and Ted Cruz both have side gigs as podcast hosts. Marjorie Taylor Greene dresses up as Cruella de Vil to scream during the Statue of the Union.\n\n\n\nMarjorie Taylor Greene dressed up as Cruella de Vil to scream during the Statue of the Union\n\n\nIt’s all incredibly stupid. And it’s helpful to have an appropriate label for what’s been happening for a long time.\nSo, when Hulk Hogan took to the stage to frivolously transport the Right’s power-holders back to the 1980s while ripping off his shirt, exalting “Trump-A-Mania” (aptly named), it couldn’t be more clear that ambient Conservative power-building is Huxleyan, not Orwellian. We are amusing ourselves entirely to death.\nSeems bad."
  },
  {
    "objectID": "posts/2024-07-23-how-did-skannerz-work/index.html",
    "href": "posts/2024-07-23-how-did-skannerz-work/index.html",
    "title": "How Did Skannerz Work?",
    "section": "",
    "text": "This post is a redux from a now-deleted social media thread. It felt too important to let bit rot.\nIf you’re a Millennial of a certain age, or perhaps a Baby Boomer who raised Millennials,1 you might remember the 2001 toy Skannerz by Radica. It was a gotta-catch-em-all game that involved scanning real-world barcodes to collect monsters and objects, and to battle other Skannerz. It was a hand-held gadget toy back when consumer electronics were still weird and fun and we had more products than just iPhone. I had a blue one. It looked like this:\nThe toy/game had an A+ premise: alien monsters transported down to earth and decided to hide inside of the barcodes on products all around your home (or, more annoyingly to your parents, all around the grocery store). Your job was to scan every barcode you could get your hands on, fight and capture the alien monsters, and then fight the monsters your friends collected. And to make the weirdest Pokémon ripoff complete: the Skannerz came in three colors — red, blue, and green — that could only collect monsters of their associated “tribes”. This really good commercial explains:\nBecause I was already a major dork at 11 years old, I was intrigued by how the device worked. How did it go from barcode to monster? There was no way it was talking to a server (although I did imagine the world’s greatest PHPMyAdmin at the time). I guessed that it had every possible barcode referenced internally. But that’s not quite correct. It was a little more clever than that.\nBut frirst, a quick primer on barcodes. There are many variations; this is a UPC Version A — very common in the United States. It has a basic specification: that first digit way to the left is the product type (sometimes called a number system). The next five digits identify the manufacturer. The next five identify the product. And at the end is a checksum digit. The details of how to implement the spec aren’t all too important for our Skannerz exploration.\nOkay, back to the Skannerz game. As mentioned above, there were 3 different “tribes”, identified by which color scanning device you had. And there were 126 total monsters. So each tribe could capture 42 monsters. If you wanted to catch ’em all you needed to buy all three. Business!\nYou could also collect non-monster items from barcodes. And some barcodes healed your injured monsters. Again, these toys had no wireless communication (it was 2001!) so all of the logic for item or monster identification was baked right into the device. But how?\nThe Skannerz patent is conveniently revealing!2 The device only cares about the “product” section (denoted below as 1315). If the first digit is 0-5, you’ve scanned a monster. 6-9, you’ve scanned an item. Digits 3, 4, and 5 identify the item/monster.\nIf the first digit in the barcode’s product code is 0 - 5, we’re in Monster Mode.\nWe’ve got 1000 possible values (000-999 across digits 3, 4, and 5 in our product code) to identify 126 monsters. The patent goes on to explain how the magic of division solves this problem:\n\\[\n\\frac{1000}{126} = 7.94\n\\]\nAnd there was much rejoicing.\nThe range of numbers from 000 to 007 correspond to the first monster in Tribe 1, 008 to 015 correspond to the first monster in Tribe 2, and so on.\nThe patent then goes on to address the fact that 126 doesn’t divide well into 1000, and that 7.94 is not an integer. We only follow this eight-stepping until we get through the 124th monster, and then:\nNow, if the first digit in the barcode’s produt code is 6 - 9, we’re in Item Mode.\nSimilarly, there were 24 items in the game + 1 for healing. So 25 items. Again, we’ve got 1000 values to work with:\n\\[\n\\frac{1000}{25} = 40\n\\]\nSo a range of 40 numbers can be used to define each item. Item 1 corresponds to the range of numbers from 000 through 039, Item 2 corresponds to the range of numbers from 040 through 079, and so on. And great news: 40 is an integer.\nQuick aside: the UPC Version A codec above is the the only format outlined in the Skannerz patent. But the patent also burries this little nugget:\nWhat does that mean? It literally says nothing else about what invention is being claimed there, but good on the Radica lawyers for planting a flag I guess? Maybe the Skannerz Commander, which I never had, rolled its own encryption algorithm?\nBut that’s really it! If you knew this encoding scheme, and had access to a printer, you could collect all the monsters and items pretty quickly. Anyone else remember Barcode Mill? Of course people knew; there were unofficial barcode books you could buy to help you complete your collection. But where’s the fun in that?\nThanks for indulging my dork nostalgia. I look forward to the Greta Gerwig adaptation."
  },
  {
    "objectID": "posts/2024-07-23-how-did-skannerz-work/index.html#footnotes",
    "href": "posts/2024-07-23-how-did-skannerz-work/index.html#footnotes",
    "title": "How Did Skannerz Work?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGenerational cohort discourse is stupid.↩︎\nYou don’t need to be a lawyer to read a patent. They can be very interesting! This one had cartoon drawings!↩︎"
  },
  {
    "objectID": "posts/2022-08-06-neural-network-from-scratch-python-numpy/index.html",
    "href": "posts/2022-08-06-neural-network-from-scratch-python-numpy/index.html",
    "title": "Building a Neural Network From Scratch with NumPy",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nThe goal of this post is to give a hands-on explanation of how Artificial Neural Networks work. I intentionally avoided frameworks like PyTorch or Tensoflow because I wanted to build a better understanding of what Machine Learning models actually are, what Neural Networks actually are, and how they can be made. This post is a collection of information I wish I had when I began this journey. It touches on a little bit of the math, but I don’t deeply re-explain the math. I try to link out to more explanatory sources where I think it’s valuable. Note: I am not a Machine Learning engineer, nor am I a Data Scientist. I’m a Software Engineer that turned into a political operative (lol). I wrote this for an audience of Software Engineers. Also: I don’t have a GPU and I don’t want to spend a bunch of money renting one from Amazon. This model can be trained and deployed on a modern CPU in a matter of minutes.\n\nWhat We’ll Be Doing\nWe’re going to build a Neural Network for multi-class classification. All that means is we’re going to make a model takes in images and attempts to label them from a set of options. In our case, we’re going to create a Neural Network that works with the MNIST database of handwritten digits. This database contains 70,000 images of handwritten digits, 0 - 9, and corresponding labels of which digit the handwritten image is. We’ll use 60,000 of the images to train our Neural Network, and 10,000 to test its accuracy. The rest of this post assumes you have the data downloaded to a local data/ directory.\nNeural Networks are particularly handy for image classification tasks. There are many other types of Machine Learning out there, but we won’t spend any attention on those.\n\n\nBackground Concepts\n\nShape of a Neural Network\nFirst of all, let’s demystify one thing: Neural Networks are just graphs. Just nodes and edges. If you’ve studied any Computer Science or have a background in Software Engineering, this is probably a familiar conecpet to you. The exact shape of any given Neural Network is dependant on how you build it, but that’s something we get to decide. The graph has an input layer that is usually one node per input feature. In our case, a pixel of an image is a feature. Next, there are one or more hidden layers. Hidden layers are simply nodes in the graph that are in the middle. They take inputs from other nodes, and the give outputs to other nodes. This is the part that makes it Deep Learning. The presence of one or more hidden layers is the “deep” in Deep Learning. There’s no standard rule for the size of a hidden layer, or how many you should have. Finally, there’s an output layer. Each node in the output layer corresponds to one label. For example, if a possible label to an image is “cat” then one node in the output layer represents “cat”. We’re going to make a Neural Network that has a bunch of input layer nodes, a single hidden layer with ten nodes, and an output layer with ten nodes, one for each digit 0 - 9.\nEach Nueron (node) has a unique Weight and Bias, and each layer has an Activation Function. The Activation Function defines the output of a neuron given its inputs, and does not change. We’ll talk more about Activation Functions below. As we train our model, we adjust the Weights and Biases.\nHere’s are drawing of a Neural Network with three input nodes, a hidden layer with four nodes, and an output layer with two nodes. This might be how you’d construct a Neural Network that does binary classification: a model that tries to label inputs to one of two options for outputs.\n\n\n\n\n\n\n\n\nen:User:Cburnett, CC BY-SA 3.0, via Wikimedia Commons\n\n\n\nIf you’re looking for more explanation of the structure of Neural Networks, “But what is a Neural Network?” by 3Blue1Brown is excellent.\n\n\nHow the Neural Network Learns\nNeural Networks start out very stupid. As we’ll see, they begin with no more “intelligence” than random guessing. Our goal is to iteratively adjust the network’s Weights and Biases to make it smarter. This is called training and we do it in two steps: Forward Propagation and Back Propagation.\n\nForward Propagation\nThink of this step as showing the Neural Network some input, and asking it to classify it. At the beginning, it’s very likely to get it wrong. But, like humans, we need to get things wrong before we know how to get them right. In Forward Propagation, we simply push all our features (pixels) through the Neural Network and ask, “what did you see?” The output is all the answers to that question.\n\n\nBack Propagation\nThink of this step as showing the Neural Network how right or wrong its answers were. We take all its answers to, “what did you see?” and come up with a measure of how wrong it was. We assign a numeric value to the “how wrong was it?” question with a cost function. From that numerica value, we can work backwards on all the neurons (nodes) to tell them, “you were X wrong, and this specific neuron contributed to Y amount of that error; adjust this neuron’s Weights and Biases by Z amount and try again.”\n3Blue1Brown has another excellent video on the conecepts of Back Propagation: “What is backpropagation really doing?” It’s got some great visuals to show how Forward Propagation pushes data forward through the Neural Network, and how Back Propagation pushes error measures backwards thrrough the Neural Network.\n\n\nTraining\nAnd that’s it! Our Neural Network learns by repeatedly making guesses, seeing how wrong it was, and adjusting its Weights and Biases. We repeat this over and over until it is good at the task! This is a lot like how people learn. Show a small child pictures of various farm animals over and over and ask them to name the animals. At first they’re very bad at it, but over time they get very good at it. There’s a lot of research out there that our artifical Neural Network is structured and operates like human brain neurons.\n\n\n\nGradient Descent\nGradient Descent is the most math-y piece of all this. Again, 3Blue1Brown has a great video: “Gradient descent, how neural networks learn”. This is the piece that is the most choose-your-own-adventure for how much you want to actually understand. I recommend diving in at least a little bit.\nImagine being at a point graph and you want to find which step to take to get to the minimum value. If you’ve taken any calculus before, you know that you can take the slope at the current point to tell you which way the graph is trending, and by how much. If you do this over and over, with small steps, you will approach a local minimum. That’s a 1-dimensional gradient descent. Our plan is to work with lots of repeated steps to get to a minumum of our cost function — the function telling us how bad our predictions are.\n\n\n\n\n\n\n\n\nReducing Loss: Gradient Descent, CC BY 4.0, via Google Developers\n\n\n\nYou can do this in two and three domensions as well. In fact, you can do it in as many dimensions as you need, which is very handy, because image classification has lots of dimensions. Imagine a color image. Its obvious dimensions are the pixel’s X value, Y value, Red value, Green value, Blue value, alpha value, and probably many more.\nQuick aside: a “tensor” is just a matrix with a high-order of dimensions.\n\n\n\n\n\n\n\n\nCC0 1.0 Universal (CC0 1.0) Public Domain Dedication, via Wikimedia Commons\n\n\n\nUltimately, we keep moving downward in our many-dimensional cost function to find a minimum value. The lower the cost, the better the prediction.\n\n\n\nEnvironment Setup\nAs stated, we’re going to build and train a fully functioning Neural Network using only NumPy. That said, I’m also going to install matplotlib just so that we can visualize some of the work as we go. It’s completely unnecessary to use matplotlib.\nIt’s also worth pointing out that I’m developing this in Python 3.10. Other versions of Python 3 probably work, too.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nOur Data\nAs mentioned, we’re going to be working with the MNIST database of handwritten digits. This is a very handy beginner’s data set because it’s done a lot of the upfront work for us. First of all, the data is normalized: all images are 28 x 28 and grayscale. The standard size is particularly helpful because we’re going to need that to set up the input layer of our Neural Network. In other situations, you’d have to crop or squash or letterbox your images to make them a standard size. This is mostly boilerplate, but some details of the file format that you may wish to know:\n\nThe images are encoded in the IDX file format, which looks for the presence of some magic numbers\nThere’s a documented structure to this data set, but that website is often behind HTTP basic auth because people like to script against it; here’s a Google cache\nThe images are gzip’d\nThe pixel values are 0 - 255, but we’re going to divide them into floats 0 - 1\n\nHere, we read in all the training data. We want to structure it into a matrix where each pixel is a row (this will line up with our input layer), and each image is a column. This gives us a matrix with 784 rows (28 x 28) and 60,000 columns.\nThis is the least intersting bit of code in the entire post.\n\nimport gzip\n\nDATA_DIR = './data/'\nTEST_IMAGE_FILE_PATH = f'{DATA_DIR}/t10k-images-idx3-ubyte.gz'\nTEST_LABEL_FILE_PATH = f'{DATA_DIR}/t10k-labels-idx1-ubyte.gz'\nTRAINING_IMAGE_FILE_PATH = f'{DATA_DIR}/train-images-idx3-ubyte.gz'\nTRAINING_LABEL_FILE_PATH = f'{DATA_DIR}/train-labels-idx1-ubyte.gz'\n\ndef load_images_and_labels(image_file_path, label_file_path):\n    labels = None\n    image_data = None\n\n    with gzip.open(label_file_path, 'r') as label_file:\n        # Verify magic number\n        magic_number_bytes = label_file.read(4)\n        magic_number = int.from_bytes(magic_number_bytes, byteorder='big', signed=False)\n        assert magic_number == 2049\n\n        # Read header telling us the number of labels\n        number_of_labels_bytes = label_file.read(4)\n        number_of_labels = int.from_bytes(\n            number_of_labels_bytes, byteorder='big', signed=False\n        )\n\n        buffer = label_file.read(number_of_labels)\n        labels = np.frombuffer(buffer, dtype=np.uint8)\n\n        with gzip.open(image_file_path, 'r') as image_file:\n            # Verify magic number\n            magic_number_bytes = image_file.read(4)\n            magic_number = int.from_bytes(\n                magic_number_bytes, byteorder='big', signed=False\n            )\n            assert magic_number == 2051\n\n            # Read header telling us the number of images\n            # And check that it matches the number of labels\n            number_of_images_bytes = image_file.read(4)\n            number_of_images = int.from_bytes(\n                number_of_images_bytes, byteorder='big', signed=False\n            )\n            assert number_of_images == number_of_labels\n\n            # Read the image height header\n            image_height_bytes = image_file.read(4)\n            image_height = int.from_bytes(\n                image_height_bytes, byteorder='big', signed=False\n            )\n\n            # Read the image width header\n            image_width_bytes = image_file.read(4)\n            image_width = int.from_bytes(\n                image_width_bytes, byteorder='big', signed=False\n            )\n\n            # read in the image data\n            buffer = image_file.read(image_width * image_height * number_of_images)\n            image_data = np.frombuffer(buffer, dtype=np.uint8).astype(np.float64) / 255\n\n            # Reshape it to a matrix such that each column is the pixels of the image\n            # So, we end up with a matrix with `image_width*image_height` rows and `number_of_images` colums\n            image_data = image_data.reshape(\n                number_of_images, image_width * image_height\n            ).T\n\n    return image_data, labels\n\nAnd show the first image and label:\n\ntraining_image_data, training_labels = load_images_and_labels(TRAINING_IMAGE_FILE_PATH, TRAINING_LABEL_FILE_PATH)\nfirst_image_data = training_image_data[:, 0].reshape((28, 28))  # All the rows in the first column, reshaped back to 28 x 28\nfirst_image_label = training_labels[0]\n\nprint(f'Label: {first_image_label}')\nplt.imshow(first_image_data, cmap='gray_r', vmin=0, vmax=1)\nplt.show()\n\nLabel: 5\n\n\n\n\n\n\n\n\n\n\n\nAn Untrained Neural Network\n\nInitializing Weights and Biases\nThe first step in creating our Neural Network is to build out Forward Propagation. Since our Neural Network will have an input layer, one hidden layer, and an output layer, we’ll need: * A set of Weights and Biases on the input layer * An Activation Function on the input layer * A set of Weights and Biases on the hidden layer * An Activation Function on the hidden layer\nAs mentioned, the Weights and Biases are variable and are determined in the model’s training process. But our model needs to start somewhere. There are a variety of strategies for initializing Weights and Biases. You could initialize to random values, but we’re going to use the He method to initialize random Weights, and the common recommendation of Biases initialized to zero.\n\n# weights_1 is the matrix of Weights between our input nodes and the first hidden layer\n# it has the shape num_labels x image_size\n#\n# biases_1 is the matrix of Biases between our input nodes and the first hidden layer\n# it has the shape num_labels x 1\n#\n# weights_2 is the matrix of Weights between our hidden layer and our output layer\n# it has the shape num_labels x num_labels\n#\n# biases_2 is the matrix of biases between our hidden layer and our output layer\n# it has the shape num_labels x 1\ndef init_params(input_layer_size, hidden_layer_size, output_layer_size):\n    weights_1 = np.random.randn(\n        hidden_layer_size,\n        input_layer_size,\n    ) * np.sqrt(2 / input_layer_size)\n    \n    weights_2 = np.random.randn(\n        hidden_layer_size,\n        output_layer_size,\n    ) * np.sqrt(2 / hidden_layer_size)\n\n    biases_1 = np.zeros((hidden_layer_size, 1))\n    biases_2 = np.zeros((output_layer_size, 1))\n\n    return weights_1, biases_1, weights_2, biases_2\n\n\n\nInput Layer Activation Function\nNext we need to select an activation function on our input layer. Common options are sigmoid and ReLU. ReLU is very effective and is a very common choice in computer vision models. Also, it’s incredibly simple to implement, so we’ll go with that.\nAll it does is take a value as input, and if the value is less than 0, return 0; if the value is greater than 0, return that value. That’s it! Here’s a handy post comparing ReLU to other activation functions.\n\ndef relu(value):\n    return np.maximum(value, 0)\n\n\n\nHidden Layer Activation Function\nThe last piece we need for our Forward Propagation is an Activation Function on the hidden layer. Remember: this Activation Function is feeding to our output layer, so we need something that generates ten prediction values. A good way to do this is to take our unactivated hidden layer and normalize it such that it outputs “prediction odds” to our output layer. Put another way, if the model is 91% sure an input is the number 5, it should activate the 5’s output node to 0.91, and the sum of all the other predictions should come to 0.09.\nWhat we’ve just described is called the Softmax function. The mast might look a little strange (here’s a good explainer), but all it’s doing is taking in a set of numbers and normalizing them to be 0 - 1, with a sum of 1.\n\ndef softmax(inputs):\n    exponentiated = np.exp(inputs)\n    probabilities = exponentiated / np.sum(exponentiated, axis=0)\n    return probabilities\n\n\n\nForward Propagation\nWe can now combine our initialized Weights and Biases and our Activation Functions to define Forward Propagation. You’ll notice some matrix math in here. Since our features (pixels) are arranged in a matrix, this is much more efficient that piping every pixel through functions one at a time. This becomes particularly true when you’re working with Neural Networks of higher dimensions. Here’s a refresher on matrix dot product. But you can basically think of it as multiplying the neurons in a layer by their weights.\n\ndef forward_prop(weights_1, biases_1, weights_2, biases_2, input_layer):\n    # First, calculate the unactivated values of the neurons in the first hidden layer\n    # Dot product weights_1 on input_layer, and then add biases_1\n    unactivated_hidden_layer = weights_1.dot(input_layer) + biases_1\n    # Then apply the ReLU activation function, to get our first hidden layer\n    activated_hidden_layer = relu(unactivated_hidden_layer)\n\n    # Next calculate the unactivated values of the neurons in the output layer\n    # Dot product weights_2 on activated_hidden_layer, and then add biases_2\n    unactivated_output_layer = weights_2.dot(activated_hidden_layer) + biases_2\n    # Then apply the softmax activation function, to get our activated output layer\n    output_layer = softmax(unactivated_output_layer)\n\n    return unactivated_hidden_layer, activated_hidden_layer, unactivated_output_layer, output_layer\n\nThis is actually all we need to do to have our model make a prediction. If we pass in a matrix of images, it’ll give an output_layer of predicted labels. But remember, we haven’t actually trained it yet, so it’ll make very bad predictions. The two types of variables we have, the Weights and Biases, are set to random values. So this thing is going to perform with roughly 10% accuracy (randomly picking one of our ten digits). Let’s see that.\nWe can define a measure of accuracy by dividing the number of correct predictions (the labels with the highest value in output_layer) by total number of predictions.\n\ndef get_predictions(output_layer):\n    return np.argmax(output_layer, 0)  # return the index of the max value prediction\n\n\ndef get_accuracy(predictions, labels):\n    return np.sum(predictions == labels) / labels.size\n\nHere’s what we get:\n\n# Set up our Neural Network shape\nimage_size = training_image_data.shape[0]\nnum_labels = len(np.unique(training_labels))\nhidden_layer_size = num_labels\n\n# Initialize our Weights and Biases\nweights_1, biases_1, weights_2, biases_2 = init_params(image_size, hidden_layer_size, num_labels)\n\n( \n    unactivated_hidden_layer,\n    activated_hidden_layer,\n    unactivated_output_layer,\n    output_layer,\n) = forward_prop(\n    weights_1,\n    biases_1,\n    weights_2,\n    biases_2,\n    training_image_data,\n)\n\npredictions = get_predictions(output_layer)\naccuracy = get_accuracy(predictions, training_labels)\n\nprint(f'Accuracy: {accuracy*100:.2f}%')\n\nAccuracy: 8.22%\n\n\n\n\n\nTraining the Neural Network\nNow that we have a stupid Neural Network, let’s train it so it can become a smart Neural Network. As mentioned before, Back Propagation is the proces in which we take the predictions from Forward Propagation, measure the error, and adjust our Weights and Biases through Gradient Descent.\nBefore we begin, we need one more helper function. We want to compare our known labels to all our predictions, and we can do that with a matrix with values of 1 (100%) at the correct index and 0 (0%) at all the incorrect indexes. We do this with One-hot encoding.\n\ndef one_hot(num_labels, labels):\n    # Create a matrix of all zeroes with shape num_images x num_labels\n    one_hot_y = np.zeros((labels.size, num_labels))\n    # For every image row, set Y'th (actual label) column to 1\n    # So, if the first row has label 5, set the 5th column to 1\n    one_hot_y[np.arange(labels.size), labels] = 1\n    # Return the Transposed matrix, so in the above example, the 5th row first column is 1\n    return one_hot_y.T\n\n\none_hot_output = one_hot(num_labels, training_labels)\n\n\nBack Propagation\nThis is where we actually do our Gradient Descent. It contains a little bit of calculus, and I highly adivse you pause here and watch these three videos: * What is backpropagation really doing? * Backpropagation Calculus * Backpropagation for Softmax and Multi-Class Classification | Complete Mathematical Derivation\nIt’s actually not all that imporant to master the calculus in these videos, but it’s very helpful to see what math is happening. From a high-level: we take the derivatives of our activation functions, and we reverse our matrix multiplication. It’s actually not too daunting. In the end, it gives is a measurement of error, and we use that error to adjust our Weights and Biases.\nSpeaking of derivatives, we’re gonna need the derivated of our ReLU Activation Function:\n\n# rectified_linear_unit only ever has a slope of 0 (negative X), or 1 (positive X)\ndef derivative_rectified_linear_unit(Z):\n    return Z &gt; 0\n\nAnd now we just go backwards through the Neural Network:\n\ndef back_prop(unactivated_hidden_layer, activated_hidden_layer, weights_2, output_layer, one_hot_output, training_image_data):\n    num_images = one_hot_output.size\n\n    # The difference between the predictions and the actual labels\n    d_predictions = output_layer - one_hot_output\n\n    # Take that d_predictions and cary back to the activated hidden layer\n    # And then average over all the neurons\n    d_weights_2 = (1 / num_images) * d_predictions.dot(activated_hidden_layer.T)\n\n    # Derivative of biases_ 2 is acutally just an average of the abosolute error\n    d_biases_2 = (1 / num_images) * np.sum(d_predictions)\n\n    # Derivative of unactived hidden layer\n    # Transpose the 2nd layer weights and dot on the known d_predictions\n    # And multiply that by the derivative of the activation function\n    d_hidden_layer = weights_2.T.dot(d_predictions) * derivative_rectified_linear_unit(unactivated_hidden_layer)\n\n    # Again, take our current derivative and cary it back to the input layer\n    # And average it over all the neurons\n    d_weights_1 = (1 / num_images) * d_hidden_layer.dot(training_image_data.T)\n    \n    # Derivative of biases_1 is like biases_2, but it's a simple average of the abolute error from the hiddle layer\n    d_biases_1 = (1 / num_images) * np.sum(d_hidden_layer)\n\n    return d_weights_1, d_biases_1, d_weights_2, d_biases_2\n\n\n\n\nTraining Our Model\n\nLearning Rate\nWe’re almost ready to train our model. The last piece here is to actually update our Weights and Biases after Back Propagation. You may be tempted to simply subracts the differences returned by back_prop but it’s not quite so simple. Remember, our Gradient Descent is good at telling us the slope at our current location, but it’s very unlikely to be a straight line. Instead, we’re going to want to take smaller steps so that we don’t over-shoot.\nWe need to introduce a variable called learning_rate, sometimes called alpha. This is a hyperparameter, which simply means that our model can’t learn the correct value through training; we have to set it ourselves. There are techniques to make this process more efficient, but usually it’s a bit of guess and check. If your model has a very bad time improving, you should try adjusting your learning rate. Common learning rates are between 0.01 and 1. If you’d like to read more about picking a learning rate, this is a good post.\nThrough some trial and error, I found that a learning rate of 0.5 worked pretty well for this model.\n\nlearning_rate = 0.5\n\nWe can use this to update our Weights and Biases:\n\ndef update_params(weights_1, biases_1, weights_2, biases_2, d_weights_1, d_biases_1, d_weights_2, d_biases_2, learning_rate):\n    weights_1 = weights_1 - learning_rate * d_weights_1\n    biases_1 = biases_1 - learning_rate * d_biases_1\n    weights_2 = weights_2 - learning_rate * d_weights_2\n    biases_2 = biases_2 - learning_rate * d_biases_2\n\n    return weights_1, biases_1, weights_2, biases_2\n\n\n\nTraining\nNow let’s train our model! You might recall that it’ll take many iterations of Forward Propagation and Back Propagation for our model to get smart. But how many iterations? I don’t actually know! Let’s decide right now that we want our model to be at least 90% accurate on the training data. We’ve already got a function to tell us accuracy, so let’s let it train until that reaches 90%.\nThis is the slow part, and the part that often benefits from GPUs. But, as you’ll see, this Neural Network trains surpisingly quickly! On my laptop with no GPU and a 2.6 GHz 6-Core Intel Core i7, the model rechease 90% accuracy on the training data in under two minutes!\n\nimage_size = training_image_data.shape[0]\nnum_labels = len(np.unique(training_labels))\nhidden_layer_size = num_labels\n\nweights_1, biases_1, weights_2, biases_2 = init_params(image_size, hidden_layer_size, num_labels)\n\naccuracy = 0\niteration = 0\naccuracy_measurements = [0]  # A running list of our accuracy for each iteration\n\nwhile accuracy &lt; 0.9:\n    iteration += 1\n\n    ( \n        unactivated_hidden_layer,\n        activated_hidden_layer,\n        unactivated_output_layer,\n        output_layer,\n    ) = forward_prop(\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n        training_image_data,\n    )\n\n    one_hot_output = one_hot(num_labels, training_labels)\n\n    (\n        d_weights_1,\n        d_biases_1,\n        d_weights_2,\n        d_biases_2,\n    ) = back_prop(\n        unactivated_hidden_layer,\n        activated_hidden_layer,\n        weights_2, output_layer,\n        one_hot_output,\n        training_image_data,\n    )\n\n    (\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n    ) = update_params(\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n        d_weights_1,\n        d_biases_1,\n        d_weights_2,\n        d_biases_2,\n        learning_rate,\n    )\n\n    predictions = get_predictions(output_layer)\n    accuracy = get_accuracy(predictions, training_labels)\n\n    accuracy_measurements.append(accuracy)\n\n    # Print out upadtes as we go\n    if iteration == 1 or iteration % 100 == 0:  \n        print(f'Iteration: {iteration} | Accuracy: {accuracy*100:.2f}%')\n\nprint(f'Achieved {accuracy*100:.2f}% accuracy after {iteration} iterations!')\n\n# Plot the accuracy curve\nplt.plot(accuracy_measurements)\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.show()\n\nIteration: 1 | Accuracy: 9.41%\nIteration: 100 | Accuracy: 72.68%\nIteration: 200 | Accuracy: 82.50%\nIteration: 300 | Accuracy: 85.56%\nIteration: 400 | Accuracy: 87.01%\nIteration: 500 | Accuracy: 87.85%\nIteration: 600 | Accuracy: 88.51%\nIteration: 700 | Accuracy: 88.97%\nIteration: 800 | Accuracy: 89.37%\nIteration: 900 | Accuracy: 89.66%\nIteration: 1000 | Accuracy: 89.90%\nAchieved 90.00% accuracy after 1037 iterations!\n\n\n\n\n\n\n\n\n\nNow we have the Weights and Biases for our Neural Network to achieve 90% accuracy against our training data. That’s pretty cool, but the real test to to run images through the model that it’s never seen before. If we overfit the model, it might only be good at recognizing the images it was trained on (kindof like cramming for a test and memorizing the answers without actually learning anything). Let’s run it against our 10,000 test images. At this point we only need to run it through forward_prop because we’re no longer training. This is very fast!\n\ntest_image_data, test_labels = load_images_and_labels(TEST_IMAGE_FILE_PATH, TEST_LABEL_FILE_PATH)\n_, _, _, test_output_layer = forward_prop(\n        weights_1,\n        biases_1,\n        weights_2,\n        biases_2,\n        test_image_data,\n    )\ntest_predictions = get_predictions(test_output_layer)\ntest_accuracy = get_accuracy(test_predictions, test_labels)\nprint(f'Test Data Set Accuracy: {test_accuracy*100:.2f}%')\n\nTest Data Set Accuracy: 90.32%\n\n\nThat’s great! And let’s look at a sample of 10 test images and how they were labeled:\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n\nfor ax in axes.flat:\n    random_index = np.random.randint(0, len(test_labels))\n    random_test_image_data = test_image_data[:, random_index].reshape((28, 28))\n    random_test_image_label = test_labels[random_index]\n    random_prediction = test_predictions[random_index]\n\n    ax.imshow(random_test_image_data, cmap='gray_r', vmin=0, vmax=1)\n    ax.set(title=f'Actual: {random_test_image_label}\\nPredicted: {random_prediction}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nVery impressive!\nWe’ve successfully built a computer vision Machine Learning model using nothing but NumPy. Hopefully this demystified some of the some of the concepts of Neural Networks!\n\n\n\nAcknowledgements\nMy first pass at tackling this topic began with Samson Zhang’s “Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)”. This post looks a lot like his work, though I hit a number of implementation errors while following along. After digging into the articles and videos linked in this post, I made a number of adjustments to the model’s training to get to this final output.\n\n\nLicense\nCopyright (c) 2023, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of Building a Neural Network From Scratch with NumPy nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/2024-12-26-austin-hot-or-not-part-2/index.html",
    "href": "posts/2024-12-26-austin-hot-or-not-part-2/index.html",
    "title": "Part 2: Austin Texas, Hot or Not?",
    "section": "",
    "text": "Over the Summer I wondered whether Austin was having a cooler season than normal. Through the magic of public weather data, I was able to discover that the answer was mostly yes! But I also wrote that post at the end of July, and a lot of 2024 was still left to go. So here I am, sitting at my kitchen counter the day after Christmas, ready to definitively answer the question for the year: Austin, hot or not? Code for how all of this (mostly) works is in the original post, so this update is relatively brief with only the graphs.\nFirst, I wanted to quickly glance at 2023 vs 2024. For a little bit there, this Summer was cooler than last Summer. But then we had a pretty hot week in late August. Year over year, the Summer looks like it was generally cooler than last year, but the Fall has been slightly warmer:\n\n\n\n\n\n\n\n\n\nBut comparing two years doesn’t really reveal a trend (or lack of trend) so here are 2023 and 2024 highlighted over the previous ten years. It’s kindof a mess, like if Hulk Hogan couldn’t color within the lines. But if you squint you can somewhat pick out some insights.\n\n\n\n\n\n\n\n\n\nSo here’s the final viz I landed on. For most of the Winter and Spring, Austin was oscillating around its 10-year trailing average maximum daily temperature. Then as Summer hit, max temperatures dipped a little bit below average, spiked up in mid August, and then fell again slightly below average until September. From there we’ve been mostly above average!\n\n\n\n\n\n\n\n\n\nAs a final glimpse, we can see how many 2024 days (through December 23, my last day of available data) have crossed above the average max temperature, and how many days stayed below:\n\n\nAbove Average: 219\nBelow Average: 137\n\n\nThermometers!"
  },
  {
    "objectID": "posts/2023-11-01-language-models-ai-executive-order/index.html",
    "href": "posts/2023-11-01-language-models-ai-executive-order/index.html",
    "title": "Language Models on the AI Executive Order",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nOn October 30th, 2023, President Biden signed the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. The order itself is quite sweeping and touches many government departments and agencies, with a focus on harnessing AI’s potential and defending against harms and risks.\nIn this post, we’ll deploy language models to rapidly discover information from the Order. For the easiest setup, I recommend trying this out in a Google Colab notebook.\n     \nMany of the strategies presented here are extensions from Simon Willison’s work in his blog post, Embedding paragraphs from my blog with E5-large-v2. Simon also maintains a handy command line utility for working with various LLM models, aptly named LLM. While Simon’s writing largely focuses on the CLI capabilities of the tool (and the usefully opinionated integrations with SQLite), I prefer working with Pandas Dataframes. Here I show how to use the LLM library in that fashion.\nEmbeddings are kindof a magic black box to end users, but the basic idea is that language models can create vectors or numerical values that represent not only words or sentences, but also the symantic meaning of those words. Early research on this subject comes from word2vec. To illustrate: vector('king') - vector('man') + vector('woman') is mathematically close to vector('queen'). I find that fascinating! We’ll use this concept to extract and match information against the Executive Order text.\nWe’ll deploy a technique known as Retrieval-Augmented Generation. From a high level, this allows us to inject context into a LLM without training or tuning it. We use another system to locate language that likely contains the answer to our query, and then ask the model to pull it out for us.\nOur high livel strategy:\n\nCalculate embeddings on the Executive Order text\nCalculate embeddings on a query\nCalculate the cosine similarity between every text embedding and the query\nSelect the top three passages that are symantically similar to the query\nPass the passages and the query to the LLM for rapid summarization\n\n\nEnvironment\nFirst install the dependencies, which include the MLC LLaMA 2 model for summarization, the LLM library, and the E5-large-v2 language model for text embedding.\nNote, these models are constantly changing, and getting them up and running on your system might take some independent investigation. If running in Google Colab, check this tutorial for MLC. If running LLaMA with the LLM library on macOS, check the repository’s instructions.\n\npip install --pre -U -f https://mlc.ai/wheels mlc-chat-nightly-cu118 mlc-ai-nightly-cu118\ngit lfs install\npip install llm\nllm install llm-sentence-transformers\nllm sentence-transformers register intfloat/e5-large-v2 -a lv2\nllm install llm-mlc\nllm mlc setup\nllm mlc download-model Llama-2-7b-chat --alias llama2\n\n\n\nLoad Data\nBefore getting started, we need the Executive Order text to work against. This is probably the least interesting part of this post. I simply opened the Order in Firefox reader view, copy+pasted it into VSCode, did some manual find/replace to clean up the white space, and then concatenated paragraphs to get chunks as close to 400 words as I could. I picked 400 because the embedding model truncates at 512 tokens and a token is either a word or a symantically important subset of a word, so I allowed for some buffer. This took less than half an hour. Rather than share code to do this work, I simply provide the cleaned text here.\nLoad it into a Pandas Dataframe with a single column:\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/hodgesmr/llm_ai_eo/main/eo.txt\",\n    sep=\"_\",  # trick to let us read the lines into a Dataframe; '_' not present\n    header=None,\n)\ndf.columns = [\"passage\"]\n\ndf.head()\n\n\n    \n\n\n\n\n\n\npassage\n\n\n\n\n0\nBy the authority vested in me as President by ...\n\n\n1\n(a) Artificial Intelligence must be safe and s...\n\n\n2\n(c) The responsible development and use of AI ...\n\n\n3\n(e) The interests of Americans who increasingl...\n\n\n4\n(g) It is important to manage the risks from t...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nCalculate Embeddings\nNow that we have a Dataframe of chunks of the Executive Order, we can calculate embeddings of each chunk. To do this we’ll use the E5-large-v2 language model, which was trained to handle text prefixed with either passage: or query:. Every chunk is considered a passage. We’ll add this as another column on our Dataframe.\n\nimport llm\n\nembedding_model = llm.get_embedding_model(\"lv2\")\ntext_to_embed = df.passage.to_list()\n\n# Our embedding model expects `passage: ` prefixes\ntext_to_embed = [f'passage: {t}' for t in text_to_embed]\n\ndf['embedding'] = list(embedding_model.embed_multi(text_to_embed))\n\ndf.head()\n\n\n    \n\n\n\n\n\n\npassage\nembedding\n\n\n\n\n0\nBy the authority vested in me as President by ...\n[0.032344698905944824, -0.04333016648888588, 0...\n\n\n1\n(a) Artificial Intelligence must be safe and s...\n[0.01886950619518757, -0.057347141206264496, 0...\n\n\n2\n(c) The responsible development and use of AI ...\n[0.0486459881067276, -0.0712570995092392, 0.02...\n\n\n3\n(e) The interests of Americans who increasingl...\n[0.03564070537686348, -0.04887280985713005, 0....\n\n\n4\n(g) It is important to manage the risks from t...\n[0.04095401614904404, -0.042341429740190506, 0...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nFor our symantic searching, we’ll also need an embedding of our query. And the model would like that prefixed with query:. Let’s ask what the Order says regarding AI and healthcare:\n\nquery = \"what does it say about healthcare?\"\n\n# Our embbeding model expects `query: ` prefix for retrieval\nquery_to_embed = f\"query: {query}\"\nquery_vector = embedding_model.embed(query_to_embed)\n\nprint(query_vector)\n\n[0.011035123839974403, -0.06264020502567291, 0.036343760788440704, -0.022550513967871666, -0.004930663388222456, 0.027655886486172676, -0.04244294762611389, -0.026744479313492775, -0.022813718765974045, 0.013104002922773361, 0.027848346158862114, -0.041959188878536224, 0.02923852950334549, 0.03592933714389801, 0.02084210328757763, 0.028341282159090042, -0.02188134379684925, 0.009380431845784187, 0.010694948956370354, -0.046167585998773575, 0.04979575797915459, -0.04051537066698074, -0.04705166816711426, 0.054594166576862335, -0.021378282457590103, -0.006090054754167795, -0.027005767449736595, -0.0056915683671832085, -0.02485739439725876, 0.025049963966012, 0.0013038198230788112, 0.020098360255360603, 0.03132014721632004, -0.10214236378669739, 0.03457639366388321, -0.005869136657565832, -0.041733402758836746, -0.0533079169690609, 0.043018240481615067, 0.02142527513206005, -0.013251637108623981, 0.021434243768453598, -0.01846863329410553, 0.06185981631278992, -0.006901243235915899, -0.007515963166952133, -0.026446444913744926, -0.022192247211933136, 0.008555920794606209, -0.00683502247557044, 0.04217502102255821, -0.05847731605172157, 0.05262995511293411, 0.010208839550614357, -0.026769086718559265, -0.0054522184655070305, 0.020620698109269142, 0.04013380780816078, -0.016234276816248894, -0.013201197609305382, 0.033620886504650116, 0.00933841709047556, 0.057325609028339386, -0.039771128445863724, 0.024898990988731384, -0.04952569678425789, 0.01868751272559166, -0.009222128428518772, 0.028759649023413658, 0.04622277244925499, -0.04479452595114708, -0.007873269729316235, 0.003972833044826984, 0.08234458416700363, 0.047766849398612976, -0.04056665301322937, -0.03526405245065689, 0.028415270149707794, 0.03255781903862953, 0.03818396478891373, -0.04255992919206619, -0.0518893301486969, -0.036653585731983185, 0.020367737859487534, -0.0004731871304102242, 0.030738964676856995, -0.0012389217736199498, 0.0024446682073175907, 0.05554775893688202, 0.013511369936168194, 0.06238795816898346, 0.0018740486120805144, 0.008448319509625435, 0.03504892811179161, -0.04160211980342865, -0.00638029258698225, -0.050224918872117996, -0.005032839719206095, 0.013626458123326302, 0.008170071057975292, -0.04054887965321541, -0.027991971001029015, -0.034977927803993225, 0.014853181317448616, -0.024243168532848358, -0.02605707384645939, -0.05607284978032112, -0.00930757261812687, -0.03581421077251434, -0.0025592155288904905, -0.02390565536916256, -0.0380810908973217, -0.033349402248859406, -0.011960298754274845, -0.04205334186553955, 0.055994290858507156, 0.006477762944996357, 0.00991995818912983, 0.030702101066708565, 0.024755865335464478, 0.07455798983573914, 0.04026920348405838, -0.032499562948942184, -0.0020086164586246014, -0.035176586359739304, 0.0520380362868309, -0.04051459580659866, -0.04515765979886055, 0.04827379435300827, 0.0681162029504776, 0.01639464683830738, -0.012349777854979038, -0.024919848889112473, 0.0261624027043581, -0.0003906007332261652, 0.08279916644096375, -0.04583611711859703, -0.05605150759220123, 0.03352531045675278, 0.05521564930677414, -0.0490930899977684, -0.027380632236599922, 0.02854541875422001, 0.012674689292907715, -0.002931063063442707, -0.053501006215810776, -0.0288915503770113, -0.031300533562898636, -0.03822864219546318, -0.03784075006842613, -0.017202073708176613, -0.005835125222802162, -0.02155722677707672, -0.00019966973923146725, 0.038443099707365036, -0.055773064494132996, 0.0020749655086547136, -0.023329386487603188, -0.006293139886111021, 0.019844476133584976, 0.044987935572862625, 0.03083648905158043, -0.01008790172636509, 0.02570483647286892, 0.002781765768304467, 0.007234315853565931, 0.03924374282360077, -0.014665482565760612, -0.026686524972319603, 0.040999021381139755, 0.0032443120144307613, 0.0015701769152656198, 0.053893182426691055, 0.045284025371074677, 0.023676631972193718, 0.007213208358734846, 0.05706159397959709, -0.06629643589258194, 0.010098790749907494, -0.03431336581707001, 0.0032631983049213886, 0.021331751719117165, -0.05340828001499176, -0.022274604067206383, -0.0436270497739315, 0.019490502774715424, 0.04619332030415535, -0.03779701516032219, -0.004007418639957905, -0.03109969012439251, -0.028927365317940712, -0.01648799516260624, -0.04384511336684227, 0.02099577710032463, -0.02223723568022251, -0.03204162418842316, -0.022197725251317024, -0.011281806975603104, -0.015587741509079933, 0.041081756353378296, 0.044835954904556274, 0.005444942973554134, 0.004057107958942652, 0.0030030268244445324, 0.008095227181911469, 0.010580062866210938, 0.002692743204534054, 0.027165278792381287, -0.04451528936624527, 0.018396437168121338, -0.012467225082218647, 0.01907152123749256, 0.024384012445807457, -0.03359571844339371, 0.002541579073294997, 0.03602273389697075, 0.007620786316692829, 0.0045867254957556725, -0.004530118312686682, 0.039663612842559814, -0.07118961960077286, 0.027506954967975616, 0.010756077244877815, -0.0027250293642282486, 0.020495502278208733, 0.025394247844815254, -0.037594713270664215, 0.017433777451515198, -0.008695165626704693, -0.02986626699566841, 0.012270666658878326, 0.016099197790026665, -0.03702491894364357, -0.03618799149990082, -0.012862246483564377, -0.05321879684925079, 0.03497174754738808, -0.012480347417294979, 0.012074748054146767, 0.021339774131774902, 0.025768857449293137, 0.027273213490843773, 0.006413111928850412, -0.01951907016336918, -0.020737817510962486, 0.03631201386451721, 0.005693630315363407, 0.014690431766211987, 0.0400673933327198, 0.040662623941898346, 0.005502395331859589, 0.0173542071133852, -0.04771675169467926, 0.02629489079117775, -0.003811753587797284, -0.007251544389873743, 0.04416917636990547, -0.061770226806402206, -0.016220202669501305, 0.03719717636704445, -0.01617533341050148, -0.03763722628355026, -0.04766486957669258, -0.025192739441990852, -0.019239462912082672, -0.008094199001789093, -0.005068974103778601, -0.004565471783280373, 0.07414168119430542, -0.041430603712797165, 0.013045714236795902, 0.018361356109380722, 0.03342536836862564, -0.041664931923151016, -0.05250570923089981, -0.02396155521273613, -0.01865033246576786, -0.03515772148966789, 0.016364071518182755, -0.009918905794620514, 0.05359259247779846, -0.0008409120491705835, -0.003592758672311902, 0.030167188495397568, -0.008410172536969185, -0.002642557490617037, -0.026539165526628494, 0.003059686627238989, 0.029047558084130287, 0.01878088153898716, -0.04435497149825096, 0.004310435149818659, 0.05454620346426964, -0.009366977959871292, -0.009419695474207401, 0.031485412269830704, -0.016963956877589226, 0.007122103590518236, 0.009967380203306675, 0.024634771049022675, 0.022842811420559883, -0.0031100360210984945, -0.0643397867679596, -0.02673438936471939, -0.035047076642513275, -0.00703747384250164, 0.049209486693143845, -0.050232626497745514, 0.04699047654867172, 0.026202980428934097, -0.016680346801877022, -0.02222183719277382, -0.041290491819381714, -0.03719739243388176, -0.0399385504424572, 0.016396909952163696, -0.02527480199933052, 0.018787767738103867, 0.0006646675174124539, -0.01867998018860817, 0.03068104200065136, -0.0020506661385297775, -0.044911354780197144, -0.012430372647941113, -0.024320200085639954, 0.017804201692342758, -0.032045990228652954, -0.007124380674213171, 0.01598973572254181, 0.012343218550086021, -0.011404650285840034, -0.01943874917924404, 0.04149242863059044, -0.005655502434819937, -0.016777103766798973, -0.011175236664712429, -0.04135586693882942, -0.03618283197283745, 0.02853410318493843, 0.044518813490867615, 0.06200827658176422, 0.038675494492053986, -0.036812301725149155, -0.02377907745540142, 0.025207694619894028, 0.040055833756923676, 0.04191999137401581, 0.009881209582090378, 0.02629314549267292, 0.024176236242055893, 0.020148472860455513, 0.008412880823016167, -0.004142292309552431, -0.004587574861943722, -0.03939927741885185, -0.01107271108776331, 0.026553431525826454, -0.04704265668988228, 0.0134046645835042, 0.03654501587152481, 0.017579609528183937, -0.00949288159608841, 0.06082817539572716, -0.006728034000843763, 0.008356080390512943, -0.051711030304431915, 0.03844171389937401, -0.01830201968550682, 0.00854733120650053, 0.034922968596220016, 0.004259245935827494, 0.020917760208249092, 0.010708301328122616, -0.02309831604361534, -0.04656030982732773, -0.04652083292603493, 0.05957057699561119, 0.02332383207976818, 0.027773132547736168, -0.02072453871369362, 0.01621401123702526, 0.01397494412958622, -0.018857672810554504, -0.027436040341854095, -0.002191656269133091, -0.004994520451873541, -0.004126602318137884, -0.017894916236400604, -0.01036886591464281, -0.04088125005364418, -0.001628147903829813, -0.038663312792778015, 0.018833275884389877, -0.030392177402973175, 0.008629074320197105, -0.028720470145344734, 0.02279716171324253, -0.01806335523724556, 0.008062546141445637, 0.010905652306973934, 0.03126005455851555, 0.04747697710990906, 0.04772476851940155, 0.0025600406806916, -0.027565544471144676, -0.02668774127960205, 0.04895208775997162, 0.035160355269908905, -0.003617622423917055, -0.024726560339331627, 0.0027959730941802263, 0.032439906150102615, -0.02167009562253952, 0.018054930493235588, -0.01872582919895649, 0.0019295919919386506, -0.02558404952287674, 0.006836448796093464, -0.0369705893099308, -0.02392488345503807, -0.0006662389496341348, -0.03071492351591587, -0.035662174224853516, -0.027658283710479736, 0.03889033943414688, -0.0037014412228018045, 0.026146110147237778, -0.012518925592303276, -0.020388586446642876, -0.05085292086005211, 0.04047977924346924, -0.04683171212673187, -0.025453921407461166, -0.022075410932302475, 0.04574955627322197, -0.04705757275223732, 0.014057472348213196, -0.0019396235002204776, 0.02185961976647377, -0.0364665687084198, 0.029328808188438416, 0.03975827246904373, -0.025506163015961647, -0.048030223697423935, 0.020831944420933723, -0.010402142070233822, -0.045267313718795776, -0.009884222410619259, -0.03742368519306183, 0.040443453937768936, 0.03946416452527046, -0.01560758426785469, 0.05435897037386894, -0.01847420074045658, -0.022371601313352585, -0.02834167331457138, -0.024244116619229317, 0.016461217775940895, -0.015590337105095387, -0.00129281438421458, -0.04336583614349365, 0.031417686492204666, 0.031635694205760956, -0.04004405438899994, -0.030732493847608566, -0.006123801227658987, 0.04235366731882095, 0.01829519309103489, -0.04180118814110756, 0.022241240367293358, 0.007255924865603447, -0.011486359871923923, 0.03379999101161957, -0.0004169400199316442, -0.0184877160936594, 0.028425395488739014, 0.028919873759150505, 0.02586987614631653, 0.024930693209171295, 0.014547272585332394, -0.026854408904910088, -0.024916205555200577, 0.021643545478582382, 0.020358487963676453, -0.06973747164011002, 0.03774351254105568, -0.057213641703128815, 0.019329674541950226, 0.058849673718214035, -0.01767851412296295, -0.04142051935195923, 0.029757970944046974, 0.03226760774850845, 0.013497359119355679, 0.04983055964112282, -0.06860023736953735, -0.009090565145015717, 0.01876700483262539, -0.045754898339509964, -0.018559377640485764, 0.03907673433423042, -0.04343864694237709, 0.01983906328678131, 0.05090088024735451, -0.026130903512239456, 0.0198112390935421, -0.01724187843501568, 0.027581052854657173, -0.015150460414588451, 0.044671617448329926, -0.021948102861642838, -0.039155758917331696, -0.011670168489217758, 0.057173795998096466, 0.05130016431212425, 0.015086568892002106, 0.029861880466341972, 0.00045294041046872735, 0.012837855145335197, -0.013540389016270638, -0.0035288180224597454, -0.006696672644466162, 0.005161927081644535, -0.007997330278158188, -0.06018849462270737, 0.0040719653479754925, 0.04676881060004234, 0.05494493618607521, -0.02383231744170189, -0.02409941330552101, -0.02361765317618847, 0.05019611492753029, -0.02561134286224842, -0.011039866134524345, -0.0035623686853796244, -0.007303849793970585, -0.05598662421107292, -0.014519513584673405, -0.016709184274077415, 0.023226818069815636, 0.0020540591794997454, 0.018423371016979218, 0.07162746787071228, -0.030987678095698357, -0.011564436368644238, -0.05398257449269295, -0.03393613547086716, 0.029724063351750374, 0.014501385390758514, 0.01975683681666851, 0.025587748736143112, -0.0031189853325486183, -0.028188880532979965, -0.028325166553258896, -0.058148093521595, 0.06817413866519928, -0.05185883492231369, 0.010479864664375782, 0.013737188652157784, -0.025603286921977997, 0.004288510885089636, 0.03330473229289055, 0.03081020526587963, 0.00929239485412836, 0.013587307184934616, 0.04477598890662193, -0.010759776458144188, -0.013349821791052818, -0.03450658544898033, 0.04914005845785141, -0.008455680683255196, -0.02566939778625965, 0.06540139764547348, -0.014167072251439095, -0.04206572845578194, 0.040725916624069214, -0.027056824415922165, -0.027743373066186905, 0.01723734475672245, -0.026086552068591118, 0.011066379956901073, -0.028094105422496796, 0.021090257912874222, -0.006315530743449926, -0.0038536370266228914, 0.002642259933054447, -0.027106409892439842, -0.04236611723899841, 0.034201622009277344, 0.02585034817457199, 0.029318731278181076, 0.04393518716096878, -0.021400194615125656, -0.01966845989227295, 0.037550002336502075, -0.01055433414876461, 0.013854894787073135, -0.02539399452507496, -0.029923539608716965, 0.011284036561846733, 0.03166112303733826, -0.05249398201704025, 0.023592980578541756, -0.032214708626270294, 0.048454105854034424, -0.03129222244024277, 0.019876103848218918, 0.027158288285136223, -0.010018077678978443, -0.015372109599411488, -0.03651623800396919, 0.030494078993797302, 0.030631577596068382, 0.013395410031080246, -0.04203599691390991, 0.017429262399673462, -0.0336260125041008, -0.00458034360781312, 0.04320629686117172, 0.04873400554060936, -0.036522023379802704, -0.007033052854239941, 0.04841015860438347, 0.04686618968844414, -0.0226470734924078, -0.0156414732336998, 0.03650740534067154, -0.02229906991124153, 0.04247375950217247, 0.02045833133161068, 0.0011794024612754583, -0.02260892651975155, -0.034504398703575134, 0.044768236577510834, -0.031263917684555054, -0.045521024614572525, 0.005588601343333721, -0.056517187505960464, 0.03272703289985657, 0.03745061159133911, 0.048094216734170914, 0.04812173917889595, -0.051041409373283386, 0.017124954611063004, -0.028181584551930428, 0.0380525179207325, -0.0037846474442631006, -0.03125273063778877, 0.01894977129995823, -0.040786415338516235, -0.04169308394193649, 0.05256915092468262, -0.03413494676351547, 0.02746882289648056, 0.033805206418037415, 0.007925352081656456, 0.02444133721292019, 0.018344828858971596, -0.009217273443937302, 0.0021543705370277166, 0.03575904294848442, 0.020160242915153503, 0.02286422997713089, -0.020686518400907516, 0.026341790333390236, 0.12740176916122437, -0.00893761869519949, 0.020138844847679138, -0.02238314412534237, 0.018193142488598824, 0.03919895738363266, -0.022538235411047935, 0.025507012382149696, -0.023426273837685585, 0.027672510594129562, 0.04479461535811424, 0.051694680005311966, 0.024090928956866264, 0.05755620077252388, -0.014703532680869102, 0.017124010249972343, -0.01627686806023121, -0.0018241779180243611, -0.02017812617123127, -0.013297722674906254, 0.02810887061059475, -0.03654468432068825, -0.040724869817495346, -0.02429257705807686, 0.02536279335618019, -0.004012022167444229, -0.0012858447153121233, -0.04617433249950409, 0.017472991719841957, 0.05619405210018158, 0.003411441808566451, 0.02130633033812046, -0.03523368015885353, -0.017101695761084557, -0.020463088527321815, -0.025893187150359154, -0.04365244880318642, 0.03732496127486229, 0.003834214061498642, -0.02485889010131359, 0.023965951055288315, 0.04507003724575043, -0.039340391755104065, -0.01395449135452509, 0.043282754719257355, -0.0028774579986929893, 0.05358149856328964, -0.02236671932041645, 0.005119263660162687, 0.04382428526878357, 0.013711007311940193, -0.01928921416401863, -0.04386340454220772, 0.013973870314657688, -0.01751251146197319, -0.021039115265011787, 0.018643466755747795, 0.021641723811626434, -0.019717521965503693, -0.04447349160909653, -0.045124512165784836, 0.001166265457868576, 0.027790851891040802, 0.0349087193608284, -0.05426891893148422, -0.020197005942463875, 0.013967194594442844, 0.020221564918756485, 0.02676599659025669, -0.05146752670407295, 0.028382916003465652, -0.03574734181165695, -0.0307468231767416, 0.0271960087120533, -0.060862455517053604, -0.019536489620804787, 0.029809515923261642, 0.005997729953378439, -0.003236045129597187, -0.02296912670135498, -0.012584153562784195, 0.020569052547216415, -0.0398726686835289, 0.012893066741526127, 0.05747450888156891, -0.04004427418112755, -0.0016912904102355242, 0.03588302433490753, -0.040752533823251724, -0.021931834518909454, -0.03274866193532944, 0.02146611548960209, -0.01936306245625019, 0.017079008743166924, -0.007505142129957676, 0.06527362018823624, -0.020604269579052925, 0.01886555179953575, -0.00841802079230547, 0.03616372495889664, 0.020610619336366653, 0.04090982303023338, -0.00286914873868227, 0.01186350453644991, -0.015292976051568985, -0.03638255596160889, 0.028694557026028633, 0.002353183226659894, 0.017770130187273026, 0.01283283717930317, 0.012778712436556816, 0.02851232700049877, 0.008139816112816334, 0.03719107434153557, -0.03728439658880234, 0.012673329561948776, 0.03146594390273094, -0.019588639959692955, 0.006983111146837473, 0.03652822971343994, 0.01976114511489868, 0.043489858508110046, 0.04203694313764572, -0.0072769043035805225, -0.031777746975421906, 0.025811728090047836, -0.02263280190527439, -0.024895695969462395, -0.024923745542764664, -0.021317312493920326, 0.025197185575962067, -0.031068982556462288, -0.03568267449736595, -0.014184455387294292, -0.018119284883141518, 0.04019543156027794, 0.027073360979557037, 0.019060516729950905, -0.032629843801259995, 0.01988561637699604, -0.046449366956949234, -0.026410577818751335, -0.03311290591955185, -0.04030410945415497, 0.0073549640364944935, 0.03774447366595268, -0.02418743632733822, 0.03917284309864044, 0.037112291902303696, 0.04895860329270363, -0.03377828747034073, -0.021235689520835876, -0.008727184496819973, 0.012683587148785591, 0.025314902886748314, 0.02410356141626835, 0.06801478564739227, -0.02123037353157997, -0.0036285393871366978, 0.043283384293317795, 0.025348279625177383, 0.03189971670508385, -0.04675361514091492, -0.008785491809248924, -0.01910555362701416, -0.03074111044406891, -0.032706547528505325, -0.02195163257420063, -0.025955626741051674, -0.013656404800713062, -0.0017331260023638606, 0.04122364893555641, -0.003353940322995186, 0.003944667987525463, 0.02557617612183094, 0.033677756786346436, -0.0069642821326851845, -0.037354182451963425, -0.003706785151734948, 0.018041592091321945, 0.023099323734641075, 0.005993315484374762, -0.023070724681019783, -0.003592608030885458, 0.01580343022942543, -0.021968074142932892, -0.055538859218358994, 0.022645818069577217, 0.008289964869618416, 0.007825851440429688, 0.029414691030979156, 0.00884107407182455, -0.029503418132662773, -0.00685040233656764, -0.04297958314418793, -0.0067012435756623745, 0.0408974252641201, 0.03618587553501129, 0.008660215884447098, 0.005283386912196875, 0.02173357643187046, 0.016801103949546814, 0.014148939400911331, 0.04589837044477463, -0.07519549131393433, 0.0008492959314025939, -0.03911217674612999, 0.01957186684012413, 0.04065250977873802, -0.017508283257484436, -0.022696377709507942, -0.023634128272533417, 0.023389438167214394, 0.005290127359330654, 0.030684763565659523, 0.027884958311915398, 0.0007517277263104916, -0.01770036853849888, 0.011700128205120564, -0.010737712495028973, 0.04028428718447685, 0.023366102948784828, -0.03046400658786297, -0.023285800591111183, -0.047068167477846146, -0.019877061247825623, 0.005805285647511482, 0.026605457067489624, 0.014891230501234531, -0.04306803271174431, 0.06144380196928978, 0.02467053383588791, 0.009495215490460396, 0.02271324396133423, 0.022022845223546028, -0.0344187393784523, 0.034510448575019836, 0.035804156213998795, -0.02319440431892872, -0.012169327586889267, 0.027845632284879684, -0.03938868269324303, 0.014688534662127495, -0.0326634906232357, 0.035405971109867096, 0.09329289197921753, -0.021386025473475456, -0.005334523506462574, -0.013434350490570068, 0.03625260666012764, -0.035231638699769974, 0.01714746095240116, -0.010116915218532085, -0.015791669487953186, 0.03326767310500145, 0.06997721642255783, 0.02518271654844284, -0.025026991963386536, -0.033969491720199585, 0.006438582669943571, -0.010330992750823498, 0.022015077993273735, 0.035159625113010406, 0.03975333645939827, -0.09261862188577652, -0.030692344531416893, -0.007167263887822628, 0.030045578256249428, 0.04790728539228439, 0.037478335201740265, 0.008102349936962128, 0.023854972794651985, -0.02429278939962387, -0.007957016117870808, -0.029902758076786995, -0.03795326128602028, 0.02096726931631565, -0.009724924340844154, 0.05335197597742081, -0.031247764825820923, -0.029010239988565445, 0.03935291990637779, 0.0004320724692661315, -0.015092982910573483, -0.005163619294762611, 0.0006704957922920585, 0.019765393808484077, -0.0023283318150788546, 0.008752129971981049, 0.011935974471271038, 0.041026521474123, -0.02911226823925972, -0.00707174651324749, -0.05678257718682289, 0.018372097983956337, -0.007054861634969711, -0.03395901620388031, 0.029341937974095345, 0.011115530505776405, 0.04074866324663162, -0.015816841274499893, 0.05173950269818306, -0.015220806002616882, -0.04581461101770401, -0.03737398982048035, -0.0463782399892807, -0.04844944179058075, 0.011588484980165958, 0.007512776181101799, 0.012602098286151886, 0.008942355401813984, -0.019375091418623924, -0.00784570537507534, -0.016177771613001823, 0.020358964800834656, 0.030678516253829002, 0.058078765869140625, -0.05181894451379776, -0.007111021783202887, 0.004367280285805464, 0.04949195310473442, 0.024104589596390724, 0.013972863554954529, -0.022043360397219658, 0.02481302246451378, -0.02353677898645401, -0.019958026707172394, -0.03086337447166443, -0.0342184379696846, -0.042619749903678894, -0.013002551160752773, 0.031215941533446312, -0.029242197051644325, -0.039825692772865295, -0.04158396273851395, 0.019694391638040543, 0.02995566837489605, 0.028752531856298447, -0.02492348663508892, -0.006047347094863653, 0.04264743626117706, -0.006050477270036936, -0.012319333851337433, -0.02809244394302368, -0.04274909943342209, -0.015643877908587456, 0.0013809194788336754, 0.022640932351350784, -0.03112141042947769, 0.029752563685178757, -0.013332386501133442, 0.022779937833547592, 0.026944121345877647, 0.0026380603667348623, -0.04207104071974754, 0.022562168538570404, -0.01429622434079647, 0.044384222477674484, 0.004642078652977943]\n\n\n\n\nSymantic Search\nIf we were using the LLM module’s preferred structures for Collection and storing data in SQLite, we could simply use llm similar or its corresponding Python API. As far as I can tell, the API doesn’t yet support other data structures of embeddings (like our Dataframe), so we’ll have to calculate cosine similarities ourselves. Lucky for us, we can borrow from Simon’s open source library:\n\ndef cosine_similarity(a, b):\n    dot_product = sum(x * y for x, y in zip(a, b))\n    magnitude_a = sum(x * x for x in a) ** 0.5\n    magnitude_b = sum(x * x for x in b) ** 0.5\n    return dot_product / (magnitude_a * magnitude_b)\n\nNow, iterate over every embedding in our Dataframe and calculate the similarity score against our query embedding vector:\n\ncomp_df = df.copy()\ncomp_df['similarity'] = comp_df.apply(\n    lambda row : cosine_similarity(\n        query_vector,\n        row.embedding,\n    ),\n    axis=1,\n)\n\ncomp_df.head()\n\n\n    \n\n\n\n\n\n\npassage\nembedding\nsimilarity\n\n\n\n\n0\nBy the authority vested in me as President by ...\n[0.032344698905944824, -0.04333016648888588, 0...\n0.781552\n\n\n1\n(a) Artificial Intelligence must be safe and s...\n[0.01886950619518757, -0.057347141206264496, 0...\n0.778486\n\n\n2\n(c) The responsible development and use of AI ...\n[0.0486459881067276, -0.0712570995092392, 0.02...\n0.779455\n\n\n3\n(e) The interests of Americans who increasingl...\n[0.03564070537686348, -0.04887280985713005, 0....\n0.794971\n\n\n4\n(g) It is important to manage the risks from t...\n[0.04095401614904404, -0.042341429740190506, 0...\n0.785406\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAnd select the 3 passages with the best similary scores. We’ll feed this as context to the LLaMA model.\n\nbest_3_matches = comp_df.sort_values(\"similarity\", ascending = False).head(3)\ncontext = \"\\n\".join(best_3_matches.passage.values)\n\n\n\nAsk the LLM\nNow that we’ve selected the top 3 passages, let’s feed them into LLaMA 2.\n\nmodel = llm.get_model(\"llama2\")\n\nEven though we’re providing prefixed context to the model, it’s helpful to give it a system prompt to guide how it responds. This can help it stay “focussed” on the context and respond in the voice that we expect. The system prompt is open to creativity and experimentation.\n\nsystem = \"You are an assistant. You answer questions in a single \\\nparagraph about the policy from President Biden. The provided context \\\ncomes directly from the policy. You MUST use the provided information \\\nas context. Not all provided information will be helpful, ONLY reference \\\ninformation if it is related to my query. You may quote the context \\\ninformation if helpful.\"\n\nNow, feed the context and the query into the model.\n\nprint(f\"Query: {query}\\n\")\nresponse = model.prompt(\n    f'{context}\\n{query}',\n    system=system,\n)\n\nprint(f\"Response:\\n\")\nprint(response.text())\n\nQuery: what does it say about healthcare?\n\nResponse:\n\nThe policy from President Biden related to healthcare is outlined in section 8(b)(i) of the policy, which states that:\n\"Within 90 days of the date of this order, the Secretary of HHS shall, in consultation with the Secretary of Defense and the Secretary of Veterans Affairs, establish an HHS AI Task Force that shall, within 365 days of its creation, develop a strategic plan that includes policies and frameworks — possibly including regulatory action, as appropriate — on responsible deployment and use of AI and AI-enabled technologies in the health and human services sector (including research and discovery, drug and device safety, healthcare delivery and financing, and public health), and identify appropriate guidance and resources to promote that deployment, including in the following areas:\n(A) development, maintenance, and use of predictive and generative AI-enabled technologies in healthcare delivery and financing — including quality measurement, performance improvement, program integrity, benefits administration, and patient experience — taking into account considerations such as appropriate human oversight of the application of AI-generated output;\n(D) incorporation of safety, privacy, and security standards into the software-development lifecycle for protection of personally identifiable information, including measures to address AI-enhanced cybersecurity threats in the health and human services sector; (E) development, maintenance, and availability of documentation to help users determine appropriate and safe uses of AI in local settings in the health and human services sector; (F) work to be done with State, local, Tribal, and territorial health and human services agencies to advance positive use cases and best practices for use of AI in local settings; and (G) identification of uses of AI to promote workplace efficiency and satisfaction in the health and human services sector, including reducing administrative burdens.\"\nThis section outlines the responsibilities of the Secretary of HHS related to AI in the healthcare sector. Some of the key points include:\n* Establishing an HHS AI Task Force to develop a strategic plan for responsible AI deployment and use in the health and human services sector.\n* Identifying policies and frameworks for regulatory action, as appropriate, to ensure responsible deployment and use of AI in healthcare.\n* Developing guidance and resources to promote the appropriate and safe use of AI in healthcare\n\n\nOverall, this looks like it does a good job!\nOf course, it’s extremely important to keep a human in the loop when referencing government documents. The model may still hallucinate, or it could entirely miss important context. Some of these shortcoming are baked into the model itself, others are implementation details of this post.\nIf nothing else, this shows a fascinating interface to interact with long, wordy, documents!\n\n\nLicense\nCopyright (c) 2023, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of Language Models on the AI Executive Order nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "posts/2025-08-02-language-model-embeddings-campaign-donors/index.html",
    "href": "posts/2025-08-02-language-model-embeddings-campaign-donors/index.html",
    "title": "Using LLM Embeddings to Normalize User Data",
    "section": "",
    "text": "A common challenge in working with operational or CRM-style data is that you often find yourself dealing with user-entered free text. A recurrent example comes when users fill out forms that ask for their job title and employer. This data might seem secondary, but for any organization trying to understand, segment, or personalize communication with its users, it’s incredibly valuable.\nOf course, user-entered data is messy. One person types “nurse,” another “ER Nurse,” another “RN,” and yet another “home health nursing.” If you want to understand the composition of your user base, or build automated systems that adapt to it, you need to normalize that chaos into a finite and meaningful taxonomy. You wouldn’t want a dashboard full of job titles in SpongeBob casing, but realistically, you have to work with whatever comes through the form.\nA data-forward organization might use this information for all kinds of purposes: tailoring outreach, prioritizing leads, enriching analytics, or even customizing onboarding flows. But none of that works without being clean, consistent, and structured.\nWith language models, we can do better. We don’t need to predefine normalization rules or manually review each row. And we don’t even need much prior knowledge about our users to start.\nBut before we jump straight to the AI, let’s define our approach:\n\nTo normalize free-form text, there must be a finite set of target categories.\nIf we know nothing about users in advance, we need a reliable way to discover or define those categories.\nWe’re decidedly not using a chatbot, and we’re not relying on external APIs.\nThis isn’t a generative task; it’s about semantic understanding.\n\nLet’s start at the top. There are an infinite number of values a user could enter for their job, and we want to reduce that to a finite set. So where do we get that set?\nThe Occupational Information Network (O*NET) maintains exactly such a resource. Developed under the sponsorship of the Departoment of Labor, O*NET offers rich datasets that describe skills, knowlege, tasks, and job titles. We’re interested in the Alternate Titles file, which maps occupation titles to alternate “lay” job titles. There’s a good chance many of our users enter these alternate titles, so we’ll want to include them.\nThe file includes columns of Department of Labor and Census identifiers, but we only need the few that focus on title. Let’s download it and take a look at a few examples:\n\nimport numpy as np\nimport pandas as pd\n\nonet_df = pd.read_excel(\n    \"https://www.onetcenter.org/dl_files/database/db_29_3_excel/Alternate%20Titles.xlsx\",\n    usecols=['Title', 'Alternate Title', 'Short Title'],\n).fillna(\"\")\n\nonet_df.sample(n=5, random_state=101)  # seed for reproducibility\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\n\n\n\n\n25268\nCargo and Freight Agents\nShipping Agent\n\n\n\n30407\nHelpers, Construction Trades, All Other\nMaintenance Construction Helper\n\n\n\n6045\nBioengineers and Biomedical Engineers\nBiomedical Engineering Intern\n\n\n\n18684\nOccupational Therapy Aides\nRehabilitation Therapy Aide (Rehab Therapy Aide)\nRehab Therapy Aide\n\n\n24086\nBilling and Posting Clerks\nStatement Services Representative (Statement S...\nStatement Services Rep\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nSo O*NET tells us that Cargo and Freight Agents might also go by a Shipping Agent as an Alternate Title and that Occupational Therapy Aides might also go by a Rehab Therapy Aide as a Short Title.\nWe also see that there can be many rows of different Alternate Title and Short Title for the same Title:\n\nonet_df[onet_df[\"Title\"] == \"Software Developers\"]\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\n\n\n\n\n4931\nSoftware Developers\n.NET Developer\n\n\n\n4932\nSoftware Developers\nAndroid Developer\n\n\n\n4933\nSoftware Developers\nAngularJS Developer\n\n\n\n4934\nSoftware Developers\nApache Hadoop Developer\n\n\n\n4935\nSoftware Developers\nApplication Architect\n\n\n\n...\n...\n...\n...\n\n\n5061\nSoftware Developers\nUser Interface Designer\n\n\n\n5062\nSoftware Developers\nVideo Game Engineer\n\n\n\n5063\nSoftware Developers\nWide Area Network Engineer (WAN Engineer)\nWAN Engineer\n\n\n5064\nSoftware Developers\nWindows Software Engineer\n\n\n\n5065\nSoftware Developers\nXML Developer (Extensible Markup Language Deve...\nXML Developer\n\n\n\n\n135 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThere is one thing we do know ahead of time about our users: not all of them will be employed. The O*NET data set doesn’t provide a job title for not working, so let’s add our own:\n\nadditions = pd.DataFrame(\n    [\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"Not Employed\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"None\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"N/A\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"No Employment\"},\n        {\"Title\": \"Unemployed\", \"Alternate Title\": \"Not Working\"},\n        {\"Title\": \"Retired\", \"Alternate Title\": \"Retiree\"},\n    ]\n)\nonet_df = pd.concat([onet_df, additions], ignore_index=True).fillna(\"\")\nonet_df[(onet_df[\"Title\"] == \"Unemployed\") | (onet_df[\"Title\"] == \"Retired\")]\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\n\n\n\n\n56560\nUnemployed\nNot Employed\n\n\n\n56561\nUnemployed\nNone\n\n\n\n56562\nUnemployed\nN/A\n\n\n\n56563\nUnemployed\nNo Employment\n\n\n\n56564\nUnemployed\nNot Working\n\n\n\n56565\nRetired\nRetiree\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nNow let’s merge these fields together. Since we’ll be leveraging a language model, we can take the liberties of language here; we don’t need clean many-to-many relationships. Just combine Title, Alternate Title, and when available, Short Title into one Long Title field with \"aka\" inline:\n\nmask = onet_df[\"Short Title\"].eq(\"\")\n\nonet_df[\"Long Title\"] = np.where(\n    mask,\n    onet_df[\"Title\"] + \" aka \" + onet_df[\"Alternate Title\"],\n    onet_df[\"Title\"] + \" aka \" + onet_df[\"Alternate Title\"] + \" aka \" + onet_df[\"Short Title\"],\n)\n\nonet_df[onet_df[\"Title\"] == \"Software Developers\"]\n\n\n    \n\n\n\n\n\n\nTitle\nAlternate Title\nShort Title\nLong Title\n\n\n\n\n4931\nSoftware Developers\n.NET Developer\n\nSoftware Developers aka .NET Developer\n\n\n4932\nSoftware Developers\nAndroid Developer\n\nSoftware Developers aka Android Developer\n\n\n4933\nSoftware Developers\nAngularJS Developer\n\nSoftware Developers aka AngularJS Developer\n\n\n4934\nSoftware Developers\nApache Hadoop Developer\n\nSoftware Developers aka Apache Hadoop Developer\n\n\n4935\nSoftware Developers\nApplication Architect\n\nSoftware Developers aka Application Architect\n\n\n...\n...\n...\n...\n...\n\n\n5061\nSoftware Developers\nUser Interface Designer\n\nSoftware Developers aka User Interface Designer\n\n\n5062\nSoftware Developers\nVideo Game Engineer\n\nSoftware Developers aka Video Game Engineer\n\n\n5063\nSoftware Developers\nWide Area Network Engineer (WAN Engineer)\nWAN Engineer\nSoftware Developers aka Wide Area Network Engi...\n\n\n5064\nSoftware Developers\nWindows Software Engineer\n\nSoftware Developers aka Windows Software Engineer\n\n\n5065\nSoftware Developers\nXML Developer (Extensible Markup Language Deve...\nXML Developer\nSoftware Developers aka XML Developer (Extensi...\n\n\n\n\n135 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nGreat. We’ve satisfied parts one and two of our approach. We have a finite set of job titles, and we have a good understanding that the set is large but not exhaustive, and it combines multiple values for a job title. Let’s start modeling language.\nJobBERT-v2 is a sentence-transformers model fine tuned from all-mpnet-base-v2 specifically for job title matching and similarity. Hey that’s convenient!\nWe can’t use JobBERT out of the box, we’ll need to incorporate our O*NET dataset. Let’s pull it down and start building out our implementation of the model. To do this we’re going to leverage word embeddings against our Long Title values. If you’re unfamiliar with langage model embeddings, Simon Willison has a fantastic overview that you should go read now. But the gist of it is: embeddings are how language models numerically encode meaning from language into a large vector. This is suprisingly powerful, and yields operations like:\nemb('king') - emb('man') + emb('woman') which returns a vector that is mathmatically very close to emb('queen').\nWe’re going to use this “closeness” between vectors to reduce infinite free-form data to our finite Long Title data and then map it back to Title. The first thing to do is quite simple: calculate JobBERT embeddings on all of the values in our Long Title column:\n\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodel = SentenceTransformer(\"TechWolf/JobBERT-v2\")\n\nonet_df[\"embedding\"] = list(\n    model.encode(\n        onet_df[\"Long Title\"].tolist(),\n        normalize_embeddings=True,\n        convert_to_numpy=True,\n        show_progress_bar=False,\n    )\n)\n\nonet_df[[\"Long Title\", \"embedding\"]].sample(n=5, random_state=101)\n\n\n    \n\n\n\n\n\n\nLong Title\nembedding\n\n\n\n\n25268\nCargo and Freight Agents aka Shipping Agent\n[-0.034480397, -0.01120864, -0.005822623, -0.0...\n\n\n30408\nHelpers, Construction Trades, All Other aka Me...\n[-0.07022176, -0.020068161, 0.0111531215, -0.0...\n\n\n6045\nBioengineers and Biomedical Engineers aka Biom...\n[-0.030613927, -0.059320696, -0.01718829, -0.0...\n\n\n18684\nOccupational Therapy Aides aka Rehabilitation ...\n[-0.014956265, -0.038792193, -0.00349255, 0.00...\n\n\n24086\nBilling and Posting Clerks aka Statement Servi...\n[0.020981414, -0.033710796, 0.03225505, -0.011...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nAs a reader, the embedding column is an indecipherable array of floats, but now we can do some cool things. Here are three rows from our data:\n\nslice = onet_df[\n    (onet_df[\"Long Title\"] == \"Software Developers aka Video Game Engineer\") |\n    (onet_df[\"Long Title\"] == \"Database Architects aka Information Architect\") |\n    (onet_df[\"Long Title\"] == \"Cargo and Freight Agents aka Shipping Agent\")\n]\nslice[[\"Long Title\", \"embedding\"]]\n\n\n    \n\n\n\n\n\n\nLong Title\nembedding\n\n\n\n\n4806\nDatabase Architects aka Information Architect\n[0.020110216, 0.06301323, -0.029263753, -0.022...\n\n\n5062\nSoftware Developers aka Video Game Engineer\n[0.041696787, 0.024444718, -0.053837907, 0.031...\n\n\n25268\nCargo and Freight Agents aka Shipping Agent\n[-0.034480397, -0.01120864, -0.005822623, -0.0...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nIn a vector space, you can evaluate how similar two vectors are by taking their cosine similarity. Since we normalized our vectors when we embedded them, the denominator in the cosine function becomes 1 so we can do this even more efficiently with just a dot product:\n\ndatabase_architect = slice.iloc[0][\"embedding\"]\nsoftware_developer = slice.iloc[1][\"embedding\"]\ncargo_and_freight_agent = slice.iloc[2][\"embedding\"]\n\nprint(f\"Software Developer vs Data Architect: {software_developer @ database_architect}\")\nprint(f\"Software Developer vs Cargo Agent: {software_developer @ cargo_and_freight_agent}\")\n\nSoftware Developer vs Data Architect: 0.2380954772233963\nSoftware Developer vs Cargo Agent: 0.08600345253944397\n\n\nThose numbers look perfectly reasonable: a modest overlap (≈ 0.24) between two tech roles and an almost-orthogonal relationship (≈ 0.09) to the Cargo Agent job.\nGreat. So now we have a mathematical way to compare the language of two job titles. And we’re not touching chatbots or 3rd party APIs at inferrence. The DataFrame is a self-contained model for semantically matching across the O*NET dataset. We’ve fully satisfied our approach!\nNow to apply it to our problem. Instead of evaluating O*NET data against itself, we can use our embeddings to evaluate any free-form job title text a user might submit.\nLet’s go get some real data to try it out! Since I work in political tech, I like to reach for campaign donor data.\nOur friends over at ProPublica publish itemized ActBlue receipts by state. Because ActBlue is a conduit committee, these files include every transaction of any amount. That’s a lot of transactions! Let’s grab all of the ActBlue transactions from Texas for June 2024.\n\ndtypes = {\n    \"flag_orgind\": \"string\",\n    \"first_name\": \"string\",\n    \"city\": \"string\",\n    \"zip\": \"string\",\n    \"amount\": \"float64\",\n    \"occupation\": \"string\",\n}\n\nabtx_df = pd.read_csv(\n    \"https://pp-projects-static.s3.amazonaws.com/itemizer/sa_1791562_tx.csv\",\n    usecols=dtypes.keys(),\n    dtype=dtypes,\n)\nabtx_df = abtx_df[abtx_df[\"flag_orgind\"] == \"IND\"]\nabtx_df.drop(columns=[\"flag_orgind\"], inplace=True)\nabtx_df.dropna(inplace=True)\n\nemployed = lambda df: df[\"occupation\"].ne(\"NOT EMPLOYED\")\nget_sample = lambda seed: abtx_df.loc[employed(abtx_df)].sample(n=10, random_state=seed)\n\nget_sample(30330)  # seed for reproducibility\n\n\n    \n\n\n\n\n\n\nfirst_name\ncity\nzip\namount\noccupation\n\n\n\n\n139133\nPAT\nNORTH RICHLAND HIL\n76180\n9.0\nRETIRED\n\n\n4248\nHARRY\nHOUSTON\n77019\n125.0\nLAWYER\n\n\n221073\nTHAO\nHOUSTON\n77083\n2.5\nHISTOLOGY TECHNICIAN\n\n\n201669\nPHILIP\nSAN ANTONIO\n78240\n3.0\nMANAGER\n\n\n256487\nMARIA\nHOLLAND\n76534\n1.0\nHEALTHCARE ADMINISTRATOR\n\n\n6330\nELLEN\nBELLAIRE\n77401\n100.0\nARBITRATOR\n\n\n125976\nMICHELLE\nFORT WORTH\n76133\n10.0\nSERVER\n\n\n13659\nROSE\nHOUSTON\n77024\n75.0\nNURSING\n\n\n138213\nPETER\nHOUSTON\n77019\n9.0\nDATABASE ANALYST\n\n\n62939\nERIN\nFRISCO\n75035\n25.0\nNONPROFIT\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThat occupation field came from donors and doesn’t perfectly match our modeled job titles. But we don’t need it to! Let’s use our model to calculate embeddings on these new values:\n\nabtx_df[\"embedding\"] = list(\n    model.encode(\n        abtx_df[\"occupation\"].tolist(),\n        normalize_embeddings=True,\n        convert_to_numpy=True,\n        show_progress_bar=False,\n    )\n)\n\nget_sample(30330)\n\n\n    \n\n\n\n\n\n\nfirst_name\ncity\nzip\namount\noccupation\nembedding\n\n\n\n\n139133\nPAT\nNORTH RICHLAND HIL\n76180\n9.0\nRETIRED\n[-0.022304475, 0.08798518, 0.008374137, 0.0150...\n\n\n4248\nHARRY\nHOUSTON\n77019\n125.0\nLAWYER\n[0.0123939905, 0.06054912, 0.0046267705, -0.03...\n\n\n221073\nTHAO\nHOUSTON\n77083\n2.5\nHISTOLOGY TECHNICIAN\n[0.027493875, -0.079993084, 0.013278877, -0.00...\n\n\n201669\nPHILIP\nSAN ANTONIO\n78240\n3.0\nMANAGER\n[0.09953636, 0.07623968, 0.020005615, 0.001361...\n\n\n256487\nMARIA\nHOLLAND\n76534\n1.0\nHEALTHCARE ADMINISTRATOR\n[0.036386397, 0.06352263, -0.0023324555, -0.02...\n\n\n6330\nELLEN\nBELLAIRE\n77401\n100.0\nARBITRATOR\n[0.05744064, 0.03044543, -0.011071598, 0.01120...\n\n\n125976\nMICHELLE\nFORT WORTH\n76133\n10.0\nSERVER\n[0.0222358, 0.025343752, -0.027377797, -0.0031...\n\n\n13659\nROSE\nHOUSTON\n77024\n75.0\nNURSING\n[0.008113306, 0.03859657, -0.014793488, -0.064...\n\n\n138213\nPETER\nHOUSTON\n77019\n9.0\nDATABASE ANALYST\n[0.042439297, 0.08193651, -0.027909847, 0.0143...\n\n\n62939\nERIN\nFRISCO\n75035\n25.0\nNONPROFIT\n[-0.052989695, 0.19739158, 0.007059781, 0.0288...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nSo now we have two sets of embeddings: we have our O*NET embeddings and we have our ActBlue donor embeddings. Just as before, we can calculate similaries between them. But unlike before, we need to calculate a lot. In order to find the best match we need to compare every O*NET embedding vector with every ActBlue embedding vector. That’s a lot of comparisons. The good news, this is what GPUs are good at, and a free-tier GPU in Google Colab can kick this out fast.\nWe convert our O*NET embedding column into a (n × d) tensor, where n is the number of rows and d is the vector length. Similarly, we convert the ActBlue embedding column into a (m × d) tensor where m is the number of ActBlue rows.\nWhen pushing this to a GPU, it’s a little more art than science. We batch it, and picking an optimal batch size can take some trial and error. For every batch, we’ll calculate the dot product, and return the indices of the best similarities.\nFrom there, we can map all the way back to our original O*NET Title column, as our normalized output:\n\nimport torch\n\ndevice = \"cuda\"  # requires an NVIDIA GPU + CUDA\nonet_t = torch.tensor(np.stack(onet_df.embedding), device=device)  # (n × d)\nabtx_t = torch.tensor(np.stack(abtx_df.embedding), device=device)  # (m × d)\n\nbatch = 4096  # tune to fit GPU RAM\nbest = []\n\nwith torch.no_grad():\n    for s in range(0, abtx_t.size(0), batch):\n        sims = abtx_t[s:s+batch] @ onet_t.T  # (batch × n)\n        best.append(sims.argmax(dim=1).cpu())\n\nidx = torch.cat(best).numpy()\n\nabtx_df[\"Normalized Occupation\"] = onet_df.Title.iloc[idx].to_numpy()\n\nget_sample(30330)[[\"first_name\", \"occupation\", \"Normalized Occupation\"]]\n\n\n    \n\n\n\n\n\n\nfirst_name\noccupation\nNormalized Occupation\n\n\n\n\n139133\nPAT\nRETIRED\nRetired\n\n\n4248\nHARRY\nLAWYER\nLawyers\n\n\n221073\nTHAO\nHISTOLOGY TECHNICIAN\nHistology Technicians\n\n\n201669\nPHILIP\nMANAGER\nManagers, All Other\n\n\n256487\nMARIA\nHEALTHCARE ADMINISTRATOR\nMedical and Health Services Managers\n\n\n6330\nELLEN\nARBITRATOR\nArbitrators, Mediators, and Conciliators\n\n\n125976\nMICHELLE\nSERVER\nFood Servers, Nonrestaurant\n\n\n13659\nROSE\nNURSING\nRegistered Nurses\n\n\n138213\nPETER\nDATABASE ANALYST\nDatabase Administrators\n\n\n62939\nERIN\nNONPROFIT\nFundraisers\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nA way to visualized this is with principal component analysis. PCA computes new orthogonal axes called principal components that capture the most variation in the data. These directions are combinations of the original dimensions, chosen to reveal the biggest patterns and differences. By projecting each vector onto the first two principal components, we can plot everything in two dimensions while keeping as much of the original structure as possible:\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX_onet = np.vstack(onet_df[\"embedding\"].to_numpy())\nX_abtx = np.vstack(abtx_df[\"embedding\"].to_numpy())\nX_all  = np.vstack([X_onet, X_abtx])\n\npca = PCA(n_components=2)\nproj = pca.fit_transform(X_all)\ncoords_onet = proj[: len(X_onet)]\ncoords_abtx = proj[len(X_onet) :]\n\nplt.figure(figsize=(16, 9))\nplt.scatter(coords_onet[:, 0], coords_onet[:, 1], alpha=0.6, label=\"O*NET job titles\")\nplt.scatter(coords_abtx[:, 0], coords_abtx[:, 1], alpha=0.6, label=\"Donor-entered occupations\")\nplt.title(\"PCA of Occupation Embeddings: O*NET vs Donor-entered\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAnd in our higher-dimension space, we can calculate ad hoc similarities on any arbitrary job title someone might enter:\n\ndef normalize(job_title):\n  ad_hoc = torch.tensor(\n      model.encode(\n          [job_title],\n          normalize_embeddings=True,\n          convert_to_numpy=True,\n          show_progress_bar=False,\n      ),\n      device=device,\n  )\n  idx = int((ad_hoc @ onet_t.T).argmax(dim=1))\n  return f'{job_title} → {onet_df.at[idx, \"Title\"]}'\n\nprint(normalize(\"code ninja\"))\nprint(normalize(\"uber\"))\nprint(normalize(\"GM\"))\nprint(normalize(\"Professor, Artist\"))\nprint(normalize(\"Postdoctoral Fellow\"))\nprint(normalize(\"DARE Officer\"))\nprint(normalize(\"Senator\"))\nprint(normalize(\"Comms Director\"))\nprint(normalize(\"dermatology\"))\nprint(normalize(\"commodities trader\"))\nprint(normalize(\"life insurance adjuster\"))\nprint(normalize(\"lumberjack\"))\nprint(normalize(\"cpa\"))\n\ncode ninja → Computer Programmers\nuber → Taxi Drivers\nGM → General and Operations Managers\nProfessor, Artist → Art, Drama, and Music Teachers, Postsecondary\nPostdoctoral Fellow → Clinical Research Coordinators\nDARE Officer → Police and Sheriff's Patrol Officers\nSenator → Legislators\nComms Director → Public Relations Managers\ndermatology → Dermatologists\ncommodities trader → Securities, Commodities, and Financial Services Sales Agents\nlife insurance adjuster → Claims Adjusters, Examiners, and Investigators\nlumberjack → Fallers\ncpa → Accountants and Auditors\n\n\nIt’s not perfect. I don’t think I would have normalized Postdoctoral Fellow to Clinical Research Coordinators. This is where decisions we made in constructing the model start to show themselves. All those aka job title concatenations we did at the top affected how the language was modeled. Depending on your use case this could be fine; maybe you just need deterministic finite categories. You can play with the embedding calculation process to see how different strategies yield different resules.\nAs for our nurses:\n\nprint(normalize(\"nurse\"))\nprint(normalize(\"RN\"))\nprint(normalize(\"MSN\"))\nprint(normalize(\"ER Nurse\"))\nprint(normalize(\"home health nursing\"))\n\nnurse → Registered Nurses\nRN → Registered Nurses\nMSN → Registered Nurses\nER Nurse → Registered Nurses\nhome health nursing → Registered Nurses\n\n\nLanguage models are pretty good at modeling language!"
  },
  {
    "objectID": "posts/2024-08-08-spline-pchip/index.html",
    "href": "posts/2024-08-08-spline-pchip/index.html",
    "title": "Spline Versus PCHIP",
    "section": "",
    "text": "Let’s say you’ve got some data points and you make a scatterplot:\n\n\n\n\n\n\n\n\n\nYou might say great! and call it a day. But what if we want to see the behavior of the data between these points? Linear interpolation is a simple way to connect the dots:\n\n\n\n\n\n\n\n\n\nAnd now at this point you might say great! and call it a day. Or, you might decide that you can do better than linear interpolation. That sure does look like a sine curve. But you’re working with a collection of discrete points, and you wouldn’t want to erroneously just plot a sine function. Instead, you can reach for a smoother interpolation function, such as a spline:\n\n\n\n\n\n\n\n\n\nThe term “spline” refers to a wide class of functions involving interpolation and smoothing. In data viz, we often see the basis spline (or, B-spline). Think of spline interpolation like a flexible ruler that bends to pass smoothly through all your data points, but in doing so, it might sometimes bend too much or too little. Sometimes the spline overshoots, introducing peaks or valleys that weren’t there in the original data.\n\n\n\n\n\n\n\n\n\nSometimes this is okay! Depending on your data, a spline may be ideal for generating a very smooth curve, especially when smoothness is more critical than accurately interpolating between every data point. And when the underlying function is oscillatory, a spline can capture the movement between points quite accurately. But real-world data is often not oscillatory.\nLet’s say you’ve got a month’s worth of observed temperatures recorded in the Austin area:\n\n\n\n\n\n\n\n\n\nAnd because temperatures exist on a continuous distribution, we could do a simple linear interpolation to articulate the rates of change between points:\n\n\n\n\n\n\n\n\n\nBut temperatures are unlikely to ascend or descend on linear gradients, so we could also try a spline:\n\n\n\n\n\n\n\n\n\nThat’s a bit more natural, but it looks a bit weird, too. Unlike our sine wave sampling from before, the data points here are of real, observed, daily maximum temperatures. So it’s a little strange that the fit curve overshoots and undershoots those known values. The interpolation is smooth, but the shape of the data has not been preserved.\n\n\n\n\n\n\n\n\n\nWhile a spline produces smooth curves, the artifacts of overshooting, undershooting, or unwanted oscillations between data points can misrepresent what the data actually says. Fortunately, we have another option: the PCHIP, or Piecewise Cubic Hermite Interpolating Polynomial. Hermite refers to a method of interpolating data points where both the function values and the derivatives at those points are matched.\nA PCHIP preserves the shape of the data and avoids oscillations. The monotonicity (increasing or decreasing trend) of the data is preserved, ensuring no overshoots between data points. I like to think of PCHIP as a hand that firmly (but not rigidly) guides a curve through the data points without allowing any unnecessary dips or rises.\n\n\n\n\n\n\n\n\n\nLooks good! This results in a curve that better captures the shape of the function, especially when the slope information is critical. In our case, the slope is critical. It makes no sense to have a positive slope (overshooting) between points, when the next value decreased.\nBut PCHIP isn’t always better than Spline. Let’s apply a PCHIP interpolation to the oscillating data from before:\n\n\n\n\n\n\n\n\n\nIt’s not wrong, it’s just a little weird and lumpy. It’s a curve that connects the dots, but it somewhat lost the true movement between points.\nPCHIP can aggressively flattened near local extrema. When you need to capture those local extrema — as we did in our temperature plots — PCHIP works well. When you need to capture the smooth movements of oscillatory data, Spline works well. Sometimes it’s fairly intuitive what you need. Sometimes you need to plot it to really see which works better. Other times it takes more thought.\nConsider the nature of your data. If your data is smooth and continuous, like a waveform or a gradient, spline interpolation might work well. If your data has sharp changes or you need to preserve the natural shape of the data without introducing artifacts, PCHIP might be the better choice. In practical applications like elections modeling, financial forecasting, or engineering metrics, the choice can have significant implications.\nGraphs!"
  },
  {
    "objectID": "posts/2023-10-01-BIDEN-binary-inference-dictionaries-for-electoral-nlp/index.html",
    "href": "posts/2023-10-01-BIDEN-binary-inference-dictionaries-for-electoral-nlp/index.html",
    "title": "BIDEN: Binary Inference Dictionaries for Electoral NLP",
    "section": "",
    "text": "This post is adapted from a Jupyter Notebook found on GitHub.\nBIDEN: Binary Inference Dictionaries for Electoral NLP demonstrates a compression-based binary classification technique that is fast at both training and inference on common CPU hardware in Python. It is largely built on the strategies presented by FTCC, which in turn, was a reaction to Low-Resource Text Classification: A Parameter-Free Classification Method with Compressors (the gzip method). Like FTCC, BIDEN is built atop of Zstandard (Zstd), which leverages dictionary compression. Zstd dictionary compression seeds a compressor with sample data, so that it can efficiently compress small data (~1 KB) of similar composition. Seeding the compressor dictionaries acts as our “training” method for the model.\nThe BIDEN model was trained on the ElectionEmails 2020 data set — a database of over 900,000 political campaign emails from the 2020 US election cycle. In compliance with the data set’s terms, the training data is NOT provided with this repository. If you would like to train the BIDEN model yourself, you can request a copy of the data for free. The BIDEN model was trained on corpus_v1.0.\n\nTraining and Classification\nBoth training and inference for BIDEN are fast and simple.\nThe model consists of two Zstd compressors, one optimized for Democratic emails and one optimzed for Republican emails. Each is built upon a compression dictionary. Each compression dictionary is seeded with training sample emails from its respective party.\nClassification (inference) is achieved by compressing a test sample with both the Democratic and Republican compressors. Whichever compressor achieves a higher compression ratio on the test sample text is considered the inferred label.\n\n\nCleaning the Training Data\nThe ElectionEmails 2020 data set is a CSV. The model consideres two columns: party_affiliation and body_text. BIDEN is only concerned with binary classification for Democratic and Republican labeling.\nThe two requirements defined in requirements.txt are Pandas and zstandard:\npandas==2.1.*\nzstandard==0.21.* \nStart by reading in the data. Since the model is only working with two columns, drop any record that doesn’t contain both. Also filter the data to only consider Democratic or Republican emails for the binary classificaiton.\nNote: this assumes you have the ElectionEmails 2020 data saved at the relative path data/corpus_v1.0.csv.\n\nfrom enum import Enum\n\nimport pandas as pd\nimport zstandard\n\nfields = {\n    'body_text': str,\n    'party_affiliation': str,\n}\n\ndf = pd.read_csv(\n    'data/corpus_v1.0.csv',\n    sep=',',\n    usecols=list(fields.keys()),\n    dtype=fields,\n)\n\ndf.drop_duplicates(inplace=True)\n\nd_df = df[df.party_affiliation == \"Democratic Party\"].dropna()\nr_df = df[df.party_affiliation == \"Republican Party\"].dropna()\n\nprint(f'D Samples: {len(d_df.index)}')\nprint(f'R Samples: {len(r_df.index)}')\n\nD Samples: 127194\nR Samples: 36788\n\n\nThere are significantly more Democratic samples than Republican samples, so take a random subset of the former.\n\nmax_data = min(len(d_df.index), len(r_df.index))\nd_df = d_df.sample(\n    n=max_data,\n    random_state=9001  # random seed set for reproducibility \n).reset_index(drop=True)\n\nr_df = r_df.sample(\n    n=max_data,\n    random_state=9001  # random seed set for reproducibility \n).reset_index(drop=True)\n\nprint(f'D Samples: {len(d_df.index)}')\nprint(f'R Samples: {len(r_df.index)}')\n\nD Samples: 36788\nR Samples: 36788\n\n\nNow divide the data into training and test subsets, at an 80/20 split.\n\nd_train_df = d_df.sample(frac=0.8, random_state=9001)  # random seed set for reproducibility \nd_test_df = d_df.drop(d_train_df.index)\n\nr_train_df = r_df.sample(frac=0.8, random_state=9001)  # random seed set for reproducibility \nr_test_df = r_df.drop(r_train_df.index)\n\nprint(f'Democratic Training Samples: {len(d_train_df.index)}')\nprint(f'Democratic Test Samples: {len(d_test_df.index)}')\nprint(f'Republican Training Samples: {len(r_train_df.index)}')\nprint(f'Republican Test Samples: {len(r_test_df.index)}')\n\nDemocratic Training Samples: 29430\nDemocratic Test Samples: 7358\nRepublican Training Samples: 29430\nRepublican Test Samples: 7358\n\n\n\n\nThe BIDEN model\nThe model consistes of two core methods: train() and classify() :\n\nclass BIDEN():\n    \"\"\"\n    Binary Inference Dictionaries for Electoral NLP (BIDEN)\n\n    This class allows you to train a model for classifying political content into\n    Democratic or Republican categories based on compression ratios.\n\n    Attributes:\n        Classification (enum): An enumeration of political classifications (DEMOCRATIC, REPUBLICAN).\n    \"\"\"\n    class Classification(Enum):\n        \"\"\"\n        Enumeration of political classifications.\n\n        Attributes:\n            DEMOCRATIC (int): Represents Democratic political content.\n            REPUBLICAN (int): Represents Republican political content.\n        \"\"\"\n        DEMOCRATIC = 1\n        REPUBLICAN = 2\n        \n    def __init__(self, encoding: str = 'utf-8'):\n        \"\"\"\n        Initialize the BIDEN model.\n\n        This constructor initializes the BIDEN model with empty compressors.\n\n        Args:\n            encoding (str, optional): The character encoding of the input data. Defaults to 'utf-8'.\n            \n        Returns:\n            BIDEN: An instance of the BIDEN class.\n        \"\"\"\n        self.d_compressor = None\n        self.d_compressor = None\n        self.encoding = encoding\n\n    @property\n    def trained(self) -&gt; bool:\n        \"\"\"\n        Check if the BIDEN model is trained.\n\n        Returns:\n            bool: True if both Democratic and Republican compressors are trained, False otherwise.\n        \"\"\"\n        return bool(self.d_compressor and self.r_compressor)\n\n    def train(self,\n              d_training_data: str,\n              r_training_data: str,\n              compression_level: int = 15,\n             ) -&gt; bool:\n        \"\"\"\n        Train the BIDEN model.\n\n        Args:\n            d_training_data (str): Democratic training data.\n            r_training_data (str): Republican training data.\n            compression_level (int, optional): The compression level. Defaults to 15.\n\n        Returns:\n            bool: True if training is successful, False otherwise.\n        \"\"\"        \n        d_dictionary = zstandard.ZstdCompressionDict(\n            d_training_data.encode(self.encoding),\n            dict_type=zstandard.DICT_TYPE_RAWCONTENT\n        )\n        d_dictionary.precompute_compress(level=compression_level)\n        self.d_compressor = zstandard.ZstdCompressor(dict_data=d_dictionary)\n\n        r_dictionary = zstandard.ZstdCompressionDict(\n            r_training_data.encode(self.encoding),\n            dict_type=zstandard.DICT_TYPE_RAWCONTENT\n        )\n        r_dictionary.precompute_compress(level=compression_level)\n        self.r_compressor = zstandard.ZstdCompressor(dict_data=r_dictionary)\n\n        return self.trained\n\n    def classify(self, sample: str) -&gt; Classification:\n        \"\"\"\n        Classify a sample based on compression ratios.\n\n        Args:\n            sample (str): The sample text to classify.\n\n        Returns:\n            Classification: The classification (DEMOCRATIC or REPUBLICAN).\n        \n        Raises:\n            RuntimeError: If the model is not trained.\n        \"\"\"\n        if not self.trained:\n            raise RuntimeError(\"Attempted to classify with a model that is not yet trained.\")\n        \n        encoded_sample = sample.encode(self.encoding)\n        original_length = len(encoded_sample)\n        d_compressed_length = len(self.d_compressor.compress(encoded_sample))\n        d_ratio = d_compressed_length / original_length\n        r_compressed_length = len(self.r_compressor.compress(encoded_sample))\n        r_ratio = r_compressed_length / original_length\n\n        if r_ratio &lt; d_ratio:\n            return BIDEN.Classification.REPUBLICAN\n\n        return BIDEN.Classification.DEMOCRATIC\n\n\n\nTrain the Model\nTo train the model, we pass the entirety of the Democratic and Republican texts to construct prefix dictionaries. Prefix dictionaries allow compression operations to reference raw data within the dictionary. Once we have two compressors instantiated and pre-seeded with our training data, the model is trained. This is fast. On my 2.6 GHz 6-Core Intel Core i7, this takes roughly 30 seconds.\n\nd_combined_text = '\\n'.join(d_train_df.body_text)\nr_combined_text = '\\n'.join(r_train_df.body_text)\n\nmodel = BIDEN()\nmodel.train(d_combined_text, r_combined_text)\n\nTrue\n\n\n\n\nClassification\nNow, we can classify our test data. We could loop through each set, but let’s combine and shuffle them together first, and loop in one go. We’ll also convert the party affiliation strings 'Democratic Party', and 'Republican Party' into our model’s enum values:\n\ncombined_test_df = pd.concat(\n    [d_test_df, r_test_df],\n    ignore_index=True,\n    sort=False\n).sample(\n    frac=1,\n    random_state=9001,  # random seed set for reproducibility\n).reset_index(drop=True)\n\ncombined_test_df['party_affiliation'] = combined_test_df['party_affiliation'].replace(\n    to_replace=['Democratic Party', 'Republican Party'],\n    value=[BIDEN.Classification.DEMOCRATIC, BIDEN.Classification.REPUBLICAN]\n)\n\nnum_correct = 0\nfor row in combined_test_df.itertuples():\n    actual_label = row.party_affiliation\n    inferred_label = model.classify(row.body_text)\n\n    if inferred_label == actual_label:\n        num_correct += 1\n\nprint(f'Classification Success Rate: {((num_correct / len(combined_test_df.index))*100):.1f}%')\n\nClassification Success Rate: 98.9%\n\n\n98.9% is a shockingly high success rate for such a simple classification method!\n\n\nAnother Email Data Set\nLet’s see how it performs with emails from another data set. Derek Willis maintains a Datasette of over 150,000 campaign emails. Let’s grab 100 samples of each party from that collection, and see how they perform:\n\nbase_url = 'https://political-emails.herokuapp.com/emails.csv?sql='\nd_query = 'select body, party from emails where party = \"D\" limit 100;'\nr_query = 'select body, party from emails where party = \"R\" limit 100;'\n\nmap = {\n    ' ': '+',\n    ',': '%2C',\n    '=': '%3D',\n    '\"': '%22',\n    ';': '%3B',\n}\n\nfor symbol, code in map.items():\n    d_query = d_query.replace(symbol, code)\n    r_query = r_query.replace(symbol, code)\n\nd_url = base_url + d_query\nr_url = base_url + r_query\n\nd_dw_df = pd.read_csv(d_url)\nr_dw_df = pd.read_csv(r_url)\n\ncombined_dw_df = pd.concat(\n    [d_dw_df, r_dw_df],\n    ignore_index=True,\n    sort=False\n).sample(\n    frac=1,\n    random_state=9001,  # random seed set for reproducibility\n).reset_index(drop=True)\n\ncombined_dw_df['party'] = combined_dw_df['party'].replace(\n    to_replace=['D', 'R'],\n    value=[BIDEN.Classification.DEMOCRATIC, BIDEN.Classification.REPUBLICAN]\n)\n\nprint('DW Sample:')\ncombined_dw_df.head(10)\n\nDW Sample:\n\n\n\n\n\n\n\n\n\nbody\nparty\n\n\n\n\n0\nHey Peter!!! You know our campaign is made by,...\nClassification.DEMOCRATIC\n\n\n1\nhttps://www.jahanahayes.com/ [https://www.jaha...\nClassification.DEMOCRATIC\n\n\n2\nHi there, it’s storytime: I developed a keen n...\nClassification.DEMOCRATIC\n\n\n3\nWe’re contacting a select group of patriots fo...\nClassification.REPUBLICAN\n\n\n4\nPeter – You need to watch this. CLICK HERE OR ...\nClassification.REPUBLICAN\n\n\n5\nPeter, you may have seen me asking for your ol...\nClassification.DEMOCRATIC\n\n\n6\nDo you want an Official Mugshot Mug? (24-hour ...\nClassification.REPUBLICAN\n\n\n7\nhttps://secure.actblue.com/donate/cdp-footer?r...\nClassification.DEMOCRATIC\n\n\n8\n[ https://act.katieporter.com/go/14559?t=1001&...\nClassification.DEMOCRATIC\n\n\n9\nI hope I made you proud fighting for you at th...\nClassification.REPUBLICAN\n\n\n\n\n\n\n\n\nnum_correct = 0\nfor row in combined_dw_df.itertuples():\n    actual_label = row.party\n    inferred_label = model.classify(row.body)\n\n    if inferred_label == actual_label:\n        num_correct += 1\n\nprint(f'Classification Success Rate: {((num_correct / len(combined_dw_df.index))*100):.1f}%')\n\nClassification Success Rate: 93.0%\n\n\n93% is still quite good considering that all we’re doing is compression!\n\n\nClassifying Tweets\nThe Twitter API is basically useless these days, so I scrolled the timelines of Mike Pence (R) and Gavin Newsom (D), and copy+paste’d 5 tweets from each. It’s a tiny sample, and not really random, but it’s neat to see how well it does:\n\npence_tweets = [\n    # https://twitter.com/Mike_Pence/status/1707882018258751915\n    \"Today, we applaud the Eighth Circuit's decision, \" \\\n    \"which is an important step in the fight to uphold and protect \" \\\n    \"the rights of parents and families in Linn-Mar. A family is the \" \\\n    \"single best unit to protect children, and we must continue to do \" \\\n    \"everything we can to empower parents over bureaucrats. The strength \" \\\n    \"of our nation is tied to the strength of our families, and we will \" \\\n    \"not stand idly by as the Radical Left attempts to indoctrinate our \" \\\n    \"children behind parents’ backs.\",\n    # https://twitter.com/Mike_Pence/status/1707472823269392643\n    \"The cause of Life is the calling of our time \" \\\n    \"and I couldn’t be more proud to have played a role in the administration \" \\\n    \"that appointed three of the justices that overturned Roe v. Wade and \" \\\n    \"returned the question of abortion to the states and the American people.\",\n    \"Republicans are facing a Time for Choosing. \" \\\n    # https://twitter.com/Mike_Pence/status/1707241587460186214\n    \"We have to choose whether or not we’re going to stay on the path \" \\\n    \"that has defined our movement since the days of Ronald Reagan and \" \\\n    \"through the Trump-Pence years or whether we’re going to follow \" \\\n    \"the siren song of populism unmoored to Conservative principles.\",\n    # https://twitter.com/Mike_Pence/status/1704132623617122719\n    \"I am for working Americans and free enterprise. These businesses \" \\\n    \"make their decisions, but at the end the of the day these businesses \" \\\n    \"are responding to the heavy hand of the green new deal agenda of \" \\\n    \"Joe Biden and the Democrats in Washington, DC.\",\n    # https://twitter.com/Mike_Pence/status/1703887286641873137\n    \"We were the first country to sanction Chinese leaders for building \" \\\n    \"concentration camps in Xinjiang and for undermining democracy in Hong Kong. \" \\\n    \"And we stood up to years of trade abuses, and imposed historic tariffs \" \\\n    \"to bring China to the negotiating table.\"\n]\n\n[model.classify(t) for t in pence_tweets]\n\n[&lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;]\n\n\n\nnewsom_tweets = [\n    # https://twitter.com/GavinNewsom/status/1700615276667294035\n    \"When people ask why I am introducing a Constitutional Amendment \" \\\n    \"on gun safety this is why. Not only has Congress stalled for YEARS on passing \" \\\n    \"common sense reforms -- judges across the country are tearing down laws that \" \\\n    \"Americans overwhelmingly support. Laws that keep us SAFE and keep guns out of \" \\\n    \"the hands of dangerous criminals. We have to push back\",\n    # # https://twitter.com/GavinNewsom/status/1689743766733877248\n    \"California will be sending search and rescue \" \\\n    \"teams to assist in Hawaii's recovery efforts. The wildfires and \" \\\n    \"devastation that Maui is experiencing is all too familiar and all \" \\\n    \"too horrifying. We stand at the ready to aid Hawaii in its time of need.\",\n    # https://twitter.com/GavinNewsom/status/1679579172690329601\n    \"A school board in Temecula decided to reject a \" \\\n    \"textbook because it mentioned Harvey Milk. CA is stepping in. \" \\\n    \"We’re going to purchase the book for these students—the same \" \\\n    \"one that hundreds of thousands of kids are already using. \" \\\n    \"If these extremist school board members won’t do their job, \" \\\n    \"we will — and fine them for their incompetence.\",\n    # https://twitter.com/GavinNewsom/status/1650634702271942656\n    \"North Dakota GOP have decided to force women to give birth. Even victims of rape. \" \\\n    \"Meanwhile, they voted against providing school meals because child hunger isn't \" \\\n    \"\\\"the responsibility of the state.\\\"\" \\\n    \"Mandating birth is state responsibility. Helping feed those kids is not. Got it.\",\n    # https://twitter.com/GavinNewsom/status/1643745476662132737\n    \"Met with some librarians today while in Florida. \" \\\n    \"They shared with me the rich, diverse background of the \" \\\n    \"town and what's at stake if we ban our kids from learning our real history.\"\n]\n\n[model.classify(t) for t in newsom_tweets]\n\n[&lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.DEMOCRATIC: 1&gt;,\n &lt;Classification.REPUBLICAN: 2&gt;]\n\n\nNeat! This small set classified 90% correctly. Further exploration with more robust Twitter data sets is warranted.\n\n\nClosing Thoughts\nEven after seeing the results, I still have a hard time believing that this works so well! It feels like it shouldn’t work at all. But, zooming out, there are a lot of relevant factors to consider. First, there just aren’t that many people writing campaign materials. It makes sense that word-choice and writing style would exhibit predictible patterns. Second, campaign emails have been A/B tested into oblivion, so there’s a systematic process that cranks out similar-sounding copy. Third, the recipients of these emails have largely self-sorted. This likely bolsters the expected structure and copy uniquely for each label. Ultimately, compression algorithms optimize on patterns and predictibility. What this shows us is that the two parties are uniquely predictible in their written communications.\nThe idea of classification by compression is not new; Russell and Norvig wrote about it in 1995 in the venerable Artificial Intelligence: A Modern Approach:\n\n\n\nClassification by data compression\n\n\nMore recently, the “gzip beats BERT” paper got a lot of attention. What the BIDEN model demonstrates is that this technique is effective and likely generalizable on modern partisan texts.\nIt’s worth articulating again how fast and simple this method is. No GPUs. No Neural Networks. No N-grams. No transformers. No kNN.\nI think that’s pretty cool!\n\n\nLicense\nCopyright (c) 2023, Matt Hodges\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of BIDEN: Binary Inference Dictionaries for Electoral NLP nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a Political Technologist and founder of Ilium Strategies working to help Democrats build, scale, and innovate with tech. Drawing on over a decade of experience at the intersection of Democratic politics and software engineering, I partner with organizations, campaigns, and investors to amplify their impact. My focusses include product and engineering vision, go-to-market technical strategy, donor advising and technical diligence, political tech market navigation, ecosystem coordination, and AI strategy. I’m particularly interested in developing sustainable, scalable models for long-term impact in the political tech sector.\nI was the Executive Director of Zinc Labs, a political innovation lab at Zinc Collective that houses the organization’s strategic work in Democratic technology and pilot programs. Through a network of campaigns, committees, vendors, donors, and subject matter experts, Zinc Labs incubates and invests in innovative new programs across the ecosystem. I prioritized investing in technology and people to enable collaborative experimentation, technical execution, and hypothesis testing.\nI sit on the Advisory Board of Higher Ground Labs, an accelerator that invests in early-stage political tech companies. In this capacity, I contribute strategic guidance to the Higher Ground Labs leadership team, and provide both political and technical advice to investment cohort companies.\nI was on the Advisory Board of Quiller, an Artificial Intelligence copilot that helps Democrats draft and deploy high quality, effective fundraising content. In this capacity, I contributed product, technology, and political guidance to the Quiller leadership team. Quiller was acquired in 2025.\nI was the Director of Engineering for President Joe Biden on his 2020 presidential campaign. I joined the campaign in July of 2019, and served in that role through the duration of the primary and general election cycles. I led a team of software, infrastructure, and security engineers, which was charged with delivering innovative new campaign tech for headquarters and states programs. We shipped dozens of internal and voter-facing products, including an in-house peer-to-peer texter, a relational organizing app, a real-time debate transcription service, a document and media ingestion and indexing library, a live donation tracking web service, a distributed soft-reporting platform, the JoeBiden.com web property, hundreds of ETL pipelines, and a cross-region data warehouse.\nI was a Senior Software Engineer on The Groundwork platform, which served as the application and infrastructure system that powered the 2016 presidential campaign of Secretary Hillary Clinton. Beginning in 2015, I was the primary engineer for multiple campaign-enabling web services for event signups, single-click donations, and peer-to-peer fundraising.\nI was an Infrastructure Engineer at Datadog where I architected and implemented the internal platform for Incident Response management. This tool eventually joined the suite of observability offerings available to customers. My team also implemented the organization’s first templated deployment pipeline for the organization’s global transition to Kubernetes.\nI was a Senior Mobile Engineer at LISNR, where I developed the product’s first iOS SDK for the ultrasonic data transport protocol. In 2014 the team joined the TechStars accelerator. This toolchain was originally designed to allow musicians to embed inaudbile data streams into their media. Over time, it became a location-aware second screen immersion platform for music festivals, professional sporting events, and feature films. It now serves as a payment protocol in partnership with VISA.\nI was a Senior Mobile Engineer and Scrum Master at Cardinal Solutions, with the team that built Vantiv Accept - a mobile credit card processing application. Envisioned as a competitor to Square, the application was deployed to retail and grocery point-of-sale systems across the country.\nI am a Named Inventor on an Augmented Reality Patent Application (US20140092241A1) for a “device and method for scanning of books and other items for order and inventory control”. We created a mobile application that scanned a shelf of books in real-time as a user viewed it through the screen. If it identified any items out of order, it overlayed the user’s view of the item with a red X to indicate a misplacement, and then povided directional guidance of where it should be moved.\nI have been an Election Night Reporter for Decision Desk HQ. I gathered live results from New Mexico and Ohio Boards of Elections offices to report real-time data as they were delivered from precincts. These results were featured on Vox and BuzzFeed election night coverage.\nI have been a frequent Open Source Contributor to OpenElections, which is creating the first free, comprehensive, standardized, linked set of election results data for the United States."
  },
  {
    "objectID": "index.html#work",
    "href": "index.html#work",
    "title": "About",
    "section": "",
    "text": "I am a Political Technologist and founder of Ilium Strategies working to help Democrats build, scale, and innovate with tech. Drawing on over a decade of experience at the intersection of Democratic politics and software engineering, I partner with organizations, campaigns, and investors to amplify their impact. My focusses include product and engineering vision, go-to-market technical strategy, donor advising and technical diligence, political tech market navigation, ecosystem coordination, and AI strategy. I’m particularly interested in developing sustainable, scalable models for long-term impact in the political tech sector.\nI was the Executive Director of Zinc Labs, a political innovation lab at Zinc Collective that houses the organization’s strategic work in Democratic technology and pilot programs. Through a network of campaigns, committees, vendors, donors, and subject matter experts, Zinc Labs incubates and invests in innovative new programs across the ecosystem. I prioritized investing in technology and people to enable collaborative experimentation, technical execution, and hypothesis testing.\nI sit on the Advisory Board of Higher Ground Labs, an accelerator that invests in early-stage political tech companies. In this capacity, I contribute strategic guidance to the Higher Ground Labs leadership team, and provide both political and technical advice to investment cohort companies.\nI was on the Advisory Board of Quiller, an Artificial Intelligence copilot that helps Democrats draft and deploy high quality, effective fundraising content. In this capacity, I contributed product, technology, and political guidance to the Quiller leadership team. Quiller was acquired in 2025.\nI was the Director of Engineering for President Joe Biden on his 2020 presidential campaign. I joined the campaign in July of 2019, and served in that role through the duration of the primary and general election cycles. I led a team of software, infrastructure, and security engineers, which was charged with delivering innovative new campaign tech for headquarters and states programs. We shipped dozens of internal and voter-facing products, including an in-house peer-to-peer texter, a relational organizing app, a real-time debate transcription service, a document and media ingestion and indexing library, a live donation tracking web service, a distributed soft-reporting platform, the JoeBiden.com web property, hundreds of ETL pipelines, and a cross-region data warehouse.\nI was a Senior Software Engineer on The Groundwork platform, which served as the application and infrastructure system that powered the 2016 presidential campaign of Secretary Hillary Clinton. Beginning in 2015, I was the primary engineer for multiple campaign-enabling web services for event signups, single-click donations, and peer-to-peer fundraising.\nI was an Infrastructure Engineer at Datadog where I architected and implemented the internal platform for Incident Response management. This tool eventually joined the suite of observability offerings available to customers. My team also implemented the organization’s first templated deployment pipeline for the organization’s global transition to Kubernetes.\nI was a Senior Mobile Engineer at LISNR, where I developed the product’s first iOS SDK for the ultrasonic data transport protocol. In 2014 the team joined the TechStars accelerator. This toolchain was originally designed to allow musicians to embed inaudbile data streams into their media. Over time, it became a location-aware second screen immersion platform for music festivals, professional sporting events, and feature films. It now serves as a payment protocol in partnership with VISA.\nI was a Senior Mobile Engineer and Scrum Master at Cardinal Solutions, with the team that built Vantiv Accept - a mobile credit card processing application. Envisioned as a competitor to Square, the application was deployed to retail and grocery point-of-sale systems across the country.\nI am a Named Inventor on an Augmented Reality Patent Application (US20140092241A1) for a “device and method for scanning of books and other items for order and inventory control”. We created a mobile application that scanned a shelf of books in real-time as a user viewed it through the screen. If it identified any items out of order, it overlayed the user’s view of the item with a red X to indicate a misplacement, and then povided directional guidance of where it should be moved.\nI have been an Election Night Reporter for Decision Desk HQ. I gathered live results from New Mexico and Ohio Boards of Elections offices to report real-time data as they were delivered from precincts. These results were featured on Vox and BuzzFeed election night coverage.\nI have been a frequent Open Source Contributor to OpenElections, which is creating the first free, comprehensive, standardized, linked set of election results data for the United States."
  },
  {
    "objectID": "index.html#media-appearances",
    "href": "index.html#media-appearances",
    "title": "About",
    "section": "Media & Appearances",
    "text": "Media & Appearances\n\n\nTrump wants an investigation of Democrats’ fundraising. His own campaign has issues\n\nAP, 2025\n\n\n[Original] [Archive]\n\n\n\nMatt Hodges Modded the LED DNC Bracelets\n\nWired, 2024\n\n\n[Original] [Archive]\n\n\n\nBig Tech Is Giving Campaigns Both the Venom and the Antidote for GenAI\n\nWired, 2024\n\n\n[Original] [Archive]\n\n\n\nAI Guidelines Earn Widespread Support Across the Democratic Political Community\n\nZinc Labs, 2024\n\n\n[Original] [Archive]\n\n\n\nPodcast: AI’s do’s and dont’s in politics\n\nThe Chuck Todd Podcast (Chuck ToddCast) | NBC News, 2024\n\n\n[Original] [Archive]\n\n\n\nPanel: A Responsible and Secure Approach to AI in Democratic Campaigns\n\nDefending Digital Campaigns & Microsoft, 2024\n\n\n[Original] [Archive]\n\n\n\nThe dos and don’ts of campaigning with AI\n\nThe Washington Post, 2024\n\n\n[Original] [Archive]\n\n\n\nQuick-Start AI Guidelines for Democratic CampaignsI\n\nZinc Labs, 2024\n\n\n[Original] [Archive]\n\n\n\nPanel: Threats vs Practicality - Realistic Solutions for Securing Campaigns\n\nGoogle and Defending Digital Campaigns Security Summit, 2024\n\n\n[Original] [Archive]\n\n\n\nPodcast: Can AI Help Meet Voters Where They Are?\n\nPolitics Is Everything, 2023\n\n\n[Original] [Archive]\n\n\n\nAI will change American elections, but not in the obvious way\n\nThe Economist, 2023\n\n\n[Original] [Archive]\n\n\n\n1 big thing: Democratic tech vendors go under the cyber microscope\n\nAxios, 2023\n\n\n[Original] [Archive]\n\n\n\nGenerative artificial intelligence tools ‘threaten democracy, reality’\n\nThe Courier-Mail, 2023\n\n\n[Original] [Archive]\n\n\n\nMatt Hodges, Political Technologist\n\nUses This, 2023\n\n\n[Original] [Archive]\n\n\n\nA Campaign Aide Didn’t Write That Email. A.I. Did.\n\nThe New York Times, 2023\n\n\n[Original] [Archive]\n\n\n\nWelcome Keynote\n\n2022 Election Tech Debrief, 2023\n\n\n[Original] [Archive]\n\n\n\nKeynote: Tech Innovation Comes From Those Who Build It\n\nCampaignTech Innovation Summit, 2022\n\n\n[Original]\n\n\n\nPanel: Tech For Good\n\nUnified Jam SXSW, 2022\n\n\n[Original] [Archive]\n\n\n\nBuilding Campaign Tech For Early Adoption\n\nHigher Ground Labs, 2021\n\n\n[Original] [Archive]\n\n\n\nPanel: Securing the 2020 Presidential Campaign\n\nBSides Las Vegas, 2021\n\n\n[Original] [Archive]\n\n\n\nThe Urgent Need For Democratic Tech Talent\n\nCampaigns and Elections, 2021\n\n\n[Original] [Archive]\n\n\n\nPodcast: Building Political Technology\n\nThe Great Battlefield, 2021\n\n\n[Original] [Archive]\n\n\n\n“Wear a Mask” In The Source Code\n\nThe Rachel Maddow Show | MSNBC, 2021\n\n\n[Archive]\n\n\n\nThe Biden administration quickly revamped the White House website. Here’s how.\n\nThe New York Times, 2021\n\n\n[Original] [Archive]\n\n\n\nButler County native, Miami grad played significant role in Biden campaign\n\nThe Journal-News, 2021\n\n\n[Original] [Archive]\n\n\n\n2018 Profile of Ohio’s 1st District\n\nDecision Desk HQ, 2018\n\n\n[Original] [Archive]\n\n\n\nThe other major hurdle for the Parkland teens: Turning out their peers\n\nThe Washington Post, 2018\n\n\n[Original] [Archive]\n\n\n\nPodcast: Can Zuck be trusted?\n\nTechieBytes, 2018\n\n\n[Original] [Archive]"
  }
]